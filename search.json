[
  {
    "objectID": "maps.html",
    "href": "maps.html",
    "title": "20  Static Maps",
    "section": "",
    "text": "Along with all the geographical data making its way into the public domain, a variety of tools to map that data have also been developed. For a long time R base provides tools to map geographical data—data with latitude and longitude coordinates attached to it. However, mapping in the early days of R was not easy, it was not elegant either. Recently, several packages have been developed for mapping geographical data that align with the ggplot framework. So they allow us to map spatial data in similar ways as other plots with a grammar of graphic principle. For example Edzer Pebesma (2018) developed an awesome sf package for mapping vector data in R. The power of this function lies in the fact that it structure the spatial data in tidy format, allowing for easy manipulation with the tidyverse function and also for plotting with the ggplot2 flavor. The sf package allows you to read in and work with geographical data in a tidy format."
  },
  {
    "objectID": "maps.html#geographical-data-in-a-tidy-format",
    "href": "maps.html#geographical-data-in-a-tidy-format",
    "title": "20  Static Maps",
    "section": "20.1 Geographical data in a tidy format",
    "text": "20.1 Geographical data in a tidy format\nThe sf package allows you to create a simple feature—a data frame with additional columns that hold spatial component called the geometry. This column contains the geometrical nature of the data needed to draw the data. Often the sf object has two classes—the simple feature and the data.frame classes. The data.frame holds attribute information of the dataset and the geometry contains the geographical coordinates. For example, the simple feature displayed below is a dataset of sampling stations, where each row gives the data for each station. The data.frame here holds the first four columns— the id, type, depth and sst, whereas the geometry column include the geometry type, for this case the point and the embedded latitude and longitude geographical coordinates. There different ways to create simple feature in R using the sf package. We will create a few of them later that we will use for mapping examples.\nWe need to load the packages we are going to use in this chapter.\n\nrequire(sf)\nrequire(tidyverse)\n\n\n\nFALSE Simple feature collection with 11 features and 4 fields\nFALSE Geometry type: POINT\nFALSE Dimension:     XY\nFALSE Bounding box:  xmin: 39.50958 ymin: -8.425115 xmax: 42.00623 ymax: -6.414011\nFALSE Geodetic CRS:  WGS 84\nFALSE First 10 features:\nFALSE     id   type depth      sst                   geometry\nFALSE 1  294 marker    29 27.87999 POINT (39.50958 -6.438159)\nFALSE 2  300 marker  -604 27.97999  POINT (39.6318 -6.621774)\nFALSE 3  306 marker  -569 27.97999 POINT (39.65447 -6.746649)\nFALSE 4  312 marker  -485 28.03999 POINT (39.62563 -6.805321)\nFALSE 5  318 marker  -325 28.03999 POINT (39.58374 -6.833973)\nFALSE 6  326 marker  -461 28.03999 POINT (39.66476 -6.837384)\nFALSE 7  414 marker  -505 28.02999 POINT (39.95728 -7.843535)\nFALSE 8  428 marker  -132 28.23999 POINT (39.67712 -8.136846)\nFALSE 9  434 marker  -976 28.16999 POINT (39.74853 -8.425115)\nFALSE 10 456 marker -3311 28.33999 POINT (42.00623 -7.025368)\n\n\n\n20.1.1 Create simple feature from data.frame\n\n\n\nIf you have a regular dataframe, you can convert it into a simple feature object with tools in the sf package. For instance, in the working directory we have a dataset of eleven stations named points.csv. We can simply import this dataset into R session with the read_csv() function. If we print the file, it give about the variables and rows presented in the datasete. There six variables—id, type, depth and sst along with the latitude and longitude coordinates. These stations contains measured variable of sea surface temperature and their maximum depth.\n\nstations = read_csv(\"assets/shp//points.csv\")\nstations\n\n# A tibble: 11 x 6\n     lon   lat    id type   depth   sst\n   <dbl> <dbl> <dbl> <chr>  <dbl> <dbl>\n 1  39.5 -6.44   294 marker    29  27.9\n 2  39.6 -6.62   300 marker  -604  28.0\n 3  39.7 -6.75   306 marker  -569  28.0\n 4  39.6 -6.81   312 marker  -485  28.0\n 5  39.6 -6.83   318 marker  -325  28.0\n 6  39.7 -6.84   326 marker  -461  28.0\n 7  40.0 -7.84   414 marker  -505  28.0\n 8  39.7 -8.14   428 marker  -132  28.2\n 9  39.7 -8.43   434 marker  -976  28.2\n10  42.0 -7.03   456 marker -3311  28.3\n11  41.8 -6.41   462 marker -3248  28.6\n\n\nAthough this dataset contains geographical coordinates in it (latitude and longitude), it’s just a regular data frame. We can use the geographical coordinates in the dataframe to convert it to simple feature with the st_as_sf() function, and specify the columns with the geographical information using the coords parameter.\n\nsimple_feature = stations %>%\n  sf::st_as_sf(coords = c(\"lon\", \"lat\"))\n\nOnce we have the simple feature object, we can set the geographica coordinate system to World Geodetic System of 1984 (WGS84). I prefer using its code, which is easy to punch in instead of the whole text. If we print out the simple feature we just created, it gives the extra information at the top of the print-out, which include the number of features and columls(fields), the geometry type as point, the geographical extent (the bounding box) of and the projection both in epsg code and string.\n\nsimple_feature = simple_feature %>%\n  sf::st_set_crs(4326)\n\n\nggplot()+\n  geom_sf(data = simple_feature, aes(col = sst, size = wior::inverse_hyperbolic((sst))))+\n  scale_colour_gradientn(colors = oce::oce.colors9A(120))\n\n\n\n\n\n\n20.1.2 Importing shapefile\nWhen you create maps, you will often want to import shapefile—a widely used format for storing geographical data in GIS. sf package offers tools to read and load shapefiles into R. Let’s import africa’s country boundary shapefile from the working directory. We use the st_read() function from sf package to read the shapefile boundary layer. Like the simple features we created, the shapefile also display extra information at the top confirming that it’s no longer a shapefile but rather a simple feature.\n\n## read\nafrica = sf::st_read(\"assets/shp/africa.shp\", quiet = TRUE)\nafrica\n\nSimple feature collection with 59 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -25.35875 ymin: -34.83983 xmax: 57.80085 ymax: 37.34962\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   COUNT               CNTRY_NAME FIPS_CNTRY LAND_AREA_ REGIONA EMPTY EMPTY2\n1     34                   Angola         AO     124670    <NA>     0      0\n2    114                  Burundi         BY       2783    <NA>     0      0\n3     77                    Benin         BN      11262    <NA>     0      0\n4    301             Burkina Faso         UV      27400    <NA>     0      0\n5     25                 Botswana         BC      58173    <NA>     0      0\n6     51 Central African Republic         CT      62298    <NA>     0      0\n7     51                 Cameroon         CM      47544    <NA>     0      0\n8    186              Ivory Coast         IV      32246    <NA>     0      0\n9     46                    Congo         CF      34200    <NA>     0      0\n10    15               Cape Verde         CV        403    <NA>     0      0\n                         geometry\n1  MULTIPOLYGON (((12.84118 -6...\n2  MULTIPOLYGON (((29.05021 -2...\n3  MULTIPOLYGON (((3.849006 10...\n4  MULTIPOLYGON (((-5.272945 1...\n5  MULTIPOLYGON (((23.14635 -1...\n6  MULTIPOLYGON (((22.03557 4....\n7  MULTIPOLYGON (((9.640797 3....\n8  MULTIPOLYGON (((-6.091862 4...\n9  MULTIPOLYGON (((16.45276 2....\n10 MULTIPOLYGON (((-24.64849 1...\n\n\nSince the layer is for the whole Africa, to reduce the processing time, we must reduce the geographical extent to the area of interest. We use the st_crop() function to chop the area that we want to map and discard the rest.\n\nkimbiji = africa %>% \n  sf::st_crop(xmin = 38.0,\n              xmax = 40.5, \n              ymin = -8, \n              ymax = -5.5)\n\n\n\n20.1.3 Reading other format\nSometimes you have geographical data that are neither in tabular form or shapefile. In that situation, you ought to use the st_layers() function to identify, first the driver used to create the file and, second, the layer name you want to extract. Once you have identified the layer of interest, you can use the st_read() function to import the layer from the file. For example, we have the track file that was recorded with a GPS device. Let’s explore the layer it contains with the `st_layers() function.\n\ntracks = sf::st_layers(\"assets//tracks/Track-181204-075451.gpx\")\ntracks\n\nDriver: GPX \nAvailable layers:\n    layer_name     geometry_type features fields\n1    waypoints             Point        1     19\n2       routes       Line String        0     12\n3       tracks Multi Line String        1     12\n4 route_points             Point        0     21\n5 track_points             Point     1467     24\n\n\nOnce we print, it shows that i’s a GPX format with five layer’s name. We are only interested with the tracks and track_points layers. We can extract them with the st_read() function, by specifying the dsn and the layer. This can be written as;\n\n## obtain track points\ntrack.points = sf::st_read(dsn =\"assets//tracks/Track-181204-075451.gpx\" ,layer = \"track_points\", quiet = TRUE)\n## drop other variable that are not needed\ntrack.points = track.points %>% select(elevation = ele, time, speed)\n## display\ntrack.points\n\nSimple feature collection with 1467 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 39.68927 ymin: -8.033337 xmax: 39.75059 ymax: -7.977127\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   elevation                time    speed                   geometry\n1       -3.7 2018-12-04 07:54:55 1.471591 POINT (39.75051 -7.977127)\n2       -5.0 2018-12-04 07:54:58 1.479312 POINT (39.75052 -7.977157)\n3       -5.6 2018-12-04 07:55:00 1.358867 POINT (39.75052 -7.977185)\n4       -5.8 2018-12-04 07:55:02 1.530269 POINT (39.75052 -7.977215)\n5       -5.9 2018-12-04 07:55:04 1.424751 POINT (39.75052 -7.977243)\n6       -6.1 2018-12-04 07:55:05 1.381000 POINT (39.75052 -7.977262)\n7       -6.2 2018-12-04 07:55:07 1.437619  POINT (39.75052 -7.97729)\n8       -6.3 2018-12-04 07:55:09 0.994958  POINT (39.75051 -7.97731)\n9       -6.3 2018-12-04 07:55:11 1.032018  POINT (39.75051 -7.97733)\n10      -6.4 2018-12-04 07:55:13 1.369676  POINT (39.7505 -7.977353)\n\n\n\ntrack = sf::st_read(dsn =\"assets//tracks/Track-181204-075451.gpx\" ,layer = \"tracks\", quiet = TRUE)\ntrack\n\nSimple feature collection with 1 feature and 12 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 39.68927 ymin: -8.033337 xmax: 39.75059 ymax: -7.977127\nGeodetic CRS:  WGS 84\n                               name  cmt\n1 Tracking android:60fd0ef637a6eb1b <NA>\n                                     desc  src link1_href link1_text link1_type\n1 Tracking recently started 12/4/18 07:54 <NA>       <NA>       <NA>       <NA>\n  link2_href link2_text link2_type number type                       geometry\n1       <NA>       <NA>       <NA>  10193 <NA> MULTILINESTRING ((39.75051 ..."
  },
  {
    "objectID": "maps.html#introduction",
    "href": "maps.html#introduction",
    "title": "20  Static Maps",
    "section": "20.2 Introduction",
    "text": "20.2 Introduction\nA satisfying and important aspect of geographic research is communicating the results in spatial context. With recent advance in technology from satellite, internet and mobile location services, the amount of geographical data has increased significantly. Plenty of data are generated daily with latitude and longitude coordinates attached to it both from satellite observation and social media. To be able to build up a good mental model of the spatial data, you need to invest considerable effort in making your maps as self-explanatory as possible. In this chapter, you’ll learn some of the tools that ggplot2 provides to make elegant maps and graphics (Wickham 2016).\nMaps are great way to understand patterns from data over space. They are scaled down versions of the physical world, and they’re everywhere. R has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile. ggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. The chapter also introduce you to some extended functionalities from sf (Pebesma 2018), cowplot (Wilke 2018), ggsn (ggsn?), ggsci (ggsci?), metR (Campitelli 2021), ggrepel (ggrepel?), gganimate (Pedersen and Robinson 2017) and egg (Auguie 2018) packages. Therefore, this chapter focuses on the tools you need to create good graphics. Rather than loading those extensions here, we’ll refer to their functions explicitly, using the :: notation. This will help make it clear which functions are built into ggplot2, and which come from other packages. Ensure you have these packages in your machine, otherwise install them with install.packages() if you don’t already have them."
  },
  {
    "objectID": "maps.html#static-maps",
    "href": "maps.html#static-maps",
    "title": "20  Static Maps",
    "section": "20.3 Static Maps",
    "text": "20.3 Static Maps\nStatic maps are the most common type of visual output from spatial objects. Fixed images for printed outputs, common formats for static maps include .png and .pdf, for raster and vector outputs, respectively. Initially static maps were the only type of map that R could produce. Things have advanced greatly since sp was released (see (sp?)). Many new techniques for map making have been developed since then. However, a decade later static plotting was still the emphasis of geographic data visualisation in R (Cheshire and Lovelace 2015).\nDespite the innovation of interactive mapping in R, static maps are still the foundation of mapping in R. The base plot() function is often the fastest way to create static maps from vector and raster spatial objects. Sometimes simplicity and speed are priorities, especially during the development phase of a project, and this is where plot() excels. The base R approach is also extensible, with plot() offering dozens of arguments. Another low-level approach is the grid package, which provides functions for low-level control of graphical outputs. This section, however, focus on how to make static maps with ggplot2, emphasizing the important aesthetic and layout options.\n\n20.3.1 The bathmetry data\nggplot2 works with data that are tidy—data frame arranged in such way that observations are in rows and variables are in columns and each value must have its own cell. But, the bathmetry data is from ETOPO1 and came in .asc format. First read the file with the raster::raster() function.\n\n## read the ascii file\ntz.bath = raster::raster(\"assets/raster/wioregio-7753.asc\")\ntz.bath %>% class()\n\n[1] \"RasterLayer\"\nattr(,\"package\")\n[1] \"raster\"\n\n\nWe notice that the file is raster and ggplot2 requires data.frame. To continue, we need to convert the data and tidy in format that is **plot2–readable. Specifically, we need to convert raster file into data frame with raster::as.data.frame(xy = TRUE) and specify the xy = TRUE argument. We then rename the x to lon, y to lat and convert bathmetry values from the double presion to integer and select values within the geographical extend of interest and depth between 0 and 1200 meter deep.\n\n## convert raster to data frame\ntz.bath.df = tz.bath %>% \n  raster::as.data.frame(xy = TRUE) %>%\n  dplyr::as_tibble()\n\n## rename the variable\ntz.bath.df = tz.bath.df %>% \n  dplyr::rename(lon = x, lat = y, depth = 3)%>% \n  dplyr::mutate(depth = as.integer(depth))\n\n## chop the area of interest \noff.kimbiji = tz.bath.df %>% \n  dplyr::filter(lon > 38.5 & lon < 40 & \n           lat > -7.2 & lat < - 6 & \n           depth > -1200& depth < 0  )\n\nThe bathmetry file now contain three variables, the lon, lat and depth as seen in table @ref(tab:tab91)\n\noff.kimbiji %>%\n  dplyr::sample_n(10) %>%\n  knitr::kable(col.names = c(\"Longitude\", \"Latitude\", \"Depth (meters)\"), digits = 3,\n               caption = \"Ten randomly selected points of bathmetry values off Kimbiji, Tanzania\", align = \"c\")%>%\n  kableExtra::column_spec(column = 2:3, width = \"3cm\")%>%\n  kableExtra::add_header_above(c(\"Coordinate (Degree)\" = 2,\"\"))\n\n\n\nTen randomly selected points of bathmetry values off Kimbiji, Tanzania\n \n\nCoordinate (Degree)\n\n\n  \n    Longitude \n    Latitude \n    Depth (meters) \n  \n \n\n  \n    39.600 \n    -6.700 \n    -549 \n  \n  \n    39.633 \n    -6.817 \n    -454 \n  \n  \n    39.700 \n    -6.900 \n    -500 \n  \n  \n    39.717 \n    -7.000 \n    -361 \n  \n  \n    39.750 \n    -6.750 \n    -387 \n  \n  \n    38.883 \n    -6.317 \n    -3 \n  \n  \n    39.483 \n    -6.700 \n    -385 \n  \n  \n    39.033 \n    -6.433 \n    -1 \n  \n  \n    39.733 \n    -6.117 \n    -1149 \n  \n  \n    39.000 \n    -6.400 \n    -2"
  },
  {
    "objectID": "maps.html#basemap",
    "href": "maps.html#basemap",
    "title": "20  Static Maps",
    "section": "20.4 Basemap",
    "text": "20.4 Basemap\nWe also need basemap—country boundary layer. We use the st_read() function from sf package to read the shapefile boundary layer. Since the layer is for the whole Africa, to reduce the processing time for ploting the map of africa, we use the st_crop() function to chop the area of interest.\n\nkimbiji = africa %>% sf::st_crop(xmin = 38.0, xmax = 40.5, ymin = -8, ymax = -5.5)"
  },
  {
    "objectID": "maps.html#creating-contour-map",
    "href": "maps.html#creating-contour-map",
    "title": "20  Static Maps",
    "section": "20.5 Creating contour map",
    "text": "20.5 Creating contour map\nOnce we have the data ready, we can tools in ggplot2 and add-on packages to create the bathmetry map off–Kimbiji located between longitude 38.5°E and 40.1°E and latitude 7.2°S and `r metR::LatLabel(-6.0). The code block below was used to create (fig901?).\n\n\n\n\n\nMap of Off-Kimbiji showing contour lines. The grey lines are contour at 50 m interval and the black line are contoured at 200 m intervals\n\n\n\n\nThere are fourteen lined of codes in the chunk to make figure @ref(fig:fig901). That’s a lot! Don’t get intimidated, I will explain in detail how each line of code work together to make this figure. As before, you start plotting ggplot2 with the ggplot() function as the first line. Surprisingly, the ggplot() is empty without any argument specified. When mapping with geom_sf() function in ggplot2 package, you are advised to leave the ggplot() function empty. This will allow the geom_sf() to label the axes with the appropriate geographical labelling for longitude and latitude. The second line of gode add a simple feature with a geom_sf() function from sf package. Note however, I specified the geom_sf() to fill the boundary layer with grey of 90 shade and the stroke with black colour.\n\nmap = ggplot()+\n  geom_sf(data = kimbiji, fill = \"grey90\", col = \"grey40\")\nmap \n\n\n\n\nnote that ggplot2 plot the map with default aesthetic settings. The plot background is filled with gray color and without stroke but the grids are white colored. The third line add the contour lines spaced at 50 meter intervals. Instead of using geom_contour() from ggplot2, the geom_contour2() from metR package was used. They both serve the same task.\n\nmap = map +\n  geom_sf(data = kimbiji, fill = \"grey90\", col = \"grey40\")+\n  metR::geom_contour2(data = off.kimbiji, \n               aes(x = lon, y = lat, z=depth), binwidth = 50, col = \"grey\")\nmap\n\n\n\n\nLike the third line, the fourth line add contour lines, but instead of spacing them into meters, these are spaced at 200 meters interval and are black in color.\n\nmap = map +\n  metR::geom_contour2(data = off.kimbiji, \n               aes(x = lon, y = lat, z=depth), binwidth = 200)\nmap\n\n\n\n\nThe fifth line add the label on contour spaced at 200 meter interval with geom_text_contour() function from metR package. Here is where you will find the useful of package like metR that extends the ggplot2, for which the current version (2.3.1.1) is unable.\n\nmap = map +\n  metR::geom_contour2(data = off.kimbiji, \n               aes(x = lon, y = lat, z=depth, label = ..level..), binwidth = 200, skip = 0)\nmap\n\n\n\n\nThe sixth line zoom the map to only the geographical extent we are interested with using the coord_sf() function from sf package. We could also use the coord_cartesin() to limit the area.\n\nmap = map +\n  coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))\nmap\n\n\n\n\nWe got a glimpse of the map now, let us use theme to make some changes. The background was set to white with panel.background = element_rect(fill = \"white\"), and removed grids with panel.grid = element_line(colour = NA) and change the font size of the axis label to 11 points with axis.text = element_text(size = 11). The theme_bw() just set the border of the plot to black with solid line.\n\nmap = map +\n  theme_bw()+\n  theme(panel.background = element_rect(fill = \"white\"),\n        panel.grid = element_line(colour = NA),\n        axis.text = element_text(size = 11))\nmap\n\n\n\n\nThe good thing to start making maps is with an understanding of the map elements. A static map can be composed of many different map elements. These include main map body, legend, title, scale indicator, orientation indicator, inset map and source or ancillary information. By increasing the font size of axis textual label to 11, the axes are cluttered. adding the scale can improve the labelling. scale_x_continuous(breaks = seq(39.2, 39.8, .2)) in line 9 force ggplot2 to label the x–axis four letter that are spaced with 0.2 latitude and scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4)) in line 10 label four digits of longitude.\n\nmap = map+\n  scale_x_continuous(breaks = seq(39.2, 39.8, .2))+\n  scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4))\n\nmap\n\n\n\n\nBecause the axes are abbreviated with longitude and latitude symbol, line 11 in the code remove the axes title label. Line 12 to 14 add textual label on the map with the annotate() function.\n\nmap = map +\n  theme(axis.title = element_blank())+\n  annotate(geom = \"text\", x = 39.28, y = -6.48, label = \"Zanzibar \\nChannel\")+\n  annotate(geom = \"text\", x = 39.5, y = -6.37, label = \"Unguja \\nIsland\")+\n  annotate(geom = \"text\", x = 39.3, y = -6.91, label = \"Dar es Salaam\")\n\nmap\n\n\n\n\nClose look of figure @ref(fig:fig901), the north arrrow and the scale bar are missing. The last two lines of our code inset the scalebar and north arrow on map using the annotation_scale() and ggspatial::annotation_north_arrow() functions from ggspatial package.\nIn a nutshell, making this map using ggplot2 and ancillary extensions used fiften line codes and hundred of arguments. This are very common task of making maps with the combination of tools from different packages.\n\nmap +\n  ggspatial::annotation_scale(height = unit(.35, \"cm\"), pad_x = unit(.5, \"cm\"),\n                              tick_height = unit(3, \"cm\"), pad_y = unit(.5, \"cm\"), text_cex = .85)+\n  ggspatial::annotation_north_arrow(location = \"tr\", width = unit(.75, \"cm\"), height = unit(1, \"cm\"))\n\n\n\n\nmetR package has geom_contour_fill() function that draw filled contour lines and geom_contour_tanaka(), which illunate contours with varying brithtness to create an illusion of relief. The code chunk to create highlighted filled contour using metR function can be written as;\n\nggplot()+\n  metR::geom_contour_fill(data = off.kimbiji, \n               aes(x = lon, y = lat, z=depth), na.fill = TRUE, show.legend = FALSE)+\n  metR::geom_contour_tanaka(data = off.kimbiji, \n               aes(x = lon, y = lat, z=depth))+\n  metR::geom_text_contour(data = off.kimbiji, aes(x = lon, y = lat, z=depth), \n                          rotate = TRUE, check_overlap = TRUE, size = 3.0)+\n  geom_sf(data = kimbiji, fill = \"grey90\", col = \"grey40\")+\n  coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+\n  theme_bw()+\n  theme(panel.background = element_rect(fill = \"white\"),\n        panel.grid = element_line(colour = NA),\n        axis.text = element_text(size = 11))+\n  scale_x_continuous(breaks = seq(39.2, 39.8, .2))+\n  scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4))+\n  scale_fill_gradientn(colours = oce::oce.colorsGebco(120))+\n  labs(x = NULL, y = NULL)+\n  annotate(geom = \"text\", x = 39.28, y = -6.48, label = \"Zanzibar \\nChannel\")+\n  annotate(geom = \"text\", x = 39.5, y = -6.37, label = \"Unguja \\nIsland\")+\n  annotate(geom = \"text\", x = 39.3, y = -6.91, label = \"Dar es Salaam\")+\n  ggspatial::annotation_scale(height = unit(.35, \"cm\"), pad_x = unit(.5, \"cm\"),\n                              tick_height = unit(3, \"cm\"), pad_y = unit(.5, \"cm\"), text_cex = .85)+\n  ggspatial::annotation_north_arrow(location = \"tr\", width = unit(.75, \"cm\"), height = unit(1, \"cm\"))"
  },
  {
    "objectID": "maps.html#inset-maps",
    "href": "maps.html#inset-maps",
    "title": "20  Static Maps",
    "section": "20.6 Inset maps",
    "text": "20.6 Inset maps\nAn inset map is a smaller map rendered within or next to the main map. It could serve many different purposes, including showing the relative position of the study area in regional area. In figure @ref(fig:fig9011) is the map showing the contour interval off-kimbiji, Tanzania. The inset map show the area of Kimbiji in the Western Indian Ocean Region. The chunk below was used to create figure @ref(fig:fig911). In a nutshell, we assign the study area map as main.map and the regional map as inset.map and then we used function from the cowplot package to combine the two maps.\n\nmain.map = ggplot()+\n  geom_sf(data = kimbiji, fill = \"grey90\", col = \"grey40\")+\n  metR::geom_contour2(data = off.kimbiji, \n               aes(x = lon, y = lat, z=depth), binwidth = 50, col = \"grey\")+\n  metR::geom_contour2(data = off.kimbiji, \n               aes(x = lon, y = lat, z=depth), binwidth = 200)+\n  metR::geom_text_contour(data = off.kimbiji, \n               aes(x = lon, y = lat, z=depth), \n               binwidth = 200, rotate = FALSE)+\n  coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+\n  theme_bw()+\n  theme(panel.background = element_rect(fill = \"white\"),\n        panel.grid = element_line(colour = NA),\n        axis.text = element_text(size = 11))+\n  scale_x_continuous(breaks = seq(39.2, 39.8, .2))+\n  scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4))+\n  labs(x = NULL, y = NULL)+\n  annotate(geom = \"text\", x = 39.28, y = -6.48, label = \"Zanzibar \\nChannel\")+\n  annotate(geom = \"text\", x = 39.5, y = -6.37, label = \"Unguja \\nIsland\")+\n  annotate(geom = \"text\", x = 39.3, y = -6.91, label = \"Dar es Salaam\")+\n  ggspatial::annotation_scale(location = \"bl\")+\n  ggspatial::annotation_north_arrow(location = \"tr\")\n\nworld = spData::world\naoi = data.frame(lon = c(38.5, 40, 40, 38.5, 38.5), \n                 lat = c(-8, -8, -6, -6, -8))\n\ninset.map = ggplot()+\n  geom_sf(data = world, fill = \"grey90\", col = 1)+\n  coord_sf(xlim = c(37, 45), ylim = c(-12,-1))+\n  geom_path(data = aoi, aes(x = lon, y = lat), size = 1.2)+\n  theme_bw()+\n  theme(plot.background = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(), panel.grid = element_line(colour = \"white\")) +\n  labs(x = NULL, y = NULL)\n\ncowplot::ggdraw()+\n  cowplot::draw_plot(plot = main.map, x = 0, y = 0, width = 1, height = 1, scale = 1)+\n  cowplot::draw_plot(plot = inset.map, x = .558, y = .05, width = .3,height = .3)\n\n\n\n\nFigure 20.1: The main map with the inset map showing the positon of the study areas in the region\n\n\n\n\n\nmain.map = ggplot()+\n  metR::geom_contour_fill(data = off.kimbiji, \n               aes(x = lon, y = lat, z=depth), na.fill = TRUE, show.legend = FALSE)+\n  metR::geom_contour_tanaka(data = off.kimbiji, \n               aes(x = lon, y = lat, z=depth))+\n  metR::geom_text_contour(data = off.kimbiji, \n               aes(x = lon, y = lat, z=depth), rotate = TRUE, check_overlap = TRUE, size = 3.4)+\n  geom_sf(data = kimbiji, fill = \"grey90\", col = \"grey40\")+\n  coord_sf(xlim = c(39.2, 39.8), ylim = c(-7, -6.3))+\n  theme_bw()+\n  theme(panel.background = element_rect(fill = \"white\"),\n        panel.grid = element_line(colour = NA),\n        axis.text = element_text(size = 11))+\n  scale_x_continuous(breaks = seq(39.2, 39.8, .2))+\n  scale_y_continuous(breaks = seq(-6.95, -6.35, length.out = 4))+\n  scale_fill_gradientn(colours = oce::oce.colorsGebco(120))+\n  labs(x = NULL, y = NULL)+\n  annotate(geom = \"text\", x = 39.28, y = -6.48, label = \"Zanzibar \\nChannel\")+\n  annotate(geom = \"text\", x = 39.5, y = -6.37, label = \"Unguja \\nIsland\")+\n  annotate(geom = \"text\", x = 39.3, y = -6.91, label = \"Dar es Salaam\")+\n  ggspatial::annotation_scale(location = \"bl\")+\n  ggspatial::annotation_north_arrow(location = \"tr\", width = unit(.75, \"cm\"))\n\nworld = spData::world\naoi = data.frame(lon = c(38.5, 40, 40, 38.5, 38.5), \n                 lat = c(-8, -8, -6, -6, -8))\n\ninset.map = ggplot()+\n  geom_sf(data = world, fill = \"grey90\", col = 1)+\n  coord_sf(xlim = c(37, 45), ylim = c(-12,-1))+\n  geom_path(data = aoi, aes(x = lon, y = lat), size = 1.2)+\n  theme_bw()+\n  theme(plot.background = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(), panel.grid = element_line(colour = \"white\")) +\n  labs(x = NULL, y = NULL)\n\ncowplot::ggdraw()+\n  cowplot::draw_plot(plot = main.map, x = 0, y = 0, width = 1, height = 1, scale = 1)+\n  cowplot::draw_plot(plot = inset.map, x = .558, y = .05, width = .3,height = .3)\n\n\n\n\nFigure 20.2: The main map with the inset map showing the positon of the study areas in the region\n\n\n\n\n\n20.6.1 Choropleth maps\nChloropleth maps use color or shading on predefined areas to map values of a numeric or categorical variable in that area. For example we are interested to map the different coral reefs in Jibondo Island, Mafia, Tanzania. First we import the data into R using the st_read() function from sf package (Pebesma 2018)\n\njibondo.reefs = sf::st_read(dsn = \"assets/shp//jibondo_reefs.shp\",quiet = TRUE)\n\nThe jibondo.reefs file is simple feature (equivalent to shapefile) with sixteeen polygons in four groups—coastal-shallow areas, islands, reef flat and submerged reefs. We use the variable type to map the different coastal features in Jibondo. The code used to make (Figure 20.3) is written as:\n\nrequire(RColorBrewer)\n\nggplot()+\n  geom_sf(data = jibondo.reefs, aes(fill = type)) +\n  coord_sf(xlim = c(39.57, 39.88), ylim = c(-8.15,-7.88)) +\n  geom_sf_text(data = jibondo.reefs, aes(label = mwamba), check_overlap = TRUE) +\n  theme_bw() %+%\n  theme(axis.text = element_text(size = 11), legend.position = c(.8,.18)) +\n  scale_fill_brewer(palette = \"Accent\") +\n  metR::scale_x_longitude(ticks = 0.1) +\n  metR::scale_y_latitude(ticks = 0.08) +\n  guides(fill = guide_legend(title = \"Reef Type\",\n                             title.position =  \"top\",\n                             keywidth = 1.1,\n                             ncol = 1))\n\n\n\n\nFigure 20.3: Reefs and non-reeef features in Jibondo Island, Mafia\n\n\n\n\nThe variable used to make (Figure 20.4) is a categorical, but we can also map continuous variables. For this case, we want to map the catch per unit effort (CPUE) of octopus at each reef to identify octopus catches at different reefs as seen in figure\n\nggplot()+\n  geom_sf(data = jibondo.reefs, \n          aes(fill = cpue %>%round(2) %>% as.factor()))+\n  coord_sf(xlim = c(39.57, 39.88), ylim = c(-8.15,-7.88))+\n  geom_sf_text(data = jibondo.reefs, aes(label = mwamba), check_overlap = TRUE) +\n  theme_bw() %+%\n  theme(axis.text = element_text(size = 11), legend.position = c(.9,.25)) +\n  # scale_fill_brewer(palette = \"Accent\") +\n  ggsci::scale_fill_d3()+\n  metR::scale_x_longitude(ticks = 0.1) +\n  metR::scale_y_latitude(ticks = 0.08)+\n  guides(fill = guide_legend(title.position =  \"top\",\n                             keywidth = 1.1,\n                             ncol = 1, \n                             title = \"CPUE\"))\n\n\n\n\nFigure 20.4: Reefs and non-reeef features in Jibondo Island, Mafia\n\n\n\n\nFinally let’s map the the spatial patterns of sea surface temperature anomaly. We plot the departure of sea surface temperature from zonal average mean. Let’s import the dataset from the workspace. For more information on how to compute the zonal departure see chapter…\n\ntemperature.anomaly = read_csv(\"assets//shp/sst_anomaly.csv\")\n\n\n\n\n\n \n\nCoordinate (Degree)\n\n\n\n  \n    Latitude \n    Longitude \n    Depth \n    Anomaly \n  \n \n\n  \n    60.5 \n    -52.5 \n    0 \n    -1.99 \n  \n  \n    -35.5 \n    165.5 \n    0 \n    0.66 \n  \n  \n    50.5 \n    148.5 \n    0 \n    -3.83 \n  \n  \n    20.5 \n    -155.5 \n    0 \n    -0.84 \n  \n  \n    -50.5 \n    -95.5 \n    0 \n    1.78 \n  \n  \n    -40.5 \n    71.5 \n    0 \n    0.08 \n  \n  \n    -15.5 \n    86.5 \n    0 \n    0.53 \n  \n  \n    85.5 \n    -109.5 \n    0 \n    -0.03 \n  \n  \n    85.5 \n    -126.5 \n    0 \n    -0.08 \n  \n  \n    0.5 \n    6.5 \n    0 \n    -0.16 \n  \n\n\n\n\n\n\nggplot() +\n  metR::geom_contour_fill(data = temperature.anomaly,\n                          aes(x = lon, y = lat, z = anomaly), na.fill = T) +\n  geom_sf(data =  spData::world, col = NA, fill = \"grey40\")+\n  coord_sf(xlim = c(-180,178), ylim = c(-90,85), clip = \"on\", expand = FALSE)+\n  scale_fill_gradientn(colours = oce::oce.colors9A(120), breaks = seq(-6,6,2))+\n  theme_bw() +\n  theme(legend.position = \"right\", panel.background = element_blank(),\n        axis.text = element_text(size = 11, colour = \"black\"), \n        legend.text = element_text(size = 10),\n        legend.title = element_text(size = 11))+\n  guides(fill = guide_colorbar(title = expression(Temperature~anomaly~(degree*C)),\n                               title.position = \"right\", \n                               title.hjust = 0.5, \n                               title.theme = element_text(angle = 90),\n                               label.theme = element_text(size = 10),\n                               direction = \"vertical\",\n                               reverse = FALSE, raster = FALSE,\n                               barwidth = unit(.4, \"cm\"),\n                               barheight = unit(6.5, \"cm\")))+\n  metR::scale_y_latitude(ticks = 30) + \n  metR::scale_x_longitude(ticks = 45)+\n  labs(x = NULL, y = NULL)\n\n\n\n\nFigure 20.5: Departure of the sea surfae temprature at each location from the zonally averaged field\n\n\n\n\n\n\n\n\nAuguie, Baptiste. 2018. Egg: Extensions for ’Ggplot2’: Custom Geom, Plot Alignment, Symmetrised Scale, and Fixed Panel Size. https://CRAN.R-project.org/package=egg.\n\n\nCampitelli, Elio. 2021. metR: Tools for Easier Analysis of Meteorological Fields. https://doi.org/10.5281/zenodo.2593516.\n\n\nPebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\nPedersen, Thomas Lin, and David Robinson. 2017. Gganimate: A Grammar of Animated Graphics. http://github.com/thomasp85/gganimate.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\nWilke, Claus O. 2018. Cowplot: Streamlined Plot Theme and Plot Annotations for ’Ggplot2’. https://CRAN.R-project.org/package=cowplot."
  },
  {
    "objectID": "mapsDynamic.html",
    "href": "mapsDynamic.html",
    "title": "21  Animated maps",
    "section": "",
    "text": "A web map is an interactive display of geographic information, in the form of a web page, that you can use to tell stories and answer questions. In the past, most digital geographic information was confined to specialized software on desktop PCs and could not be easily shared. With the advent of web mapping, geographical information can be shared, visualized, and edited in the browser. The most important advantage to this is accessibility: a web map, just like any website, can be reached by anyone from any device that has an internet browser and an internet connection.\nWeb maps are interactive. The term interactive implies that the viewer can interact with the map. This can mean selecting different map data layers or features to view, zooming into a particular part of the map that you are interested in, inspecting feature properties, editing existing content, or submitting new content, and so on.\nWeb maps are also said to be powered by the web, rather than just digital maps on the web. This means that the map is usually not self-contained; in other words, it depends on the internet. At least some of the content displayed on a web maps is usually loaded from other locations on the web, such as a tile server (Section 6.5.10.2).\nWeb maps are useful for various purposes, such as data visualization in journalism (and elsewhere), displaying real-time spatial data, powering spatial queries in online catalogs and search tools, providing computational tools, reporting, and collaborative mapping."
  },
  {
    "objectID": "mapsDynamic.html#what-is-leaflet",
    "href": "mapsDynamic.html#what-is-leaflet",
    "title": "21  Animated maps",
    "section": "21.2 What is Leaflet?",
    "text": "21.2 What is Leaflet?\nLeaflet is an open-source JavaScript library for building interactive web maps (Cheng, Karambelkar, and Xie 2018). Leaflet was initially released in 2011 (Table 6.1). It is lightweight, relatively simple, and flexible. Leaflet is probably the most popular open-source web-mapping library at the moment. As the Leaflet home page puts it, the guiding principle behind this library is simplicity:\n“Leaflet doesn’t try to do everything for everyone. Instead it focuses on making the basic things work perfectly.”\nAdvanced functionality is still available through Leaflet plugins. Towards the end of the book, we will learn about two Leaflet plugins: Leaflet.heat (Section 12.6) and Leaflet.draw (Section 13.3).\n\nrequire(tidyverse)\nrequire(leaflet)\nrequire(tmap)\nrequire(tidyterra)\n\ntmap_mode(mode = \"view\")\n\n\nraster.files = list.files(\"assets//raster/wc2.1_10m_tavg/\", pattern = \".tif\", full.names = TRUE)\n\n\nmonths = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \n           \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n\nclimatology.temperature = list()\n\nfor (i in 1:length(raster.files)){\n  \n  \n  climatology.temperature[[i]] = raster.files[i] %>% \n    raster::raster() %>%\n    # wior::raster_tb() %>%\n    raster::as.data.frame(xy = TRUE) %>%\n    rename(lon = 1, lat = 2, z = 3) %>% \n    mutate(month = months[i])\n}\n\nclimatology.temperature = climatology.temperature %>% \n  bind_rows() \n\n\n21.2.1 Creating a basic web map\nIn this section, we will learn to create a basic web map using Leaflet. The map is going to contain a single background (tile) layer, initially zoomed-in on the Ben-Gurion University. The final result is shown in Figure 6.5.\n\nrequire(leaflet)\n\n\n## read the file\njan.raster = raster.files[1] %>%\n  raster::raster()%>%\n  raster::projectRaster(crs = 4326)\n\n## create color pallet\npal = colorNumeric(c(\"#7f007f\", \"#0000ff\",  \"#007fff\", \"#00ffff\", \"#00bf00\", \"#7fdf00\",\n\"#ffff00\", \"#ff7f00\", \"#ff3f00\", \"#ff0000\", \"#bf0000\"), raster::values(jan.raster),  na.color = \"transparent\")\n\n\n## interactive map of temperature\nleaflet() %>%\n  addTiles() %>%\n  setView(lng = 5, lat = 20, zoom = 2) %>% \n  addRasterImage(x = jan.raster, colors = pal, opacity = .8)  %>%\n  addLegend(pal = pal, values = raster::values(jan.raster),\n    title = \"Temperature\")\n\n\n\n\n\nFigure 21.1: Interactive map showing a spatial variation of global land surface temperature for January\n\n\n\n\n## read the file\naug.raster = raster.files[8] %>%\n  raster::raster() %>%\n  raster::projectRaster(crs = 4326)\n\naug.raster[aug.raster< -15] = NA\n\n## create color pallet\npal = colorNumeric(c(\"#7f007f\", \"#0000ff\",  \"#007fff\", \"#00ffff\", \"#00bf00\", \"#7fdf00\",\n\"#ffff00\", \"#ff7f00\", \"#ff3f00\", \"#ff0000\", \"#bf0000\"), raster::values(aug.raster),  na.color = \"transparent\")\n\n\n## interactive map of temperature\nleaflet() %>%\n  addTiles() %>%  \n  setView(lng = 5, lat = 20, zoom = 2) %>% \n  addRasterImage(x = aug.raster, colors = pal, opacity = .6)  %>%\n  addLegend(pal = pal, values = raster::values(aug.raster),\n    title = \"Temperature\")\n\n\n\n\nFigure 21.2: Interactive map showing a spatial variation of global land surface temperature for August\n\n\n\n\nrast.temp = raster.files[1:12] %>% \n  terra::rast() %>% \n  rename(Jan = 1)%>% \n  rename(Feb = 2)%>% \n  rename(Mar = 3)%>% \n  rename(Apr = 4)%>% \n  rename(May = 5)%>% \n  rename(Jun = 6)%>% \n  rename(Jul = 7)%>% \n  rename(Aug = 8)%>% \n  rename(Sep = 9)%>% \n  rename(Oct = 10)%>% \n  rename(Nov = 11)%>% \n  rename(Dec = 12)\n\n\nggplot() +\n  geom_spatraster(data = rast.temp)+\n  facet_wrap(~lyr, nrow = 4)+\n  scale_fill_whitebox_c(\n    palette = \"muted\",\n    na.value = \"white\")+\n  scale_fill_gradientn(colours = oce::oce.colors9A(120))+\n  theme_minimal() +\n  theme(strip.background = element_blank())\n\n\n\n\nFigure 21.3: Climatology of temperature\n\n\n\n\n\nafrica = spData::world %>% filter(continent == \"Africa\") %>% terra::vect()\n\n\nafrica.temp = rast.temp %>% \n  terra::crop(africa) %>% \n  terra::mask(africa)\n\nggplot() +\n  geom_spatraster(data = africa.temp)+\n  metR::geom_contour2()+\n  facet_wrap(~lyr, nrow = 3)+\n  scale_fill_whitebox_c(\n    palette = \"muted\",\n    na.value = \"white\")+\n  scale_fill_gradientn(colours = oce::oce.colors9A(120))+\n  theme_minimal() +\n  theme(strip.background = element_blank())"
  },
  {
    "objectID": "mapsDynamic.html#for-web",
    "href": "mapsDynamic.html#for-web",
    "title": "21  Animated maps",
    "section": "21.3 for web",
    "text": "21.3 for web\n\nafrica = spData::world %>% filter(continent == \"Africa\") %>% terra::vect()\n\n# surface temperaature (degree)\ntemp.files = list.files(\"assets/raster/wc2.1_10m_tavg/\", pattern = \".tif\", full.names = TRUE)\n\nrast.temp = temp.files[1:12] %>% \n  terra::rast() %>% \n  rename(Jan = 1)%>% \n  rename(Feb = 2)%>% \n  rename(Mar = 3)%>% \n  rename(Apr = 4)%>% \n  rename(May = 5)%>% \n  rename(Jun = 6)%>% \n  rename(Jul = 7)%>% \n  rename(Aug = 8)%>% \n  rename(Sep = 9)%>% \n  rename(Oct = 10)%>% \n  rename(Nov = 11)%>% \n  rename(Dec = 12)\n\ntemperature = rast.temp %>% \n  terra::crop(africa) %>% \n  terra::mask(africa)\n\n# surface wind speed (m/s)\nwind.files = list.files(\"assets/raster/wc2.1_10m_wind/\", pattern = \".tif\", full.names = TRUE)\n\nrast.wind = temp.files[1:12] %>% \n  terra::rast() %>% \n  rename(Jan = 1)%>% \n  rename(Feb = 2)%>% \n  rename(Mar = 3)%>% \n  rename(Apr = 4)%>% \n  rename(May = 5)%>% \n  rename(Jun = 6)%>% \n  rename(Jul = 7)%>% \n  rename(Aug = 8)%>% \n  rename(Sep = 9)%>% \n  rename(Oct = 10)%>% \n  rename(Nov = 11)%>% \n  rename(Dec = 12)\n\nwind = rast.wind %>% \n  terra::crop(africa) %>% \n  terra::mask(africa)\n\n# water vapour (kPa)\ntemp.files = list.files(\"assets/raster/wc2.1_10m_vapr/\", pattern = \".tif\", full.names = TRUE)\n\nrast.water = temp.files[1:12] %>% \n  terra::rast() %>% \n  rename(Jan = 1)%>% \n  rename(Feb = 2)%>% \n  rename(Mar = 3)%>% \n  rename(Apr = 4)%>% \n  rename(May = 5)%>% \n  rename(Jun = 6)%>% \n  rename(Jul = 7)%>% \n  rename(Aug = 8)%>% \n  rename(Sep = 9)%>% \n  rename(Oct = 10)%>% \n  rename(Nov = 11)%>% \n  rename(Dec = 12)\n\nwatervapor = rast.water %>% \n  terra::crop(africa) %>% \n  terra::mask(africa)\n\n# solar radiations\ntemp.files = list.files(\"assets/raster/wc2.1_10m_srad/\", pattern = \".tif\", full.names = TRUE)\n\nrast.radiation = temp.files[1:12] %>% \n  terra::rast() %>% \n  rename(Jan = 1)%>% \n  rename(Feb = 2)%>% \n  rename(Mar = 3)%>% \n  rename(Apr = 4)%>% \n  rename(May = 5)%>% \n  rename(Jun = 6)%>% \n  rename(Jul = 7)%>% \n  rename(Aug = 8)%>% \n  rename(Sep = 9)%>% \n  rename(Oct = 10)%>% \n  rename(Nov = 11)%>% \n  rename(Dec = 12)\n\n\n\nradition = rast.radiation %>% \n  terra::crop(africa) %>% \n  terra::mask(africa)\n\n\ntemp.data = temperature %>% \n  select(data = Jul)\n\ntemp.tb = temp.data %>% \n  terra::as.data.frame(xy = TRUE)\n\n\ndata.stats = temp.tb %>%  \n  select(data) %>% \n    ggpubr::get_summary_stats(type = \"common\")\n\ntemp.tb %>% \n  sample_n(5000) %>% \n  ggplot(aes(x = data))+\n  geom_histogram(bins = 30, color = \"ivory\", fill = \"cyan4\", alpha = .4)+\n  geom_vline(xintercept = data.stats$median, color = \"red\", linetype = 2, lwd = 1.3)\n\n\n\ntm_shape(shp = temp.data)+\n  tm_raster(col = \"values\", style = \"fisher\", \n            palette = oce::oce.colorsJet(120), n = 60, \n            title = \"Temperature <br> Degree Celcius\",  \n            midpoint = data.stats$median, \n            legend.show = FALSE)\n\n\n21.3.1 What are tile layers?\n\n21.3.1.1 Overview\nTile layers are a fundamental technology behind web maps. They comprise the background layer in most web maps, thus helping the viewer to locate the foreground layers in geographical space. The word tile in tile layers comes from the fact that the layer is split into individual rectangular tiles. Tile layers come in two forms, which we are going to cover next: raster tiles (Section 6.5.10.2) and vector tiles (Section 6.5.10.3).\n\n\n21.3.1.2 Raster tiles\nThe oldest and simplest tile layer type is where tiles are raster images, also known as raster tiles. With raster tiles, tile layers are usually composed of PNG images. Traditionally, each PNG image is 256×256 pixels in size. A separate collection of tiles is required for each zoom level the map can be viewed on, with increasing numbers of tiles needed to cover a (global) extent in higher zoom levels. Conventionally, at each sequential zoom level, all of the tiles are divided into four “new” ones (Figure 6.2). For example, for covering the world at zoom level 0, we need just one tile. When we go to zoom level 1, that individual tile is split to 2×2=4 separate tiles. When we go further to zoom level 2, each of the four tiles is also split to four, so that we already have 4×4=16 separate tiles, and so on. In general, a global tile layer at zoom level z contains 2z×2z=4z tiles. At the default maximal zoom level in a Leaflet map (19), we need 274,877,906,944 tiles to cover the earth54!\n\n\n21.3.1.3 Vector tiles\nA more recent tile layer technology is where tiles are vector layers, rather than PNG images, referred to as vector tiles. Vector tiles are distinguished by the ability to rotate the map while the labels keep their horizontal orientation, and by the ability to zoom in or out smoothly—without the strict division to discrete zoom levels that raster tile layers have (Figure 6.2). Major advantages of vector tiles are their smaller size and flexible styling. For example, Google Maps made the transition from raster to vector tiles in 2013.\nThe Leaflet library does not natively support vector tiles, though there is a plugin called Leaflet.VectorGrid for that. Therefore, in this book we will restrict ourselves to using raster tiles as background layers. There are other libraries specifically built around vector tile layers, such as the Google Maps API and Mapbox GL JS, which we mentioned previously (Section 6.4). The example-06-01.html shows a web map with a vector tile layer built with Mapbox GL JS (Figure 6.4). This is the only non-Leaflet web-map example that we are going to see in the book; it is provided for demonstration and comparison of raster and vector tiles.\n\n\n21.3.1.4 Adding a tile layer\nWe now go back to discussing Leaflet and raster tile layers. Where can we get a tile layer from? There are many tile layers prepared and provided by several organizations, available on dedicated servers (Section 6.5.12) that you can add to your maps. Most of them are based on OpenStreetMap data (Section 13.2), because it is the most extensive free database of map data with global coverage. The tile layer we use in the following examples, and the one that the tile shown in Figure 6.3 comes from, is developed and maintained by OpenStreetMap itself. It is the default tile layer displayed on the https://www.openstreetmap.org/ website.\n\n\n\n\nCheng, Joe, Bhaskar Karambelkar, and Yihui Xie. 2018. Leaflet: Create Interactive Web Maps with the JavaScript ’Leaflet’ Library. https://CRAN.R-project.org/package=leaflet."
  },
  {
    "objectID": "keyobjects.html",
    "href": "keyobjects.html",
    "title": "Appendix A — Key objects",
    "section": "",
    "text": "An important strength of R is that it is very rich in the types of objects that it supports. That strength is rather a disadvantage when you are first learning R.\nBut to start, you only need to get your head around a few types of objects."
  },
  {
    "objectID": "keyobjects.html#key-actions",
    "href": "keyobjects.html#key-actions",
    "title": "Appendix A — Key objects",
    "section": "A.1 Key actions",
    "text": "A.1 Key actions\nThree basic actions in R are assignment, subscripting and random generation.\n\nA.1.1 assignment\nThe action in R is precipitated by function calls. Most functions return a value (that is, some data object). You will often want to assign that result to a name. There are two ways of doing that. You can do:\n\n meanx <- mean(x)\n\nor\n\nmeanx = mean(x)\n\nOnce you have executed one of those commands, then meanx will be an object in your global environment.\nThere is a shocking amount of controversy over which form of assignment to use. The position I’ll take here is to say to use whichever one you are more comfortable with. There are ways of running into trouble with either one, but using the arrow surrounded by spaces is probably the safest approach by a slight margin.\nNote that R is case-sensitive. The two names meanx and Meanx are different.\n\n\nA.1.2 subscripting\nSubscripting is important. This is the act of extracting pieces from objects. Subscripting is done with square brackets:\n\nx[1]\n\nextracts the first element from x.\n\nx[1,3]\n\nextracts the element in the first row and third column of a matrix or data frame.\nSubscripting also includes replacing pieces of an object. The command:\n\nx[1] = 9\n\nwill change the first element of x to 9."
  },
  {
    "objectID": "keyobjects.html#random-generation",
    "href": "keyobjects.html#random-generation",
    "title": "Appendix A — Key objects",
    "section": "A.2 random generation",
    "text": "A.2 random generation\nThere is a variety of functions that produce randomness. For example, the command:\n\nrunif(n = 9)\n\ncreates a vector of 9 numbers that are uniformly distributed between 0 and 1. You will get different answers from this command if you do it again.\nrunif function is also used to randomly generate element of vector when the minimum and maximum values are provided.\n\nrunif(n = 9, min = 20, max = 100)"
  },
  {
    "objectID": "keyobjects.html#probability-distributions",
    "href": "keyobjects.html#probability-distributions",
    "title": "Appendix A — Key objects",
    "section": "A.3 Probability distributions",
    "text": "A.3 Probability distributions\nR has functions for a number of probability distributions. In general, there are four functions for each distribution as shown in Table A.1.\n\n\n\n\nTable A.1:  Prefix name of probability distributions functions in R \n \n  \n    Function name \n    Description \n  \n \n\n  \n    rxxx \n    random generation \n  \n  \n    dxxx \n    density function \n  \n  \n    pxxx \n    cumulative probability function \n  \n  \n    qxxx \n    quantile function \n  \n\n\n\n\n\n\nFor example rnorm is the random generation function for the normal distribution. dnorm is the density for the normal. pnorm is the cumulative probability function for the normal — that is, this gives the probability of being less than or equal to a given quantile. qnorm is the quantile function — the inverse of the probability function (that is, it returns a quantile given a probability).\n\n\n\n\nTable A.2:  Probility distribution functions in R \n \n  \n    Distribution \n    Functioons \n  \n \n\n  \n    Uniform \n    runif, dunif, punif, qunif \n  \n  \n    Normal \n    rnorm, dnorm, pnorm, qnorm \n  \n  \n    Student’s t \n    rt, dt, pt, qt \n  \n  \n    F \n    rf, df, pf, qf \n  \n  \n    Exponential \n    rexp, dexp, pexp, qexp \n  \n  \n    Log normal \n    rlnorm, dlnorm, plnorm, qlnorm \n  \n  \n    Beta \n    rbeta, dbeta, pbeta, qbeta \n  \n  \n    Binomial \n    rbinom, dbinom, pbinom, qbinom \n  \n  \n    Poisson \n    rpois, dpois, ppois, qpois \n  \n\n\n\n\n\n\nYou can see a more complete list with the command:\n\n??distributions\n\nThe ecdf function takes a data vector as an argument and returns a function that is the cumulative probability function of the data.\nMany contributed packages contain functions for additional distributions."
  },
  {
    "objectID": "keyobjects.html#pseudorandomness",
    "href": "keyobjects.html#pseudorandomness",
    "title": "Appendix A — Key objects",
    "section": "A.4 Pseudorandomness",
    "text": "A.4 Pseudorandomness\nIn a certain sense most of what is said on this page is a lie. When you use a function like rnorm or sample, you are not generating randomness at all. These are pseudorandom functions. Technically you are generating chaos when you use them, not randomness. There are two main reasons to use pseudorandomness rather than randomness.\n\nThe first is convenience. In the early days of computing there was no way to actually get true random values, so they had to invent pseudorandom methods. Now there is the possibility of using truly random values, but it is generally harder to do and seldom offers an advantage.\nThe second reason to prefer pseudorandomness is reproducibility. Random numbers (by definition) are not reproducible. A program without reproducible results is a program that can not be debugged.\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is largely accidental that we have pseudorandom functions and not truly random functions. It\\’s a happy accident."
  },
  {
    "objectID": "keyobjects.html#errors-and-such",
    "href": "keyobjects.html#errors-and-such",
    "title": "Appendix A — Key objects",
    "section": "A.5 Errors and such",
    "text": "A.5 Errors and such\nSometime, probably soon, you are going to get an error in R.\nHint: the universe doesn’t collapse into a singularity just because of an error in R. Actually, it builds character — see Make mistakes on purpose.\nR produces errors and warnings. Both errors and warnings write a message — the difference is that errors halt the execution of the command but warnings do not.\nWe’ll categorize errors into three types: syntax errors, object-not-found errors, and all the rest.\n\nA.5.1 syntax errors\nIf you get a syntax error, then you’ve entered a command that R can’t understand. Generally the error message is pretty good about pointing to the approximate point in the command where the error is.\nCommon syntax mistakes are missing commas, unmatched parentheses, and the wrong type of closing brace for example, an opening square bracket but a closing parenthesis).\n\n\nA.5.2 object not found\nErrors of the object-not-found variety can have one of several causes:\n\nthe name is not spelled correctly, or the capitalization is wrong\nthe package or file containing the object is not on the search list\nsomething else (let your imagination run wild)\n\n\n\nA.5.3 other errors\nThere are endless other ways of getting an error. Hence some detective work is generally necessary — think of it as a crossword puzzle that needs solving.\nIt should become a reflex reaction to type:\nThe results might not mean much to you at the moment, but they will at some point. The traceback tells you what functions were in effect at the time of the error. This can give you a hint of what is going wrong.\n\n\n\n\n\n\nImportant\n\n\n\ntraceback()\nwhenever you get an error.\n\n\n\n\nA.5.4 warnings\nA warning is not as serious as an error in that the command runs to completion. But that can mean that ignoring a warning can be very, very serious if it is suggesting to you that the answer you got was bogus.\nIt is good policy to understand warning messages to see if they indicate a real problem or not."
  },
  {
    "objectID": "lbspr.html",
    "href": "lbspr.html",
    "title": "19  Length Based Spawning Potential Ratio",
    "section": "",
    "text": "require(tidyverse)\nrequire(TropFishR)\nrequire(magrittr)\nrequire(LBSPR)\nrequire(hrbrthemes)\nrequire(patchwork)\nrequire(sf)\n\noptions(digits = 3)"
  },
  {
    "objectID": "lbspr.html#references",
    "href": "lbspr.html#references",
    "title": "19  Length Based Spawning Potential Ratio",
    "section": "References",
    "text": "References\n\n\n\n\nAhmadi, Mohammad Sadeq, Janez Sušnik, William Veerbeek, and Chris Zevenbergen. 2020. “Towards a Global Day Zero? Assessment of Current and Future Water Supply and Demand in 12 Rapidly Developing Megacities.” Sustainable Cities and Society 61: 102295.\n\n\nBoard, Ocean Studies, National Research Council, et al. 2010. Ocean Acidification: A National Strategy to Meet the Challenges of a Changing Ocean. National Academies Press.\n\n\nBoettiger, Carl, Duncan Temple Lang, and Peter Wainwright. 2012. “Rfishbase: Exploring, Manipulating and Visualizing FishBase Data from r.” Journal of Fish Biology, November. https://doi.org/10.1111/j.1095-8649.2012.03464.x.\n\n\nBown, Natalie K, Tim S Gray, and Selina M Stead. 2013. “Co-Management and Adaptive Co-Management: Two Modes of Governance in a Honduran Marine Protected Area.” Marine Policy 39: 128–34.\n\n\nBradley, Darcy, Matt Merrifield, Karly M. Miller, Serena Lomonico, Jono R. Wilson, and Mary G. Gleason. 2019. “Opportunities to Improve Fisheries Management Through Innovative Technology and Advanced Data Systems.” Journal Article. Fish and Fisheries 20 (3): 564–83. https://doi.org/https://doi.org/10.1111/faf.12361.\n\n\nBrey, Thomas, and Daniel Pauly. 1986. “Electronic Length Frequency Analysis: A Revised and Expanded User’s Guide to ELEFAN 0, 1 and 2.”\n\n\nCampitelli, Elio. 2021. metR: Tools for Easier Analysis of Meteorological Fields. https://doi.org/10.5281/zenodo.2593516.\n\n\nDick, EJ, and Alec D MacCall. 2011. “Depletion-Based Stock Reduction Analysis: A Catch-Based Method for Determining Sustainable Yields for Data-Poor Fish Stocks.” Fisheries Research 110 (2): 331–41.\n\n\nGayanilo, FC, and Daniel Pauly. 1997. FAO-ICLARM Stock Assessment Tools: Reference Manual. Vol. 8. Food; Agriculture Organization of the United Nations Rome.\n\n\nGoetz, Andrew R. 2019. “Transport Challenges in Rapidly Growing Cities: Is There a Magic Bullet?” Taylor & Francis.\n\n\nGolden, Christopher D, Edward H Allison, William WL Cheung, Madan M Dey, Benjamin S Halpern, Douglas J McCauley, Matthew Smith, Bapu Vaitla, Dirk Zeller, and Samuel S Myers. 2016. “Nutrition: Fall in Fish Catch Threatens Human Health.” Journal Article. Nature News 534 (7607): 317.\n\n\nGrolemund, Garrett, and Hadley Wickham. 2011. “Dates and Times Made Easy with lubridate.” Journal of Statistical Software 40 (3): 1–25. https://www.jstatsoft.org/v40/i03/.\n\n\nHilborn, Ray, Ricardo Oscar Amoroso, Christopher M Anderson, Julia K Baum, Trevor A Branch, Christopher Costello, Carryn L De Moor, et al. 2020. “Effective Fisheries Management Instrumental in Improving Fish Stock Status.” Proceedings of the National Academy of Sciences 117 (4): 2218–24.\n\n\nHordyk, Adrian. 2019. LBSPR: Length-Based Spawning Potential Ratio. https://CRAN.R-project.org/package=LBSPR.\n\n\nJackson, Jeremy BC, Michael X Kirby, Wolfgang H Berger, Karen A Bjorndal, Louis W Botsford, Bruce J Bourque, Roger H Bradbury, Richard Cooke, Jon Erlandson, and James A Estes. 2001. “Historical Overfishing and the Recent Collapse of Coastal Ecosystems.” Journal Article. Science 293 (5530): 629–37.\n\n\nJiao, Yan, Enric Cortés, Kate Andrews, and Feng Guo. 2011. “Poor-Data and Data-Poor Species Stock Assessment Using a Bayesian Hierarchical Approach.” Ecological Applications 21 (7): 2691–2708.\n\n\nMildenberger, Tobias K., Marc H. Taylor, and Matthias Wolff. 2017. “TropFishR: An r Package for Fisheries Analysis with Length-Frequency Data.” Methods in Ecology and Evolution 8 (11): 1520–27. https://doi.org/10.1111/2041-210X.12791.\n\n\nPatil, Indrajeet. 2021. “Visualizations with statistical details: The ’ggstatsplot’ approach.” Journal of Open Source Software 6 (61): 3167. https://doi.org/10.21105/joss.03167.\n\n\nPauly, Daniel. 1981. “The Relationships Between Gill Surface Area and Growth Performance in Fish: A Generalization of von Bertalanffy’s Theory of Growth.”\n\n\n———. 1982. “Studying Single-Species Dynamics in a Tropical Multispecies Context.” In Theory and Management of Tropical Fisheries. ICLARM Conference Proceedings, 9:33–70. 360. International Centre for Living Aquatic Resources Manila, Philippines.\n\n\nPebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\nPedersen, Thomas Lin. 2020. Patchwork: The Composer of Plots. https://CRAN.R-project.org/package=patchwork.\n\n\nPierce, David. 2019. Ncdf4: Interface to Unidata netCDF (Version 4 or Earlier) Format Data Files. https://CRAN.R-project.org/package=ncdf4.\n\n\nPope, John G, Dave S Macdonald, Niels Daan, John D Reynolds, and Simon Jennings. 2000. “Gauging the Impact of Fishing Mortality on Non-Target Species.” ICES Journal of Marine Science 57 (3): 689–96.\n\n\nPowell, David G. 1979. “Estimation of Mortality and Growth Parameters from the Length Frequency of a Catch [Model].” Rapports Et Proces-Verbaux Des Reunions (Denmark).\n\n\nPunt, André E, Michael G Dalton, Wei Cheng, Albert J Hermann, Kirstin K Holsman, Thomas P Hurst, James N Ianelli, et al. 2021. “Evaluating the Impact of Climate and Demographic Variation on Future Prospects for Fish Stocks: An Application for Northern Rock Sole in Alaska.” Deep Sea Research Part II: Topical Studies in Oceanography 189: 104951.\n\n\nPunt, André E, David C Smith, and Anthony DM Smith. 2011. “Among-Stock Comparisons for Improving Stock Assessments of Data-Poor Stocks: The ‘Robin Hood’ Approach.” ICES Journal of Marine Science 68 (5): 972–81.\n\n\nR Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nSemba, Masumbuko, and Nyamisi Peter. 2020. Wior: Easy Tidy and Process Oceanographic Data.\n\n\nShen, Xinqiang, K Richardson, E Nielsen, S Mellergaard, and Charlotten1und Cast1e. n.d. “International Council for the Exploration of the Seas.”\n\n\nTorrejon-Magallanes, Josymar. 2020. sizeMat: Estimate Size at Sexual Maturity. https://CRAN.R-project.org/package=sizeMat.\n\n\nWetherall, JA, JJ Polovina, and S Ralston. 1987. “Estimating Growth and Mortality in Steady-State Fish Stocks from Length-Frequency Data.” In ICLARM Conf. Proc, 13:53–74.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2021. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n\n\nWickham, Hadley, and Jennifer Bryan. 2019. Readxl: Read Excel Files. https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2021. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, and Jim Hester. 2020. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nXie, Yihui. 2014. “Knitr: A Comprehensive Tool for Reproducible Research in R.” In Implementing Reproducible Computational Research, edited by Victoria Stodden, Friedrich Leisch, and Roger D. Peng. Chapman; Hall/CRC. http://www.crcpress.com/product/isbn/9781466561595.\n\n\n———. 2016. Bookdown: Authoring Books and Technical Documents with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/bookdown.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook.\n\n\nZhu, Hao. 2021. kableExtra: Construct Complex Table with ’Kable’ and Pipe Syntax. https://CRAN.R-project.org/package=kableExtra."
  },
  {
    "objectID": "lbspr.html#introduction",
    "href": "lbspr.html#introduction",
    "title": "19  Length Based Spawning Potential Ratio",
    "section": "19.1 Introduction",
    "text": "19.1 Introduction\nOverall purpose of fisheries science is to provide decision-markers with advice on the relative merits of alternative management (Bradley et al. 2019). Fisheries managers recognize that stock assessments of species that have poor data (quality is poor) or are data poor (data are limited in quantity) are very challenging. Because of this dual limitations, assessments of the status of exploited fish stock are often highly uncertain. However, to achieve an effective fisheries management require a better understanding of life history information of fishery stock (Golden et al. 2016; Jackson et al. 2001). According to Bown, Gray, and Stead (2013), fisheries management is a complex socio-political process, and there is no silver bullet to improve global fisheries. Yet understanding the population biology of fish species is essential to meet one of the main objective of fishery science of maximizing yield to fisheries, while safeguarding the long-term viability of populations and ecosystem (Pope et al. 2000).\nRapidly global population growth (Goetz 2019; Ahmadi et al. 2020) and changing ocean conditions (Board, Council, et al. 2010) caused by climate change and natural variability have posed a threat on fishery resources (Bradley et al. 2019). This challenges demands fisheries an adjust of biological reference points (Hilborn et al. 2020). However, to address growing uncertainty about stock status, requires more precise data collection of high resolution spatial and temporal fisheries data (Punt et al. 2021). But, the ecological characteristics of fished stocks make data collection difficult. The relative invisibility in the oceans, wide distribution and mobility across jurisdictional boundaries, and complex interactions within marine ecosystems and the physical environment has made collection of fishery data time-consuming and expensive.\nA key element to determine stock status is to estimate or specify reference points that serve as baselines against which current stock size estimates are compared. The two widely used reference points to assess and manage fish stocks are stock size and exploitation. While a stock size reference point determines whether the stock is in an over-fished, an exploitation reference point identifies whether over-fishing is taking place. Stock status and reference points are most often derived from stock assessments that requires certain minimum information on the biology, catch or effort, and measures of relative abundance of the stock from fishery data. In developing countries, fishery data are especially poor and basic biological studies are limited because of low financial capital allocated to capacity infrastructure and scientific training to collect, analyse and interpret fishery data.\nThe past decade has seen an advancement and emergence of several data-poor methods (Punt, Smith, and Smith 2011; Jiao et al. 2011; Dick and MacCall 2011). However, the International Council for the Exploration of the Seas (ICES) identified and discussed the use of length-based methods for assessing stock of data-poor fishery (Shen et al., n.d.). Other method include catch-only and catch-based methods. Many of these size-based techniques were developed to estimate the growth and mortality rates of fish without the need for expensive and difficult to obtain age data. Recently, Hordyk (2019) developed a model capable to estimate stock status of exploited fishery from a data-poor. The model suits small-scale and data-poor fisheries like the kawakawa fishery in coastal waters of Tanzania. Unlike the industrial fishery, data for small-scale fishery is often restricted by lack of technical expertise and expense. However, measurement of length composition data of an exploited stock are economically cheap and simple to collect for small-scale fisheries (Quinn and Deriso, 1999).\nIn this study, we used a Kawakawa (Euthynnus affinis) as an example to demonstrate the advantages of using length-based model in assessing the stock status of data-poor species. Kawakawa is an epipelagic migratory tuna species that is widely distributed in the tropical and subtropical waters of the Indo-Pacific region. In the Indian Ocean, this species extends from South Africa (Smith and Heemstra, 1986), along the coasts of east Africa, Arabian Peninsula, the Indian sub-continent, and Malaysian Peninsula. It is also found in the Red Sea, Persian Gulf, and off islands in the Indian Ocean, including Madagascar, Comoros Islands, Mauritius, Reunion, Seychelles, Lakshadweep, Andaman Islands, Nicobar Islands, Sri Lanka, and Maldives (Boettiger, Temple Lang, and Wainwright 2012). In Tanzania, this species is mostly caught in the neritic waters. Kawakawa is also a merit species and is most commonly seen in the ringnet catches, although few of individual are caught by hand-line, longline and small purse seine.\n\nAs important as this modeling is to science-based management, no published application of SPR models exists for kawakawa from the Pemba Channel. Our study aimed to use the length-based method to determine reference points and estimate the stock of of Kawakawa in neritic waters of Tanzania. To help achieve this goal, several objectives were defined. The first objective was to determine the length at first maturity of kawakawa individuals and test whether the length of maturity is different from the median fork length of species. The second objective was to establish biological and exploitation reference points from data-poor kawakawa fishery using traditional methods. The third objective was to use the estimated biological and exploitation reference points to simulate across the range of life histories, based on relationships between length of capture and length at maturity to estimate the stock of kawakawa in the Pemba channel.\n\n19.1.1 Estimate the Stock\nWe used the Length-Based Spawning Potential Ratio (LBSPR) model to assess the stock status based on length frequency data of Kawakawa in the territorial water. The LBSPR was chosen for this analysis because is able to estimate fisheries stock from limited data— a situation where few data are available. The LBSPR models are equilibrium based, and assume that the length composition data is representative of the exploited population at steady state. Because of limited data, LBSPR was deploy to first to simulate the expected length composition and growth curve. These parameters are then fitted to the model with the empirical length frequency data to estimate relative apical fishing mortality and spawning potential ratio of kawakawa.\n\n\n19.1.2 Length frequency\nThe raw raw length measurements were first converted to length frequency and summarized the length-frequency matrix data into one column per year. This structure of LFQ data is needed when catch data is converted to growth. Then length frequency data set was restructured according to a list of steps to emphasize cohorts in the data. The steps can be found in various publications, see Brey and Pauly (1986) or Pauly (1981). Here, the steps documented in Gayanilo and Pauly (1997) are used.\n\n\n19.1.3 Recruitment\nRecruitment patterns of a stock was computed by backward projection onto the length axis of a set of length frequency data using the von Bertallanfy growth curve (Pauly 1982). The method assumes that (i) all individual fish in a data set grow as described by a single set of growth parameters and (ii) one month out of twelve always has zero recruitment. The second assumption is probably not met, since temperate species may contain more than one month with zero recruitment, while tropical species may have more constant recruitment without months of no recruitment.\nAlthough the default setting provided a reliable recruitment, however, the time lag between peak spawning and recruitment made the 0.25 tsample inaccurate to estimate the recruitment period of Kawakawa because it contains matrix of sampling dates. Therefore, to have a vector of date of each sampling data, the date were converted to fraction date by extracting the day of the year of sampling date and divide by 365.25\n\n\n19.1.4 Cohort identification\nWe identified kawakawa cohorts by means of length-frequency histograms analyses. Length-frequency histograms were created for each sampling months from size data for 1,866 observation based on class intervals of 2.0cm fork length. From the length-frequency histograms, we used the modal progression routine of FiSAT to identify potential cohorts. Modes were accepted as distinct cohorts only when differentiated by a separation index above the critical value of 2 and validated with growth curves. One-sample Wilcoxon tests were used to confirm the goodness of fit of observed and predicted length frequencies and detect departures from normality of the observed length-frequency distributions (R Core Team 2020)."
  },
  {
    "objectID": "lbspr.html#derivation-of-reference-points",
    "href": "lbspr.html#derivation-of-reference-points",
    "title": "19  Length Based Spawning Potential Ratio",
    "section": "19.2 Derivation of reference points",
    "text": "19.2 Derivation of reference points\n\n19.2.1 Estimate size at first maturity\nSize at first maturity (\\(L_{50}\\)) describe the length at which 50 percent of the individuals fish caught are adults or with mature gonads. The size at 50% maturity (\\(L_{50}\\)) was estimated as the length at which a randomly chosen specimen has a 50% chance of being mature (Torrejon-Magallanes 2020). We used a morphometric instead of gonado somatic technique to estimate \\(L_{50}\\) of Kawakawa because is robust in assessing the stock using the length-based-spawning potential ratio (LBSPR). In the regression analysis, the fork length is denoted as the explanatory variable and the classification of maturity (juveniles versus adults) is considered the response variable (binomial). The variables are fitted to a logit function in equation below:\n\\[\nP_{CS} = \\frac{1}{1+e^{-(\\hat{\\beta}_{0} + \\hat{\\beta}_{1}\\times X)}}\n\\]\nwhere PCS is the probability of an individual of being mature at a determinate X length. \\(\\beta_{0}\\) (intercept) and \\(\\beta_{1}\\) (slope) are parameters estimated. The L_50_ is calculated in equation below:\n\\[\nL_{50} = -\\frac{\\hat{\\beta}_{0}}{\\hat{\\beta}_{1}}\n\\]\n\n\n19.2.2 Catch Converted Curve\nLife history parameters were determined from growth parameters (length asymptotic and growth coefficient), mortality and size at first maturity. Total mortality was estimated using the length–converted catch curve model. The selectivity parameters were estimated by the length-converted catch curve analysis with simultaneous estimation of the selection ogive. The construction of the catch curves requires information about growth parameters to estimate the relative age of the individuals. Here, growth parameters (\\(L_\\infty\\) and \\(K\\)) of the von Bertlanffy growth equation (VBGE) were computed. Based on the growth parameters, the instantaneous natural mortality rate (\\(M\\)) were approximated by means of the empirical formula in equation below:\n\\[\nM = 4.118K^{0.73} L_{\\infty}-0.33\n\\]\nwhere K and L∞ are the growth parameters of the VBGE. By subtraction of \\(M\\) from \\(Z\\), an estimate of the instantaneous fishing mortality rate (F) was estimated and an indicator of the exploitation rate was estimated as (\\(E = \\frac{F}{Z}\\)).\n\nA Powell-Wetherall method was used to estimate the instantaneous total mortality rate (\\(Z\\)) and the infinite length (\\(L_\\infty\\)) of the von Bertalanffy growth equation (Powell 1979; Wetherall, Polovina, and Ralston 1987). The Electronic LEngth Frequency ANalysis (ELEFAN) method was used to fitting the best growth curve, which allows the fitted curve through the maximum number of peaks of the length frequency distribution.With the aid of the best growth curve, the growth constant and asymptotic length (\\(L_{\\infty}\\)) were estimated. The von Bertalanffy growth equation was defined in equation below;\n\\[\nL_t = L_{\\infty} [1-exp (-K(t-t_0))]\n\\]\nWhere \\(L_t\\) is length at time \\(t\\), \\(L_\\infty\\), the asymptotic length, \\(K\\), the growth coefficient and \\(t_0\\), is the hypothetical time at which length is equal to zero. The \\(t_0\\) value estimated using the empirical equation below;\n\\[\nLog_{10}(-t_0) = -0.3922 - 0.2752 \\times Log_{10}\\times L_\\infty - 1.038 \\times Log_10 \\times K\n\\]"
  },
  {
    "objectID": "lbspr.html#data-analysis-and-packages",
    "href": "lbspr.html#data-analysis-and-packages",
    "title": "19  Length Based Spawning Potential Ratio",
    "section": "19.3 Data Analysis and Packages",
    "text": "19.3 Data Analysis and Packages\nWe used R–a language and environment for statistical computing version 4.5 (R Core Team 2020) to process, analyse, and visualize, plots and maps. Wilcoxon test was used to test whether the length at maturity was significant difference from the median fork length of kawakawa. Several packages were used to perform create length frequency, compute biological references points and estimate exploitation parameters, which are TropFishR (Mildenberger, Taylor, and Wolff 2017), sizeMat (Torrejon-Magallanes 2020) and LBSPR (Hordyk 2019) packages. Other packages used for data management and analytic tasks includes readr (Wickham and Hester 2020) package for reading comma–separated files, readxl for Excel spreadsheets (Wickham and Bryan 2019), and ncdf4 (Pierce 2019) for importing satellite data stored in NECDF file format and sf (Pebesma 2018) package for loading shapefile and simple feature files.\nOther packages include lubridate (Grolemund and Wickham 2011) for dealing with date and group observations to monsoon seasons, wior package Semba and Peter (2020) was used to detect outlier observations from the datasets, dplyr (Wickham et al. 2021) for manipulating data, and tidyr (Wickham 2021) for organizing the file into a consistent format for subsequent analysis and mapping. The results from the analysis were summarized and presented in tables with knitr (Xie 2014) and kableExtra (Zhu 2021) packages while plotting and mapping was made with the ggplot2 (Wickham 2016), and ggstatsplot (Patil 2021) and metR (Campitelli 2021) packages. patchwork (Pedersen 2020) was used to combine plots and export them in both PDF and PNG formats. The bookdown (Xie 2016) and rmarkdown (Xie, Dervieux, and Riederer 2020) packages were used to typeset and compile LATEX document of the manuscript an knit with knitr package (Xie 2014) to various format including PDF, tex and word. The script file that can be used to reproduce the analysis and plotting figures used in this study can be accessed from this github link."
  },
  {
    "objectID": "lbspr.html#dsfa-neritic-data",
    "href": "lbspr.html#dsfa-neritic-data",
    "title": "19  Length Based Spawning Potential Ratio",
    "section": "19.4 DSFA neritic data",
    "text": "19.4 DSFA neritic data\n\nneritic = read_csv(\"d:/semba/dsfa_fumba/length_weight_neritic.csv\") %>% janitor::clean_names()\n\n\nneritic = neritic %>% \n  mutate(fishing_date = lubridate::mdy(fishing_date)) %>% \n  rename(fl = focul_length, wt = uzito, species =aina_ya_tuna_walioteuliwa, \n         gear = aina_ya_mtego) %>% \n  mutate(species = if_else(species == \"Sehewa (Kawakawa)\", \"Kawakawa\",species))\n\n\nneritic %>% \n  group_by(kijiji) %>% count(sort = TRUE)\n\n# A tibble: 11 x 2\n# Groups:   kijiji [11]\n   kijiji             n\n   <chr>          <int>\n 1 Shangani East   4135\n 2 Halmashauri     3808\n 3 Nungwi          2429\n 4 Wete            1299\n 5 Magengeni        541\n 6 Kunduchi Pwani   239\n 7 Kilindoni        135\n 8 Jimbiza           19\n 9 Bweni              5\n10 Other              3\n11 Tumbuju            2"
  },
  {
    "objectID": "lbspr.html#l50",
    "href": "lbspr.html#l50",
    "title": "19  Length Based Spawning Potential Ratio",
    "section": "19.5 L50",
    "text": "19.5 L50\n\nkawa = neritic %>% \n  filter(species == \"Kawakawa\" & wt < 5 & fl > 31 & fl < 80 & kijiji %in% c(\"Halmashauri\", \"Shangani East\")) \n\n\n\nclassify = kawa %>% \n  sizeMat::classify_mature(varNames = c(\"fl\", \"wt\"), varSex = NULL, method = \"qd\", selectSex = NULL) \n\nall individuals were used in the analysis \n\nclassify %>% plot()\n\n\n\n# classify \n\n\n## convert classify dataset to tibble/dataframe\nclassifiy.tb = data.frame(\n  x = classify$x, \n  y = classify$y, \n  mature = classify$mature\n  ) %>% \n  mutate(group = if_else(mature == 1,  \"Adult\", \"Juvenile\")) \n#%>%   wior::outlier_remove(y) \n\n\nclassifiy.tb %>% \n  bind_cols(kawa) %>% \n  mutate(month = lubridate::month(fishing_date, label = TRUE)) %>% \n  ggplot(aes(x = month, fill = group))+\n  geom_bar(position = position_dodge(preserve = \"single\"), width = 0.8)+\n  ggsci::scale_fill_lancet()+\n  theme(legend.position = c(.3,.85))\n\n\n\n\n\n# Instead of relying on the base plot, we can exract the data and use the power of ggplot2 and its sister packages to make elegant plots\n\nformula = y~x\n\n\n\n## compute percentage of simulated avalue\npercentage = classifiy.tb %>% \n  group_by(group) %>% \n  summarise(n = n()) %>% \n  mutate(percentage = round(n/sum(n)*100),2)\n\ntextual = data.frame(xa = 80,ya=6, group = NA, label = paste(\"Percentage of adult is\", percentage$percentage[1], \" and that of Juvenile is \",  percentage$percentage[1]))\n\nkaw.juv.adult =  classifiy.tb %>%\n  ggplot(aes(x = x, y = y, color = group, fill = group, shape = group))+\n  geom_point(size = 2)+\n  geom_smooth(method = \"lm\", formula =formula, se = TRUE )+\n  scale_y_continuous(trans = scales::log10_trans(), breaks = scales::breaks_width(width = 1.5))+\n  scale_x_continuous(breaks = scales::breaks_width(width = 10), expand = c(0,0), limits = c(NA, 75))+\n  ggsci::scale_color_lancet()+\n  ggsci::scale_fill_lancet(alpha = .1) +\n  # ggpmisc::stat_poly_line(formula = formula) +\n  ggpmisc::stat_poly_eq(aes(label = stat(rr.label)),\n                        formula = formula, label.x = .04, label.y = c(.82,.75))+\n  ggpmisc::stat_poly_eq(aes(label = stat(eq.label)), \n                        formula = formula) +\n  theme_bw() +\n  theme(legend.title = element_blank(), legend.position = c(.75,.15), \n        legend.background = element_rect(fill = \"white\"), \n        panel.grid = element_blank())+\n  labs(x = \"Fork length (cm)\", y = \"Weight (kg)\")\n\nkaw.juv.adult\n\n\n\n\n\nfit = classifiy.tb %$% \n  glm(y~x + group)\n\nfit %>% \n  car::Anova(type = \"III\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: y\n      LR Chisq Df Pr(>Chisq)    \nx         1931  1    < 2e-16 ***\ngroup       50  1    1.5e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nfit %>% \n  ggstatsplot::ggcoefstats()\n\n\n\nfit %>% \n  equatiomatic::extract_eq(\n    # operatorname,\n  var_colors = c(\n    x = \"#ffa91f\",\n    y = \"#00d1ab\"\n  ),\n  greek_colors = c(\n    \"black\", \"#3A21B3\", \"#58A1D9\", \"#FF7582\", \"black\"\n  ),\n  subscript_colors = c(\n     NA_character_, \"#3A21B3\", \"#58A1D9\", \"#FF7582\", NA_character_\n  ),\n  wrap = TRUE,\n  terms_per_line = 3\n)\n\n\\[\n\\begin{aligned}\nE( \\operatorname{y} ) &= {\\color{black}{\\alpha}} + {\\color{#3A21B3}{\\beta}}_{{\\color{#3A21B3}{1}}}({\\color{#ffa91f}{\\operatorname{x}}}) + {\\color{#58A1D9}{\\beta}}_{{\\color{#58A1D9}{2}}}(\\operatorname{group}_{\\operatorname{Juvenile}})\n\\end{aligned}\n\\]\n\n\n\n#### estimate maturity\n\nmyMature = classify%>% \n  sizeMat::morph_mature(method = \"fq\", niter = 100)\n\n\nmyMature%>% \n  print()\n\nformula: Y = 1/1+exp-(A + B*X) \n\n\n    Original Bootstrap (Median)\nA   -81.3653 -83.1864          \nB   1.6349   1.6699            \nL50 49.7685  49.7425           \nR2  -        0.9481            \n\n#### PLot estimated morphomometric maturity\n\nmyMature %>% \n  plot(onlyOgive = TRUE)\n\n\n\n\nSize at morphometric maturity = 49.7 \nConfidence intervals = 49.5 - 50 \nRsquare = 0.95\n\n\n\nl50 = 49.8\nr2 = 0.95\nxpos = 65\n\nmarkx = tibble(x = 25, x1 = l50, y = 0.5,y1 = 0.5)\nmarky = tibble(x = l50, x1 = l50, y = 0,y1 = 0.5)\n\nmystats = data.frame(x =c(xpos,xpos), y = c(0.9,.75), \n                     variable = c(\"R2\", \"L50\"), value = c(r2, l50)) %>% \n  mutate(label = glue::glue(\"*R*<sup>2</sup> = {round(value, 2)}\")) %>% \n  add_row(x = xpos, y = 0.75, variable = \"L50\", value = l50, \n          label = glue::glue(\"*L*<sub>50</sub> = {round(value, 2)}\")) %>% \n  slice(-2)\n\n\n\n\nkaw.morpho = myMature$out %>% \n  ggplot(aes(x = x))+\n  geom_line(aes(y = fitted), color = \"black\")+\n  geom_segment(data = markx, aes(x = x, y = y, xend = x1, yend = y1), linetype = 2, color = \"red\")+\n  geom_segment(data = marky, aes(x = x, y = y, xend = x1, yend = y1), linetype = 2, color = \"red\")+\n  theme_bw()+\n  theme(panel.grid = element_blank())+\n  # coord_cartesian(expand = FALSE)+\n  labs(x = \"Fork length (cm)\", y = \"Maturity\")+\n  scale_y_continuous(labels = scales::percent_format())+\n  scale_x_continuous(breaks = scales::breaks_width(width = 10), expand = c(0,0), limits = c(NA, 85))+\n  annotate(geom = \"point\", x = l50, y = .5, size = 2.2)+\n  ggtext::geom_richtext(data = mystats, aes(x = x, y = y, label = label))\n\nkaw.morpho\n\n\n\n\n\nmedian(kawa$fl)\n\n[1] 57\n\nkaw.hist = kawa %>% \n    # wior::outlier_remove(fl) %>% \n  # select(fl, maturity_stage) %>% \n  ggstatsplot::gghistostats(x = fl, \n                            test.value = myMature$L50_boot %>% median(),\n                            type = \"n\",\n                            bf.message = FALSE,\n                            # normal.curve = TRUE, \n                            binwidth = 1) +\n  geom_vline(xintercept = myMature$L50_boot %>% median(), linetype = 2, color = \"red\", size = 1.2)+\n  theme_bw()+\n  theme(panel.grid.minor = element_blank())+\n  coord_cartesian(expand = FALSE)+\n  labs(x = \"Fork length (cm)\")+\n  scale_x_continuous(breaks = scales::breaks_width(width = 10))\n\nkaw.hist"
  },
  {
    "objectID": "lbspr.html#length-frequency-1",
    "href": "lbspr.html#length-frequency-1",
    "title": "19  Length Based Spawning Potential Ratio",
    "section": "19.6 Length frequency",
    "text": "19.6 Length frequency\nConvert raw length measurements to length frequency data (lfq class).\nModify length-freqeuncy (LFQ) data. Allows to summarise catch matrix of LFQ data to one column per year. This is required for e.g. catchCurve. Allows to change bin size of LFQ data. Allows to ad plus group to catch matrix.\nThen length frequency data set was restructured according to a list of steps to emphasise cohorts in the data. The steps can be found in various publications, see Brey and Pauly (1986) or Pauly (1981). Here, the steps documented in Gayanilo and Pauly (1997) are used.\n\nkawa.lfq = kawa %>% \n  TropFishR::lfqCreate(Lname = \"fl\", Dname = \"fishing_date\", \n                       bin_size = 1, species = \"Kawakawa\", Lmin = 31, \n                       length_unit = \"cm\", aggregate_dates = TRUE)\n\n\nkaw.mod = kawa.lfq %>% \n  TropFishR::lfqModify(bin_size = 2, aggregate = \"quarter\", vectorise_catch = FALSE) %>% \n  TropFishR::lfqRestructure(MA = 11, addl.sqrt = FALSE)\n\n\npar(mfrow = c(2,1))\nkaw.mod %>% \n  plot(Fname = \"catch\", las = 1, ylim = c(35,75))\n\n\nkaw.mod %>% \n  plot(Fname = \"rcounts\", las = 1, ylim = c(35,75))"
  },
  {
    "objectID": "lbspr.html#powell-wetherall-method",
    "href": "lbspr.html#powell-wetherall-method",
    "title": "19  Length Based Spawning Potential Ratio",
    "section": "19.7 Powell-Wetherall method",
    "text": "19.7 Powell-Wetherall method\nA method to estimate the instantaneous total mortality rate (Z) and the infinite length of the von Bertalanffy growth equation (Powell 1979).\n\nres_PW <- powell_wetherall(param = kaw.mod,\n                           catch_columns = 1:ncol(kaw.mod$catch),\n                           reg_int = c(1,14))\n\n\n\n# show results\npaste0(\"Linf =\",round(res_PW$Linf_est,2), \"±\", round(res_PW$se_Linf))\n\n[1] \"Linf =61.25±3\""
  },
  {
    "objectID": "lbspr.html#elefan",
    "href": "lbspr.html#elefan",
    "title": "19  Length Based Spawning Potential Ratio",
    "section": "19.8 ELEFAN",
    "text": "19.8 ELEFAN\nElectronic LEngth Frequency ANalysis for estimating growth parameter. This functions allows to perform the K-Scan and Response surface analysis to estimate growth parameters. It combines the step of restructuring length-frequency data (lfqRestructure) followed by the fitting of VBGF curves through the restructured data (lfqFitCurves). K-Scan is a method used to search for the K parameter with the best fit while keeping the Linf fixed. In contrast, with response surface analysis both parameters are estimated and the fits are displayed in a heatmap.\n\n# ELEFAN with K-Scan\nres_KScan <- ELEFAN(kaw.mod, Linf_fix = res_PW$Linf_est,\n                    MA=5, addl.sqrt = TRUE, hide.progressbar = TRUE)\n\nOptimisation procuedure of ELEFAN is running. \nThis will take some time. \nThe process bar will inform you about the process of the calculations.\n\n\n\n\n# show results\nres_KScan$par; res_KScan$Rn_max\n\n$Linf\n[1] 61.3\n\n$K\n[1] 1.42\n\n$t_anchor\n[1] 0.0563\n\n$C\n[1] 0\n\n$ts\n[1] 0\n\n$phiL\n[1] 3.73\n\n\n[1] 0.231\n\n\n\npar(mfrow = c(2,1))\nkaw.mod %>% \n  plot(Fname = \"catch\", las = 1, ylim = c(35,75))\n\nlt <- kaw.mod %>% \n  TropFishR::lfqFitCurves(par = list(Linf=61.25, K=1.42, t_anchor=0.056, C=0.5, ts=0),\n                   draw = TRUE, col = \"red\", lty = 3, lwd=1.5)\n\n\nkaw.mod %>% \n  plot(Fname = \"rcounts\", las = 1, ylim = c(35,75))\n\nlt <- kaw.mod %>% \n  TropFishR::lfqFitCurves(par = list(Linf=61.25, K=1.42, t_anchor=0.056, C=0.5, ts=0),\n                   draw = TRUE, col = \"red\", lty = 3, lwd=1.5)\n\n\n\n# this figure was exported in console with a title lfq_cohort.pdf"
  },
  {
    "objectID": "lbspr.html#catch-converted",
    "href": "lbspr.html#catch-converted",
    "title": "19  Length Based Spawning Potential Ratio",
    "section": "19.9 catch-converted",
    "text": "19.9 catch-converted\n\nab = kaw.mod %>% \n  TropFishR::lfqModify(vectorise_catch = TRUE, \n                       # plus_group = 67,\n                       bin_size =4,\n                       # minDate = as.Date(\"2020-06-30\"),\n                       # maxDate = as.Date(\"2020-12-31\"),\n                       year = 2020) \n\nab$Linf = 61.25\nab$K = 1.42\n\ncatch.converted = ab %>% \n  TropFishR::catchCurve(reg_int = c(0,68), \n                        calc_ogive = TRUE,\n                        # cumulative = TRUE,\n                        auto = TRUE, \n                        plot = TRUE)\n\n\n\n\n\n\ncatch.converted$F\n\nNULL"
  },
  {
    "objectID": "lbspr.html#recruitment-1",
    "href": "lbspr.html#recruitment-1",
    "title": "19  Length Based Spawning Potential Ratio",
    "section": "19.10 Recruitment",
    "text": "19.10 Recruitment\nThis function calculates recruitment patterns of a stock by backward projection onto the length axis of a set of length frequency data using the (special, generalised or seasonalised) von Bertallanfy growth curve (Pauly 1982). The method assumes that (i) all fish in a data set grow as described by a single set of growth parameters and (ii) one month out of twelve always has zero recruitment. The second assumption is probably not met, since temperate species may contain more than one month with zero recruitment, while tropical species may have more constant recruitment without months of no recruitment. If t0 is not provided, a relative recruitment pattern will be estimated without specific month values returned in the results. However, an estimate of t0 can be obtained by the time lag between peak spawning and recruitment. Several length-frequency data sets can be used to estimate the recrutiment pattern by providing catch as a matrix and setting catch_column to NA (default). Then the fraction per time is calculated for each size class in each sample and then pooled together. For the generalised vBGF, D is required, for the seasonalised vBGF C, ts and D.\n\ncatch.converted %>% \n  recruitment(tsample = 0.25, plot = TRUE)\n\n\n\n\nIf no t0 is provided only relative recruitment pattern can be estimated.\n\n\n$species\n[1] \"Kawakawa\"\n\n$stock\n[1] NA\n\n$dates\n[1] \"2020-01-01\"\n\n$midLengths\n [1] 34 38 42 46 50 54 58 62 66 70\n\n$catch\n [1]   2  17  63  94  96 193 314 222  67  14\n\n$comment\n[1] \"\"\n\n$rcounts\n    X2019.08.15 X2019.11.15 X2020.02.15 X2020.05.15 X2020.08.15 X2020.11.15\nV1      0.00000      0.0000      0.0000      0.0000     0.00000     -0.8194\nV2      0.00000      0.0000      0.0000      0.0000     0.00000     -0.8468\nV3      0.00108      0.0000      0.0000      0.0000     0.00000     -0.8727\nV4      0.00337      0.0000      0.0000      0.0000     0.00000      0.1036\nV5      0.00000      0.0000     -0.3684      0.0000     0.00000      0.9354\nV6      0.00000     -0.2288     -0.6595      0.0000     0.00000      0.7009\nV7      0.00000     -0.4853     -0.2477      0.0000    -0.11889     -0.0016\nV8      0.00000     -0.5031      0.0941      0.0000    -0.04782      0.1357\nV9      0.01158      0.4732      0.7015      0.0000    -0.18403     -0.7371\nV10     0.00216     -0.3383     -0.3056      0.0000    -0.19768     -0.7894\nV11     0.00216      0.0633      0.0641     -0.2905    -0.18070     -0.4138\nV12     0.00000      0.3884      1.3428     -0.2169     0.07256      0.5164\nV13     0.00000     -0.0456      0.7258     -0.0183     0.16572      0.9703\nV14     0.00000      0.5332      0.1291      0.3833     0.21984      1.2248\nV15     0.00000      0.5951      0.3791      0.4087     0.26284      0.7729\nV16     0.00000     -0.0651     -0.3723     -0.0291     0.00816      0.0575\nV17     0.00000     -0.2826     -0.5594     -0.1949     0.00000      0.0450\nV18     0.00000     -0.1043      0.0000     -0.0424     0.00000     -0.6049\nV19     0.00000      0.0000     -0.2083      0.0000     0.00000     -0.3766\nV20     0.00000      0.0000      0.0000      0.0000     0.00000      0.0000\nV21     0.00000      0.0000      0.0000      0.0000     0.00000      0.0000\nV22     0.00000      0.0000      0.0000      0.0000     0.00000      0.0000\n    X2021.02.15 X2021.05.15\nV1       0.0000      0.0000\nV2       0.0000      0.0000\nV3      -0.5810      0.0000\nV4      -0.0679      0.0000\nV5       0.0248      0.0000\nV6       0.3158      0.0000\nV7       0.1953     -0.1289\nV8       0.0892     -0.0529\nV9      -0.3506      0.0000\nV10     -0.6001      0.0000\nV11     -0.1193      0.0158\nV12     -0.0738      0.3697\nV13      0.7595      0.2058\nV14      1.2925      0.1599\nV15      0.8188      0.3838\nV16      0.2460     -0.0649\nV17      0.0845     -0.1515\nV18     -0.4996      0.0000\nV19     -0.4935     -0.0781\nV20      0.0000      0.0000\nV21     -0.3049      0.0000\nV22      0.0000      0.0000\n\n$peaks_mat\n    X2019.08.15 X2019.11.15 X2020.02.15 X2020.05.15 X2020.08.15 X2020.11.15\nV1            0           0           0           0           0           0\nV2            0           0           0           0           0           0\nV3            4           0           0           0           0           0\nV4            4           0           0           0           0          19\nV5            0           0           0           0           0          19\nV6            0           0           0           0           0          19\nV7            0           0           0           0           0           0\nV8            0           0          10           0           0          20\nV9            5           7          10           0           0           0\nV10           5           0           0           0           0           0\nV11           5           8          11           0           0           0\nV12           0           8          11           0          16          21\nV13           0           0          11           0          16          21\nV14           0           9          11          13          16          21\nV15           0           9          11          13          16          21\nV16           0           0           0           0          16          21\nV17           0           0           0           0           0          21\nV18           0           0           0           0           0           0\nV19           0           0           0           0           0           0\nV20           0           0           0           0           0           0\nV21           0           0           0           0           0           0\nV22           0           0           0           0           0           0\n    X2021.02.15 X2021.05.15\nV1            0           0\nV2            0           0\nV3            0           0\nV4            0           0\nV5           22           0\nV6           22           0\nV7           22           0\nV8           22           0\nV9            0           0\nV10           0           0\nV11           0          25\nV12           0          25\nV13          23          25\nV14          23          25\nV15          23          25\nV16          23           0\nV17          23           0\nV18           0           0\nV19           0           0\nV20           0           0\nV21           0           0\nV22           0           0\n\n$ASP\n[1] 8.48\n\n$MA\n[1] 11\n\n$Linf\n[1] 61.2\n\n$K\n[1] 1.42\n\n$t_midL\n [1] 0.570 0.682 0.815 0.979 1.193 1.503 2.068   NaN   NaN   NaN\n\n$lnC_dt\n [1] 2.96 4.94 6.06 6.23 5.94 6.18 5.74  NaN  NaN   NA\n\n$reg_int\n[1] 5 7\n\n$linear_mod\n\nCall:\nlm(formula = yvar ~ xvar, data = df.CC.cut)\n\nCoefficients:\n(Intercept)         xvar  \n      6.423       -0.296  \n\n\n$Z\n[1] 0.296\n\n$se\n[1] 0.403\n\n$confidenceInt\n[1] -4.83  5.42\n\n$intercept\n[1] 6.42\n\n$linear_mod_sel\n\nCall:\nlm(formula = ln_1_S_1 ~ t_ogive, na.action = na.omit)\n\nCoefficients:\n(Intercept)      t_ogive  \n       15.7        -21.7  \n\n\n$Sobs\n[1] 0.0371 0.2780 0.8864 1.0975\n\n$ln_1_S_1\n[1]  3.256  0.954 -2.054    NaN\n\n$Sest\n [1] 0.0355 0.2949 0.8825 0.9962 1.0000 1.0000 1.0000    NaN    NaN    NaN\n\n$t50\n[1] 0.722\n\n$t75\n[1] 0.773\n\n$t95\n[1] 0.858\n\n$L50\n[1] 39.3\n\n$L75\n[1] 40.8\n\n$L95\n[1] 43.1\n\n$ti\n [1] 0.320 0.432 0.565 0.729 0.943 1.253 1.818   NaN   NaN   NaN\n\n$tS_frac\n [1] 0.320 0.432 0.565 0.729 0.943 0.253 0.818   NaN   NaN   NaN\n\n$cor_months\n [1]   4   6   7   9  12   4  10 NaN NaN NaN\n\n$months\n [1]  1  2  3  4  5  6  7  8  9 10 11 12\n\n$all_recruits\n [1]   0   0   0 195   0  17  63   0  94 314   0  96\n\n$mean_recruits\n [1]   0   0   0 195   0  17  63   0  94 314   0  96\n\n$per_recruits\n [1]  0.00  0.00  0.00 25.03  0.00  2.18  8.09  0.00 12.07 40.31  0.00 12.32\n\nattr(,\"class\")\n[1] \"recruitment\"\n\n\nAlthough the detault setting provided a reliable recruitment, however, the 0.25 tsample is not accurate for our data because our dataset contain matrix of sampling dates. We therefore need to convert to fraction date by extracting the day of the year of sampling date and divide it by 365.25\n\n# retrieve sampling times from catch matrix\ns_dates <- as.POSIXlt(kaw.mod$dates, format=\"%d.%m.%Y\")\n## convert to day in a year\nyear.day = s_dates %>% lubridate::yday()\n\nsampling.time = year.day/365.25\n\n\npar(mfrow = c(2,1))\n\nkawa.lfq %>% \n  TropFishR::lfqModify(bin_size = 2, aggregate = \"month\", years = 2020) %>% \n  TropFishR::lfqRestructure(MA = 11, addl.sqrt = FALSE) %>% \n  plot(Fname = \"catch\", \n       par = list(Linf = 61.25, K = 1.42, t_anchor = 0.056, C = 0.25, ts = 0),\n       ylim = c(35,75), las=1)\n\n\nrecruits = catch.converted %>% \n  recruitment(tsample = sampling.time, plot = TRUE)\n\n\n\n\nIf no t0 is provided only relative recruitment pattern can be estimated."
  },
  {
    "objectID": "lbspr.html#length-based-spawning-potential-ratio-lbspr",
    "href": "lbspr.html#length-based-spawning-potential-ratio-lbspr",
    "title": "19  Length Based Spawning Potential Ratio",
    "section": "19.11 Length-based spawning potential ratio (LBSPR)",
    "text": "19.11 Length-based spawning potential ratio (LBSPR)\n\n# estimation of M\nMs <- TropFishR::M_empirical(Linf = res_KScan$par$Linf, \n                             K_l = res_KScan$par$K, \n                             method = \"Then_growth\")\n\nkaw.mod$M <- as.numeric(Ms)\n\n\nkaw.pars = new(\"LB_pars\")\n\n\nmyMature%>% \n  print()\n\nformula: Y = 1/1+exp-(A + B*X) \n\n\n    Original Bootstrap (Median)\nA   -81.3653 -83.1864          \nB   1.6349   1.6699            \nL50 49.7685  49.7425           \nR2  -        0.9481            \n\nkawa %>% \n  # mutate(uzito = as.numeric(uzito)) %>% \n  # filter(code == \"FRI\" & fl < 60 & between(new.date, lubridate::dmy(010620), lubridate::dmy(311220))) %>%\n  dplyr::select(fl, species) %>% \n  group_by(species) %>% \n  ggpubr::get_summary_stats()\n\n# A tibble: 1 x 14\n  species  variable     n   min   max median    q1    q3   iqr   mad  mean    sd\n  <chr>    <chr>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Kawakawa fl        1866    33    74     57    52    60     8  5.78  55.5  6.62\n# ... with 2 more variables: se <dbl>, ci <dbl>\n\n\n\n## biological parameters\nkaw.pars@Species = \"Kawakawa\"\nkaw.pars@L_units = \"cm\"\nkaw.pars@L50 = myMature$out %>% filter(between(fitted, 0.49,0.60)) %>% slice(1) %>% pull(x)\nkaw.pars@L95 = myMature$out %>% filter(between(fitted, 0.93,0.971)) %>% slice(1) %>% pull(x)\nkaw.pars@MK = kaw.mod$M/res_KScan$par$K\nkaw.pars@Linf = res_KScan$par$Linf\n\n## Exploitation parameters\nkaw.pars@SL50 = catch.converted$L50\nkaw.pars@SL95 = catch.converted$L95\nkaw.pars@FM = 1.07#catch.converted$FM %>% as.numeric() %>% abs()\nkaw.pars@BinWidth = 2\n\nkaw.pars@BinMin = 33\nkaw.pars@BinMax = 74\n\n# kaw.pars\n\n\nkaw.sim = kaw.pars %>% \n  LBSPRsim()\n\n\nkaw.sim %>% plotSim()\n\n\n\n\nFigure 19.1: Estimated a) the expected (equilibrium) size structure of the catch and the expected unfished size structure of the vulnerable population, b) the maturity and selectivity-at-length curves, c) the von Bertalanffy growth curve with relative age, and d) the SPR and relative yield curves as a function of relative fishing mortality (see note above on the F/M ratio).\n\n\n\n\n\nkaw.sim %>% \n  plotSim(lf.type = \"pop\")\n\n\n\n\nestimated stock population\n\n\n\n\n\nkaw.sim %>% plotSim(type = \"growth\")\nkaw.sim %>% plotSim(type = \"len.freq\")\nkaw.sim %>% plotSim(type = \"yield.curve\")\n\n\ntibble(Z = catch.converted$Z, K = catch.converted$K,MK = kaw.sim@MK,FM = kaw.sim@FM, t50 = catch.converted$t50, t95 = catch.converted$t95,   L50 = kaw.sim@L50, L95 = kaw.sim@L95, SL50 = kaw.sim@SL50, SL95 = kaw.sim@SL95,Linf = kaw.sim@Linf, SPR = kaw.sim@SPR, yield = kaw.sim@Yield, YPR = kaw.sim@YPR, SSB = kaw.sim@SSB, SSB0 = kaw.sim@SSB0) %>% \n  pivot_longer(cols = 1:16) %>% \n  kableExtra::kbl(col.names = c(\"Reference\", \"Value\")) %>% \n  kableExtra::kable_styling(latex_options = \"hold_position\")\n\n\n\nTable 19.1:  Biological and exploitation Reference Points for Kawakawa tuna along the neritic waters of Tanzania \n \n  \n    Reference \n    Value \n  \n \n\n  \n    Z \n    2.96e-01 \n  \n  \n    K \n    1.42e+00 \n  \n  \n    MK \n    9.63e-01 \n  \n  \n    FM \n    1.07e+00 \n  \n  \n    t50 \n    7.22e-01 \n  \n  \n    t95 \n    8.58e-01 \n  \n  \n    L50 \n    4.98e+01 \n  \n  \n    L95 \n    5.13e+01 \n  \n  \n    SL50 \n    3.93e+01 \n  \n  \n    SL95 \n    4.31e+01 \n  \n  \n    Linf \n    6.13e+01 \n  \n  \n    SPR \n    2.49e-01 \n  \n  \n    yield \n    1.41e+08 \n  \n  \n    YPR \n    2.21e+04 \n  \n  \n    SSB \n    5.90e+07 \n  \n  \n    SSB0 \n    3.73e+04"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "WIO-fish",
    "section": "",
    "text": "To raise awareness of countries in the Western Indian Ocean (WIO) in monitoring their progress toward the achievement of the Sustainable Development Goals (SDGs), the FAO has partnered with WIOMSA to support capacity-building activities in collection, monitoring, and assessment of fisheries data for the WIO region. In general, these activities aim to strengthen the workflow from fisheries data collection to support the monitoring and reporting of SDG 14.4.1.\nThe prime goal of this initiative align with the the United Nations Decade of Ocean Science for Sustainable Development (2021-2030, Ocean Decade for short), which focus to support a new cooperative framework to ensure that global ocean science provides greater benefits for ocean ecosystems and wider society.\n\n\n\nUN Decade of the OCean\n\n\nDecade provides a common framework to ensure that ocean science can fully support countries’ actions to sustainably manage the ocean and more particularly to achieve the 2030 Agenda for Sustainable Development – through the creation of a new foundation, across the science-policy interface, to strengthen the management of the ocean and coasts for the benefit of humanity.\nA vast majority of the ocean remains unmapped, unobserved and unexplored. Our understanding of the ocean and its contribution to sustainability largely depends on our capacity to conduct effective ocean science - through research and sustained observations, supported by adequate infrastructures and investments.\nThis decade of the ocean comes down with a slogan the science we want for the ocean we need! But we flip it a little bit and should read the information we want for the ocean we need, why. Because we are at the time of precedented generation of data than ever before. Data generated from sensors to satellite is enormous.\n\n\n\nUN Sustainable Goals\n\n\nIn the WIO, small-scale fisheries dominate and are crucial for the livelihoods of coastal communities in the region, contributing to the twin imperatives of poverty reduction and economic development. However, these fisheries are dispersed, open-access in nature, multispecies and multi-gear, making their monitoring and determination of stock status for individual species incredibly challenging as data are insufficient for conventional stock assessment routines.\nEfforts to determine stock status and provide evidence-based fisheries management advice are beset with problems, including insufficient or inadequate scientific data and expertise, which are compromised by economic and socio-political realities. For example, the regional state of the coast report for the western Indian Ocean (WIO) states that almost all countries in the region cannot adequately assess their marine resources and lack the financial capacity and technical expertise for effective management. Important impediments to fisheries management in the region relate to the following aspects:\n\nThe lack of quality data available for analysis,\nInadequate capacity for in-depth analysis of data, especially in monitoring and assessment of the state of fisheries and the likely effects of alternative management interventions, and\nPoor coordination between institutions and insufficient sharing of information on which to build good governance.\n\nThese have been highlighted as some of the impediments to meeting the targets of SDG 14.\nAlthough the national fisheries institutes in the region do collect data on their fisheries, these data are often not detailed to an adequate granularity (e.g. regarding time, space, fleet or species levels), are not properly organized or linked, lack sufficient quality assurance and control, or are very difficult to retrieve for analysis. Additionally, there is often very limited technical capacity for managing, accessing and extracting data in a way that can be used for analysis.\nThus, even though data may exist, they often remain underutilized, strongly limiting the possibility of applying even data-limited approaches to stock monitoring. As such, analyses of stock status may be missing or be based on inappropriate metrics and methods, thereby hindering the formulation of relevant policies for the sector.\nIn its role as custodian agency of the SDG 14 indicators, FAO has a mandate to support countries to strengthen their capacities to collect, process, analyse and report data while ensuring that different national data sets are comparable and can be aggregated at sub-regional, regional and global levels to monitor the SDGs.\n\n\nTwo workshops have been undertaken in the WIO/East African region, first a project kick-off meeting of the Fishing Data East Africa (FIDEA) project (Dar es Salaam, Tanzania, 16-17 September 2019), focused on Tanzania-Mainland, Tanzania-Zanzibar and Mozambique, at which FAO agreed to partner with FIDEA to support a capacity development workshop in the East Africa region. This led to a mission by FAO staff to investigate the data infrastructure in Tanzania-Mainland, Tanzania-Zanzibar and Mozambique, as well as the SDG 14.4.1 reporting capacity development workshop in Zanzibar from 2-14 March 2020, which included 10 East African countries and WIO island nations.\nThe report of this second workshop recommended some key actions to support countries in improving the collection and use of data for monitoring the SDG 14.4 target. A common point in these recommendations is a greater need to provide long-term support for developing appropriate data management systems. This goes beyond collection to focus particularly on validation, organization, protection, retrieval and summary of the data, essential steps for allowing reliable estimation and reporting of the SDG14.4.1 indicator.\nIt has been stated many times that The collection of data is not an end in itself, but is essential for informed decision-making, and data can only be useful for supporting decision-making if they are properly stored, managed, and curated so that they have the quality necessary for providing meaningful and reliable advice. The report also recommended stronger collaboration between the SWIOFC regional process in monitoring the status of stocks and national processes in developing capacities for the monitoring of SDG14.4.1.\n\n\n\nThe objective of this consultancy is to support development and refining of training tools related to fisheries data management workflow. This will involve working closely with the fisheries expert towards the development of training tools for enhancing stock monitoring status and national processes for SDG14.4.1 monitoring.\n\n\n\nIn executing this consultancy, the Consultant will work closely with WIOMSA and FAO and the following tasks will be undertaken for guided data management, analysis and reporting for decision making and management of the fisheries resources in the region using Excel spreadsheet and R language tools. This will include development of the Excel and R tools that help to reproduce lecture material and tutorials.\nThis is a tutorial for beginning to learn the R programming language. It is a tree of pages — move through the pages in whatever way best suits your style of learning. You are probably impatient to learn R — most people are. That’s fine. But note that trying to skim past the basics that are presented here will almost surely take longer in the end. This page has several sections, they can be put into the four categories: General, Objects, Actions, Help.\nThe wio-fishStats guide and course provide an introduction to data science that is tailored to the needs of fisheries, but is also suitable for freshwater and marine scientists and other biological or social sciences. This audience typically has some knowledge of statistics, but rarely an idea how data is prepared and shaped to allow for statistical testing.\nBy using various data types and working with many examples, the book will guide on transforming, summarizing, and visualizing data. By keeping our eyes open for the perils of misleading representations, the book fosters fundamental skills of data literacy and cultivates reproducible research practices that enable and precede any practical use of statistics.\nThis guide provides datasets and functions that help people want to code fisheries data in R language. Its source code is hosted at https://github.com/lugoga/fishstats. The book and course introduce the principles and methods of data science for fisheries scientists and other biological or social sciences. The guide is available at https://github.com/lugoga/fishstats/manual.\n\n\n\nScan the qrcode to access the interactive web app\n\n\n\n\n\nBooks do not get written without a wide network of support for the author. I am fortunate to have had the support of David Grubbs at Taylor & Francis; the faculty, students, and administration of Northland College; many colleagues in the sheries community; and my friends and family. I am thankful for your understanding for when I missed deadlines and declined invitations as I worked on this project.\nMy deepest thanks, however, go to Arthur Tuda and Paul Tuda. Your unwavering faith in me and this project continuously inspired and motivated me. I cannot possibly thank both of you enough!!\n\n\n\nI have been fortunate enough to have had three signicant mentors during my"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Freshwater and marine scientists make observations and gather data about the natural processes and ecology of aquatic organisms and their habitats. They formulate and test hypotheses on the factors that have influenced particular region to create its structure and also make predictions about future changes to the planet.\nAll of these steps in exploring the fisheries involve the acquisition and analysis of numerical data. A scientist therefore needs to have a firm understanding of statistical and numerical methods as well as the ability to utilize relevant computer software packages, in order to be able to analyze the acquired data.\nThis guide introduces some of the most important methods of data analysis employed in aquatic sciences and illustrates their use through examples using the R® programming language. These examples can then be used as recipes for the analysis of the reader’s own data, after having learned their application with synthetic data. This introductory chapter deals with data acquisition (Section 1.2), the various types of data (Section 1.3) and the appropriate methods for analyzing data (Section 1.4). We therefore first explore the characteristics of typical data sets and subsequently investigate the various ways of analyzing data using R."
  },
  {
    "objectID": "intro.html#data-collection",
    "href": "intro.html#data-collection",
    "title": "1  Introduction",
    "section": "1.2 Data Collection",
    "text": "1.2 Data Collection\nMost data sets in sciences have a very limited sample size and also contain a significant number of uncertainties. Such data sets are typically used to describe rather large natural phenomena, such as a length and size of fish, a optimum conditions for certain habitats or a stock size of certain fish species. The methods described in this guide aim to find a way of predicting the characteristics of a larger population from a much smaller sample. An appropriate sampling strategy is the first step towards obtaining a good data set. The development of a successful strategy for field sampling requires decisions on the sample size and the spatial sampling scheme.\n\n\n\n\n\n\nTip\n\n\n\nA population is the entire group that you want to draw conclusions about. A sample is the specific group that you will collect data from.\n\n\nThe sample size includes the sample volume, the sample weight and the number of samples collected in the field. The sample weights or volumes can be critical factors if the samples are later analyzed in a laboratory and most statistical methods also have a minimum requirement for the sample size. The sample size also affects the number of sub-samples that can be collected from a single sample. If the population is heterogeneous then the sample needs to be large enough to represent the population’s variability, but on the other hand samples should be as small as possible in order to minimize the time and costs involved in their analysis.\nFigure 1.1 shows a schematic that underpins the concepts of population versus sample. The lefthand side represents a population that, in statistics, is assumed to follow an underlying but unknown distribution. The only thing available is the sample data and its empirical distribution, shown on the righthand side. To get from the lefthand side to the righthand side, a sampling procedure is used (represented by an arrow). Traditional statistics focused very much on the lefthand side, using theory based on strong assumptions about the population. Modern statistics has moved to the righthand side, where such assumptions are not needed.\n\nknitr::include_graphics(\"images/population_sample.png\")\n\n\n\n\nFigure 1.1: Population and sample with their distributions\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe collection of smaller pilot samples is recommended prior to defining a suitable sample size.\n\n\nThe design of the spatial sampling scheme is dependent on the availability of outcrops or other material suitable for sampling. Sampling in coral reefs typically leads to clustered data, whereas sampling along shoreline cliffs results in one-dimensional traverse sampling schemes. A more uniform sampling pattern can be designed where there is 100% exposure or if there are no financial limitations. A regular sampling scheme results in a gridded distribution of sample locations, whereas a uniform sampling strategy includes the random location of a sampling point within a grid square. Although these sampling schemes might be expected to provide superior methods for sampling collection, evenly-spaced sampling locations tend to miss small-scale variations in the area.\nThe correct sampling strategy will depend on the objectives of the investigation, the type of analysis required and the desired level of confidence in the results. Having chosen a suitable sampling strategy, the quality of the sample can be influenced by a number of factors resulting in the samples not being truly representative of the larger population. It is therefore recommended that the quality of the samples, the method of data analysis employed and the validity of the conclusions drawn from the analysis be checked at each stage of the investigation."
  },
  {
    "objectID": "intro.html#types-of-data",
    "href": "intro.html#types-of-data",
    "title": "1  Introduction",
    "section": "1.3 Types of Data",
    "text": "1.3 Types of Data\nMost data sets consist of numerical measurements, although some information can also be represented by a list of names such as monsoon seasons (northeast and southeast) and species (Fig. 1.3). The available methods for data analysis may require certain types of data sets. These are\n\nnominal data – Information in sciences is sometimes presented as a list of names, e.g., the various fish species collected from a seagrass bed identified in a thin section. In some studies, these data are converted into a binary representation, i.e., one for present and zero for absent. Special statistical methods are available for the analysis of such data sets.\nordinal data – These are numerical data representing observations that can be ranked, but in which the intervals along the scale are irregularly spaced. A classical example or ordinal data is the maturity stage of fish determined from gonads, which is juvenile (I) to spend (V).\nratio data – These data are characterized by a constant length of successive intervals, therefore offering a great advantage over ordinal data. The zero point is the natural termination of the data scale, and this type of data allows for either discrete or continuous data sampling. Examples of such data sets include length or weight data.\ninterval data – These are ordered data that have a constant length of successive intervals, but in which the data scale is not terminated by zero. Temperatures C and F represent an example of this data type even though arbitrary zero points exist for both scales. This type of data may be sampled continuously or in discrete intervals.\nspatial data – These are collected in a 2D or 3D study area. The spatial distribution of a certain fish species, the spatial variation in chlorophyll-a concentration and the distribution of coral reefs are examples of this type of data."
  },
  {
    "objectID": "intro.html#methods-of-data-analysis",
    "href": "intro.html#methods-of-data-analysis",
    "title": "1  Introduction",
    "section": "1.4 Methods of Data Analysis",
    "text": "1.4 Methods of Data Analysis\nData analysis uses precise characteristics of small samples to hypothesize about the general phenomenon of interest. Which particular method is used to analyze the data depends on the data type and the project requirements. The various methods available include:\n\nUnivariate methods – Each variable is assumed to be independent of the others and is explored individually. The data are presented as a list of numbers representing a series of points on a scaled line. Univariate statistical methods include the collection of information about the variable, such as the minimum and maximum values, the average, and the dispersion about the average. This information is then used to attempt to infer the underlying processes responsible for the variations in the data. Examples are the effects of temperature on the growth of certain fish species.\nBivariate methods – Two variables are investigated together to detect relationships between these two parameters. For example, the correlation coefficient may be calculated to investigate whether there is a linear relationship between two variables. Alternatively, the bivariate regression analysis may be used to find an equation that describes the relationship between the two variables. An example of a bivariate plot is relationship between total length and weight of certain fish species.\nTime-series analysis – These methods investigate data sequences as a function of time. The time series is decomposed into a long-term trend, a systematic (periodic, cyclic, rhythmic) component and an irregular (random, stochastic) component. A widely used technique to describe cyclic components of a time series is that of spectral analysis. Examples of the application of these techniques include the investigation of cyclic climatic variations in aquatic organisms.\nSpatial analysis – This is the analysis of parameters in 2D or 3D space and hence two or three of the required parameters are coordinate numbers. These methods include descriptive tools to investigate the spatial pattern of geographically distributed data. Other techniques involve spatial regression analysis to detect spatial trends. Also included are 2D and 3D interpolation techniques, which help to estimate surfaces representing the predicted continuous distribution of the variable throughout the area. Examples are spatial distribution of oceanographic variables such as temperature, wind, etc.\nImage processing – The processing and analysis of images has become increasingly important in sciences. These methods involve importing and exporting, compressing and decompressing, and displaying images. Image processing also aims to enhance images for improved intelligibility, and to manipulate images in order to increase the signal-to-noise ratio. Advanced techniques are used to extract specific features or analyze shapes and textures, such as for counting microbes in microscope images. Another important application of image processing is in the use of satellite remote sensing to map oceanographic variables and mangrove vegetation.\nMultivariate analysis – These methods involve the observation and analysis of more than one statistical variable at a time. Since the graphical representation of multidimensional data sets is difficult, most of these methods include dimension reduction. Multivariate methods are widely used on ecological data, for instance an important usage is in the comparison of species assemblages in ocean sediments for the reconstruction of paleoenvironments.\n\n\n\n\n\n\n\nTip\n\n\n\nSome of these methods of data analysis require the application of numerical methods such as interpolation techniques."
  },
  {
    "objectID": "intro.html#references",
    "href": "intro.html#references",
    "title": "1  Introduction",
    "section": "References",
    "text": "References"
  }
]