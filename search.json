[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "WIO-fish",
    "section": "",
    "text": "The wio-fishStats guide and course provide an introduction to data science that is tailored to the needs of fisheries, but is also suitable for freshwater and marine scientists and other biological or social sciences. This audience typically has some knowledge of statistics, but rarely an idea how data is prepared and shaped to allow for statistical testing.\nBy using various data types and working with many examples, the book will guide on transforming, summarizing, and visualizing data. By keeping our eyes open for the perils of misleading representations, the book fosters fundamental skills of data literacy and cultivates reproducible research practices that enable and precede any practical use of statistics.\nThis guide provides datasets and functions that help people want to code fisheries data in R language. Its source code is hosted at https://github.com/lugoga/fishstats. The book and course introduce the principles and methods of data science for fisheries scientists and other biological or social sciences. The guide is available at https://github.com/lugoga/fishstats/manual.\n\n\n\nScan the qrcode to access the interactive web app"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "To raise awareness of countries in the Western Indian Ocean (WIO) in monitoring their progress toward the achievement of the Sustainable Development Goals (SDGs), the FAO has partnered with WIOMSA to support capacity-building activities in collection, monitoring, and assessment of fisheries data for the WIO region. In general, these activities aim to strengthen the workflow from fisheries data collection to support the monitoring and reporting of SDG 14.4.1.\nThe prime goal of this initiative align with the the United Nations Decade of Ocean Science for Sustainable Development (2021-2030, Ocean Decade for short), which focus to support a new cooperative framework to ensure that global ocean science provides greater benefits for ocean ecosystems and wider society.\nDecade provides a common framework to ensure that ocean science can fully support countries’ actions to sustainably manage the ocean and more particularly to achieve the 2030 Agenda for Sustainable Development – through the creation of a new foundation, across the science-policy interface, to strengthen the management of the ocean and coasts for the benefit of humanity.\nA vast majority of the ocean remains unmapped, unobserved and unexplored. Our understanding of the ocean and its contribution to sustainability largely depends on our capacity to conduct effective ocean science - through research and sustained observations, supported by adequate infrastructures and investments.\nThis decade of the ocean comes down with a slogan the science we want for the ocean we need! But we flip it a little bit and should read the information we want for the ocean we need, why. Because we are at the time of precedented generation of data than ever before. Data generated from sensors to satellite is enormous.\nIn the WIO, small-scale fisheries dominate and are crucial for the livelihoods of coastal communities in the region, contributing to the twin imperatives of poverty reduction and economic development. However, these fisheries are dispersed, open-access in nature, multispecies and multi-gear, making their monitoring and determination of stock status for individual species incredibly challenging as data are insufficient for conventional stock assessment routines.\nEfforts to determine stock status and provide evidence-based fisheries management advice are beset with problems, including insufficient or inadequate scientific data and expertise, which are compromised by economic and socio-political realities. For example, the regional state of the coast report for the western Indian Ocean (WIO) states that almost all countries in the region cannot adequately assess their marine resources and lack the financial capacity and technical expertise for effective management. Important impediments to fisheries management in the region relate to the following aspects:\nThese have been highlighted as some of the impediments to meeting the targets of SDG 14.\nAlthough the national fisheries institutes in the region do collect data on their fisheries, these data are often not detailed to an adequate granularity (e.g. regarding time, space, fleet or species levels), are not properly organized or linked, lack sufficient quality assurance and control, or are very difficult to retrieve for analysis. Additionally, there is often very limited technical capacity for managing, accessing and extracting data in a way that can be used for analysis.\nThus, even though data may exist, they often remain underutilized, strongly limiting the possibility of applying even data-limited approaches to stock monitoring. As such, analyses of stock status may be missing or be based on inappropriate metrics and methods, thereby hindering the formulation of relevant policies for the sector.\nIn its role as custodian agency of the SDG 14 indicators, FAO has a mandate to support countries to strengthen their capacities to collect, process, analyse and report data while ensuring that different national data sets are comparable and can be aggregated at sub-regional, regional and global levels to monitor the SDGs."
  },
  {
    "objectID": "intro.html#initiatives-conducted-in-the-wio",
    "href": "intro.html#initiatives-conducted-in-the-wio",
    "title": "1  Introduction",
    "section": "Initiatives conducted in the WIO",
    "text": "Initiatives conducted in the WIO\nTwo workshops have been undertaken in the WIO/East African region, first a project kick-off meeting of the Fishing Data East Africa (FIDEA) project (Dar es Salaam, Tanzania, 16-17 September 2019), focused on Tanzania-Mainland, Tanzania-Zanzibar and Mozambique, at which FAO agreed to partner with FIDEA to support a capacity development workshop in the East Africa region. This led to a mission by FAO staff to investigate the data infrastructure in Tanzania-Mainland, Tanzania-Zanzibar and Mozambique, as well as the SDG 14.4.1 reporting capacity development workshop in Zanzibar from 2-14 March 2020, which included 10 East African countries and WIO island nations.\nThe report of this second workshop recommended some key actions to support countries in improving the collection and use of data for monitoring the SDG 14.4 target. A common point in these recommendations is a greater need to provide long-term support for developing appropriate data management systems. This goes beyond collection to focus particularly on validation, organization, protection, retrieval and summary of the data, essential steps for allowing reliable estimation and reporting of the SDG14.4.1 indicator.\nIt has been stated many times that The collection of data is not an end in itself, but is essential for informed decision-making, and data can only be useful for supporting decision-making if they are properly stored, managed, and curated so that they have the quality necessary for providing meaningful and reliable advice. The report also recommended stronger collaboration between the SWIOFC regional process in monitoring the status of stocks and national processes in developing capacities for the monitoring of SDG14.4.1."
  },
  {
    "objectID": "intro.html#objectives",
    "href": "intro.html#objectives",
    "title": "1  Introduction",
    "section": "Objectives",
    "text": "Objectives\nThe objective of this consultancy is to support development and refining of training tools related to fisheries data management workflow. This will involve working closely with the fisheries expert towards the development of training tools for enhancing stock monitoring status and national processes for SDG14.4.1 monitoring."
  },
  {
    "objectID": "intro.html#scope-of-the-work",
    "href": "intro.html#scope-of-the-work",
    "title": "1  Introduction",
    "section": "Scope of the work",
    "text": "Scope of the work\nIn executing this consultancy, the Consultant will work closely with WIOMSA and FAO and the following tasks will be undertaken for guided data management, analysis and reporting for decision making and management of the fisheries resources in the region using Excel spreadsheet and R language tools. This will include development of the Excel and R tools that help to reproduce lecture material and tutorials."
  },
  {
    "objectID": "rstudio.html",
    "href": "rstudio.html",
    "title": "2  Getting Started with R and RStudio",
    "section": "",
    "text": "Though R/RStudio may seem intimidating, it is actually quite straight forward to set up and, after learning a few basics, you can start running analyses and writing your own in no time. The objective of this guide is to provide an introduction to R/RStudio basics so that interested resource managers without programming experience can start leveraging R for their management decisions.\nR and RStudio are separate programs and that need to be installed and updated individually. If you do not keep both relatively up-to-date you will likely run into problems."
  },
  {
    "objectID": "rstudio.html#r",
    "href": "rstudio.html#r",
    "title": "2  Getting Started with R and RStudio",
    "section": "2.1 R",
    "text": "2.1 R\nTo install R, go to the R webpage and follow the link to your operating system of choice (Linux, Max OS X, Windows). Then, click on the most recent .pkg file to download it. Follow the instructions to complete the installation process."
  },
  {
    "objectID": "rstudio.html#rstudio",
    "href": "rstudio.html#rstudio",
    "title": "2  Getting Started with R and RStudio",
    "section": "2.2 RStudio",
    "text": "2.2 RStudio\nAfter installing R, visit the RStudio Products site and click the DOWNLOAD RSTUDIO DESKTOP button located partway down the page.\n\nNext, scroll to the bottom and click on the link under Installers that again corresponds to your operating system of choice.\n\nSave the file on your desktop. Once it finishes downloading, open the file and follow the instructions to complete the installation process. You may then delete the file.\nCongratulations! You successfully completed the installation process and are one step closer to using R and RStudio for analysis!"
  },
  {
    "objectID": "rstudio.html#helpful-resources",
    "href": "rstudio.html#helpful-resources",
    "title": "2  Getting Started with R and RStudio",
    "section": "2.3 Helpful Resources",
    "text": "2.3 Helpful Resources\n\nInstalling R and RStudio by Jenny Bryan\n\nRstudio is what’s referred to as an Integrated Development Environment (IDE) for the R programming language. It provides a single interface for an R user to manage all aspects of an analysis (write code, manage and plot data. see outputs, get help, etc.)."
  },
  {
    "objectID": "rstudio.html#rstudio-interface",
    "href": "rstudio.html#rstudio-interface",
    "title": "2  Getting Started with R and RStudio",
    "section": "2.4 RStudio Interface",
    "text": "2.4 RStudio Interface\nThere are four main panels in RStudio (Figure 1).\n\n\nCode Editor - This is where you write the code for your analysis. Each tab represents a different R script file (e.g. snapper_analysis.R)\nConsole - This is where R prints the output of your code when it’s run. You can also write code directly in the console after the > symbol\nEnvironment/History - This panel generally has the following two tabs:\n\nEnvironment - Displays all your data, variables, and user-defined functions. These are created by the user either in the code editor or directly in the console.\nHistory - A list of your command history\n\nFiles/Packages/Help/Viewer - This panel contains numerous helpful panels:\n\nFiles - The list of all files contained in your current working directory. You can also navigate to different folders on your computer. This is where you can click on different R scripts to open them in the code editor.\nPlots - When you produce plots with your code they will be displayed here\nPackages - The list of packages (groups of functions) currently installed on your computer. You can install new packages or update existing packages from this tab by clicking Install or Update.\nHelp - Where you can search the R documentation to get help using the different R functions. This is a very useful feature of RStudio! You can also get help for a function by typing ? followed by the function name in the console (e.g. ?data.frame())."
  },
  {
    "objectID": "rstudio.html#working-directory",
    "href": "rstudio.html#working-directory",
    "title": "2  Getting Started with R and RStudio",
    "section": "2.5 Working Directory",
    "text": "2.5 Working Directory\nThe working directory is an important concept in R (and programming in general) and refers to the current directory (folder) that you are working in. Basically, R requires that you tell it where in your computer’s file system it should start looking for files from. This is important because the code used to load data and save results and plots will differ depending on your current working directory.\nAs an example, let’s imagine you are working on an analysis of coral reef fisheries and you have a folder on your Desktop called reef_fish. Inside this reef_fish directory is the file reef_fish_data.csv that you want to analyze. Open RStudio and click Set Working Directory... under the Session menu in the toolbar. This asks you to specify the folder on your computer that R should consider to be the working directory.\n\n2.5.1 Pathnames (Path)\nNow that the working directory is set, you can load your reef fish data into RStudio by specifying the appropriate pathname to the file. In this case, the pathname is simply reef_fish_data.csv since the file is in the working directory and the data could be loaded with read.csv(file = \"reef_fish_data.csv\"). If, however, reef_fish_data.csv was actually stored in a subfolder called data the previous read.csv() command will not work because reef_fish_data.csv is not in the working directory. In this case, you can either tell R where the file is using the absolute, complete path (e.g. /Users/You/Desktop/reef_fish/data/reef_fish_data.csv), or with the path relative to the working directory (e.g. data/reef_fish_data.csv)."
  },
  {
    "objectID": "rstudio.html#section",
    "href": "rstudio.html#section",
    "title": "2  Getting Started with R and RStudio",
    "section": "2.6 ",
    "text": "2.6"
  },
  {
    "objectID": "Rlanguage.html",
    "href": "Rlanguage.html",
    "title": "3  R as a Programming Language",
    "section": "",
    "text": "You may or may not have used other programming languages before coming to R. Either way, R has several distinctive features which are worth noting."
  },
  {
    "objectID": "Rlanguage.html#data-frames",
    "href": "Rlanguage.html#data-frames",
    "title": "3  R as a Programming Language",
    "section": "3.1 Data frames",
    "text": "3.1 Data frames\nOne of R's greatest strengths is in manipulating data. One of the primary structures for storing data in R is called a Data Frame. Much of your work in R will be working with and manipulating data frames. Data frames are made up of rows and columns. The top row is a header and describes the contents of each column. Each row represents an individual data row or observation. Rows can also have names. Each row contains multiple cells which contain the content of the data.\nLet's look at a data frame that is included in R as an example. This data frame is called mtcars and contains some data about common car models. We can look at this data frame using the head function, which previews the first few rows. You can also use the functions colnames or rownames to get the column or row names of a data frame, respecitvely.\nhead(mtcars)\n\n##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nThe columns in data frames can contain different types of information. In this particular data frame, all columns contain numbers, as denoted by the indication <dbl>, a type of number that allows numbers after the decimal point. Columns could also be integers represented by <int> (which don't allow numbers after the decimal point), dates represented by <date>,booleans or logicals represented by lgl (TRUE or FALSE), characters represented by <chr> (these contain text), or factors represented by <fctr> (these are helpful for storing categorical information such as species names or gear types).\nThese days, you may also see the word tibble, which is a modern version of the R data frame and is being used more and more widely. Tibbles generally function much like data frames, but do away with some frustrating features and are generally cleaner. We recommend using tibbles."
  },
  {
    "objectID": "Rlanguage.html#vectors",
    "href": "Rlanguage.html#vectors",
    "title": "3  R as a Programming Language",
    "section": "3.2 Vectors",
    "text": "3.2 Vectors\nAnother very common data type is the vector, which stores 1-dimensional information, such as a list of numbers or characters. Vectors are built using the c function. Below are two examples:\nmyNumericVector <- c(1, 2, 3, 4)\n\nmyCharacterVector <- c(\"A\", \"B\", \"C\", \"D\")"
  },
  {
    "objectID": "Rlanguage.html#functions",
    "href": "Rlanguage.html#functions",
    "title": "3  R as a Programming Language",
    "section": "3.3 Functions",
    "text": "3.3 Functions\nR is a \"functional\" programming language, which means it gets much of its power by relying on the concept of functions. Functions are small chunks of code that can do a certain task. They require a certain number of inputs, and provide a certain number of outputs. They allow for common tasks to be performed easily and reproducibly.\nR contains many built-in functions, including several for helpful statistics, as shown below. In these examples, there are also some helpful comments to tell you what each line is doing in the code blocks below. Comments start with the # operation, and are not evaluated by R - they are simply there to document the code.\n# The sum function takes a vector of numbers as an input, calculates the sum of those numbers, and produces a single number as an output\nsum(c(1, 2, 3))\n\n## [1] 6\n\n# The mean function takes a vector of numbers as an input, calculates the mean of those numbers, and produces a single number as an\nmean(c(1 ,2, 3))\n\n## [1] 2\n\n# The median function takes a vector of numbers as an input, calculates the median of those numbers, and produces a single number as an\nmedian(c(1 ,2, 3))\n\n## [1] 2\nYou can even define your own functions, which can be an incredibly powerful way to save time when doing a task you anticipate needing to do many times. The example below shows how you would write a function that takes in two numbers as an input, manipulates the numbers, and then provides a single number as an output. Once you have defined a function, you can use it again later on.\n# Define the function\nmyFunction <- function(x,y){\n  z <- (x + 2 * y) / x\n  return(z)\n}\n# Test the function\nmyFunction(3,4)\n\n## [1] 3.666667"
  },
  {
    "objectID": "Rlanguage.html#tips-on-coding-and-style",
    "href": "Rlanguage.html#tips-on-coding-and-style",
    "title": "3  R as a Programming Language",
    "section": "3.4 Tips on coding and style",
    "text": "3.4 Tips on coding and style\nIt is helpful to code in a consistent manner. This will not only make your code readble by others, but will even be helpful for you as you revisit code you have previously written. Using consistent code stying also makes it much easier to collaborate with others. We highly recommend following Google's style guidelines for R."
  },
  {
    "objectID": "Rlanguage.html#helpful-resources",
    "href": "Rlanguage.html#helpful-resources",
    "title": "3  R as a Programming Language",
    "section": "3.5 Helpful resources",
    "text": "3.5 Helpful resources\n\nAn interactive tutorial on learning the basics of R programming\nMore information on data frames\nMore information on tibbles, the modern version of data frames\nFunctions\nGoogle's style guidelines"
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "4  Packages",
    "section": "",
    "text": "Packages are groups of functions that are designed to excel at certain tasks (making plots, standardizing dates, reading/writing large data files, etc.). Many useful packages come standard with R when you download it, however, many more are available online.\nTo install a new package, click on the “Install” button located under the “Packages” tab in RStudio. This will open a pop-up where you can search for and install R packages hosted on CRAN. Alternatively, if you know the name of the package you want to install, you can run install.packages('package_name').\nOnce installed, the packages you need for an analysis are loaded by the library('package_name') function.\nThe following packages are commonly used by UCSB for fisheries analyses:"
  },
  {
    "objectID": "packages.html#r-packages-for-fishery-analysis",
    "href": "packages.html#r-packages-for-fishery-analysis",
    "title": "4  Packages",
    "section": "4.1 R Packages for Fishery Analysis",
    "text": "4.1 R Packages for Fishery Analysis\nFortunately, there are already numerous R packages available that are specifically designed for different types of fisheries analyses. The following four packages all contain very useful functions, and numerous other packages are available. Before jumping into these packages, however, we recommend you become familiar with R and more basic analyses first. Also, keep in mind that R is an open-source language, which means that anyone can submit packages. You should therefore always be sure to read the package documentation and double check your work to be sure things are performing as expected.\n\nTropFishR - Fish stock assessment methods and fisheries models based on the FAO Manual “Introduction to tropical fish stock assessment” by P. Sparre and S.C. Venema. Focus is the analysis of length-frequency data and data-poor fisheries.\nDLMTool - Implementation of management procedures for data-limited fisheries\nLBSPR - Functions to run the Length-Based Spawning Potential Ratio (LBSPR) method. The LBSPR package can be used in two ways: 1) simulating the expected length composition, growth curve, and SPR and yield curves using the LBSPR model and 2) fitting to empirical length data to provide an estimate of the spawning potential ratio (SPR).\nfishmethods - Fishery science methods and models from published literature\nrfisheries— This package provides programmatic access to the openfisheries API. Open Fisheries is a platform that aggregates global fishery data and currently offers global fish capture landings from 1950 onwards (more data coming soon). Read more about that effort here.\nsizeMat—a package that allows to estimate morphometric and gonadal size at sexual maturity for organisms, usually fish and invertebrates. It includes methods for classification based on relative growth (using principal components analysis, hierarchical clustering, discriminant analysis), logistic regression (Frequentist or Bayes), parameters estimation and some basic plots.\nFSA—This package provides R functions to conduct typical introductory fisheries analyses. Example analyses that use FSA can be found in the Introductory Fisheries Analyses with R book (see note below) and on the Examples page of the fishR website.\nCatDyn Fishery Stock Assessment by Catch Dynamics Models. Based on fishery Catch Dynamics instead of fish Population Dynamics (hence CatDyn) and using high-frequency or medium-frequency catch in biomass or numbers, fishing nominal effort, and mean fish body weight by time step, from one or two fishing fleets, estimate stock abundance, natural mortality rate, and fishing operational parameters. It includes methods for data organization, plotting standard exploratory and analytical plots, predictions, for 100 types of models of increasing complexity, and 72 likelihood models for the data.\ncuttlefish.model – Perform LPUE standardization and stock assessment of the English Channel cuttlefish stock using a two-stage biomass model\nEGRET – An R-package for the analysis of long-term changes in water quality and streamflow, including the water-quality method Weighted Regressions on Time, Discharge, and Season\nfecR – Calculates fishing effort following the DG MARE Ad-Hoc Workshops on Transversal Variables in Zagreb (2015) and Nicosia (2016)\nFESta: Fishing effort standardization\nFSAWs – construct and validate standard weight (Ws) equations\n\nfishMod – fits models to catch and effort data\nfishmove – predict fish movement parameters\nfishualize: Color palettes based on fish species\nhafroAssmt – fisheries stock assessment at Hafro, the Marine Research Institute in Iceland\nkobe – methods for summarising results from stock assessments and Management Strategy Evaluations in the Kobe format\nLBSPR: Length-Based Spawning Potential Ratio – Simulate expected equilibrium length composition, YPR, and SPR using the LBSPR model. Fit the LBSPR model to length data to estimate selectivity, relative fishing mortality, and spawning potential ratio for data-limited fisheries.\nLeMaRns – Set-up, run, and explore the outputs of the Length-based Multi-species model (LeMans; Hall et al. 2006), focused on the marine environment. Introductory publication is here.\nmapplots – *Create simple maps; add sub-plots like pie plots to a map or any other plot; format, plot and export gridded data\nmarindicators: Functions to calculate indicators for four marine ecosystem attributes (Biodiversity, Ecosystem Structure and Functioning, Ecosystem Stability and Resistance to Perturbations, Resource Potential) and one marine ecosystem pressure (Fishing Pressure) from fishery independent trawl survey data and commercial landings data.\nmixdist – fit mixture distributions — e.g., possibly separating age-classes from a length frequency\nmizer – multispecies, trait based and community size spectrum ecological models, focused on the marine environment See Introduction to mizer\nMQMF: Companion for “Using R for Modelling and Quantitative Methods in Fisheries” book\nObsCovgTools – Estimates minimum fisheries observer coverage required to achieve a specific objective. Related Shiny app is here.\noceanmap – Plotting toolbox for 2D oceanographic data\noptistock – Functions that aid calculating the optimum time to stock hatchery-reared fish into a body of water given the growth, mortality, and cost of raising a particular number of individuals to a certain length.\nPBSmapping – two-dimensional plotting features similar to those commonly available in a Geographic Information System\nPBSmodelling – facilitates the design, testing, and operation of computer models; focused on tools that make it easy to construct and edit a customized graphical user interface\nRchivalTag – functions to generate, access and analyze standard data products from archival tagging data\nrct3 – Predict fish year-class strength by calibration regression analysis of multiple recruitment index series.\nr4ss – analyze and plot the output from Stock Synthesis version 3, a fisheries stock assessment model written in ADMB\nRFishBC – helps fisheries scientists collect growth data from calcified structures and back-calculate estimated lengths at previous ages\nrfishdraw – Automatic generation of fish drawings based on JavaScript library.\nscape – import and plot results from statistical catch-at-age models; generally used to connect with ADMB.\nselect – a flexible and user-extendable framework for size-selectivity via the SELECT model\nshapeR – study otolith shape variation among fish populations\nsimecol – implementation of dynamic simulation models\nskewtools – estimation of growth models parameters using the robust ECME method via heteroscedastic nonlinear regression growth model with scale mixture of skew-normal distributions\nss3sim – framework for fisheries stock assessment simulation testing with Stock Synthesis 3\nsmartR: Spatial management and assessment of demersal resources for trawl fisheries\nstacomiR – Graphical outputs and treatment for a database of fish pass monitoring\nswfscDAS: Southwest Fisheries Science Center shipboard DAS data processing\nswfscMisc – Collection of conversion, analytical, geodesic, mapping, and plotting functions used by the Southwest Fisheries Science Center\ntoxEval – functions to analyze, visualize, and organize measured concentration data as it relates to toxicity forecasting or other user-selected chemical-biological interaction benchmark data such as water quality criteria\ntRophicPosition – Bayesian estimation of trophic position from consumer stable isotope ratios\nVAST – estimate spatial variation in density using spatially referenced data, with the goal of habitat associations (correlations among species and with habitat) and estimating total abundance for a target species in one or more years\nvmsbase – vessel monitoring system and logbook data management and analysis\nX2R with FishGraph – import the structured output from a given numerical model written in a compiled language (e.g., ADMB, fortran) into R for postprocessing (graphing, further analysis)\nypr – yield-per-recruit analysis\n\n\nAll these packages are available on CRAN and can be downloaded directly in RStudio by running the following command:\n    install.packages(c('TropFishR','DLMTool', 'LBSPR', 'fishmethods'))"
  },
  {
    "objectID": "packages.html#packages-of-data",
    "href": "packages.html#packages-of-data",
    "title": "4  Packages",
    "section": "4.2 Packages of Data",
    "text": "4.2 Packages of Data\n\nFSAdata: Data for FSA – Collection of many fisheries-related data sets\nfishdata – Four datasets (fish size and per-day growth history) of migratory fish, from 2015-2016, in the Wellington region of New Zealand\nfishkirkko2015 – Length and weight measurements of fish species at Kirkkojarvi Lake, Finland\nfishbc: Raw and curated data on the codes, classification, and conservation status of freshwater fishes in British Columbia"
  },
  {
    "objectID": "packages.html#packages-to-access-online-databases",
    "href": "packages.html#packages-to-access-online-databases",
    "title": "4  Packages",
    "section": "4.3 Packages to Access Online Databases",
    "text": "4.3 Packages to Access Online Databases\n\nDATRAS – read and manipulate trawl survey data from the DATRAS database\nfishtree: interface to the Fish Tree of Life API to download taxonomies, phylogenies, fossil calibrations, and diversification rate information for ray-finned fishes\nLTERdata – functions for retrieving NTL LTER data\nramlegacy – Download, import, convert, and cache the RAM Legacy Stock Assessment Database\nrfishbase – a programmatic interface to fishbase.org\nrfisheries – a programmatic interface to OpenFisheries.org"
  },
  {
    "objectID": "packages.html#packages-for-limnological-data",
    "href": "packages.html#packages-for-limnological-data",
    "title": "4  Packages",
    "section": "4.4 Packages for Limnological Data",
    "text": "4.4 Packages for Limnological Data\n\nGLMr – the General Lake Model in R\nglmtools – Tools for interacting with the General Lake Model in R\nLakeMetabolizer – collection of lake metabolism functions\nrLakeAnalyzer – standardized methods for calculating common important derived physical features of lakes\nwaterData – Imports U.S. Geological Survey (USGS) daily hydrologic data from USGS web services, plots the data, addresses some common data problems, and calculates and plots anomalies"
  },
  {
    "objectID": "packages.html#additional-resources",
    "href": "packages.html#additional-resources",
    "title": "4  Packages",
    "section": "4.5 Additional Resources",
    "text": "4.5 Additional Resources\nThroughout this guidebook we provided links to additional useful R resources. We now include all of these resources below for your convenience.\n\nR for Data Science\nCookbook for R\nRStudio Cheat Sheets\nJenny Bryan’s Stat 545 Course Syllabus\nSimple Guidelines for Effective Data Management\nResearch Data Management"
  },
  {
    "objectID": "packages.html#helpful-resources",
    "href": "packages.html#helpful-resources",
    "title": "4  Packages",
    "section": "4.6 Helpful Resources",
    "text": "4.6 Helpful Resources\n\nJenny Bryan’s Guide to Installing R and RStudio"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "5  Data Entry and Management",
    "section": "",
    "text": "The careful entry, documentation, and management of data is essential to any data-related project. Being strategic about this process will keep the project organized, protect against data loss, and facilitate analysis and data sharing. This section will be most helpful if you are the person responsible for first setting up data collection forms and organization. However, it may also be helpful even if you are working with data that someone else has collected and organized. If you are working with data that someone else has collected and organized, you will find the next section on data cleaning (QA/QC, or quality assurance / quality control) especially helpful.\nBest Practices"
  },
  {
    "objectID": "data.html#helpful-resources",
    "href": "data.html#helpful-resources",
    "title": "5  Data Entry and Management",
    "section": "5.1 Helpful Resources",
    "text": "5.1 Helpful Resources\n\nhttps://www.nceas.ucsb.edu/files/news/ESAdatamng09.pdf\nhttp://ucsd.libguides.com/c.php?g=90957&p=585435"
  },
  {
    "objectID": "loadingData.html",
    "href": "loadingData.html",
    "title": "6  Loading Data and Data Cleaning",
    "section": "",
    "text": "Screening and cleaning your data to identify and fix any potential errors (missing data, typos, errors, etc.) is an important step before conducting any analyses. This is known as Quality Assurance/Quality Control, or QA/QC. This section includes an overview of steps that should be taken to properly screen your data and introduces some functions that can come in handy when cleaning your data. If you have a small dataset that won’t be updated often, screening and cleaning your data may be easiest in Microsoft Excel by sorting and filtering your data columns. However, we recommend performing your data cleaning using R. This has the advantage that all changes made to a raw dataset will be recorded in a script that is reproducible, which may be especially useful when working with large datasets, if you want to quickly modify any steps of your cleaning process, or if you receive additional data."
  },
  {
    "objectID": "loadingData.html#loading-data",
    "href": "loadingData.html#loading-data",
    "title": "6  Loading Data and Data Cleaning",
    "section": "6.1 Loading Data",
    "text": "6.1 Loading Data\nThroughout this section and the following fisheries analysis and plotting sections, we’ll use a common data set. First, create a new folder in your working directory called “_data”. Next, download the following file onto your computer. Right click on the link, and save it in your”_data” folder.\nRight click and save-link-as to download landings data\nLet’s read this dataset into R and determine the structure of the dataset. The landings_data data frame is from a fishery-dependent landing site survey. The species included in this data set is Caesio cuning, a yellowtail fusilier. We’ll be able to use these data to create length-frequency histograms that describe the size structure of the population, as well as trends in catch and CPUE.\nWe can look at the raw data just by typing landings_data.\nlandings_data <- read_csv(\"_data/sample_landings_data_raw.csv\")\n\nlandings_data\n\n## # A tibble: 7,214 x 8\n##       yy     dat  trip effort    gr             sp  l_cm      w_cm\n##    <int>   <chr> <int>  <int> <chr>          <chr> <dbl>     <dbl>\n##  1  2003 4/30/03     1     10  Trap Caesoi cunning    36 1089.1402\n##  2  2003 4/30/03     1     10  trap  Caesio cuning    29  565.3879\n##  3  2003 4/30/03     1     10  Trap  Caesio cuning    34  915.8276\n##  4  2003 4/30/03     1     10  Trap  Caesio cuning    36 1089.1402\n##  5  2003 4/30/03     1     10  Trap  Caesio cuning    34  915.8276\n##  6  2003 4/30/03     1     10  Trap Caesoi cunning    28  508.3185\n##  7  2003 4/30/03     1     10  Trap  Caesio cuning    30  626.6000\n##  8  2003 4/30/03     1     10  Trap  Caesio cuning    27  455.2443\n##  9  2003 4/30/03     1     10  Trap  Caesio cuning    33  836.5681\n## 10  2003 4/30/03     1     10  Trap  Caesio cuning    35  999.9688\n## # ... with 7,204 more rows\nYou’ll first notice that R calls this data frame a tibble, which is just another word for a nice clean version of a landings_data frame. This format is automatically used when you read in data using read_csv, which we always recommend. e can see that there are [7214] individual fish catch observations (rows) in our data frame and [8] variables (columns). The columns include the year and date when the measurement was collected, the fishing trip ID, how many hours were fished for each trip, what gear was used, the species, the length of the fish, and the weight of the fish."
  },
  {
    "objectID": "loadingData.html#data-structure",
    "href": "loadingData.html#data-structure",
    "title": "6  Loading Data and Data Cleaning",
    "section": "6.2 Data Structure",
    "text": "6.2 Data Structure\nFirst, let’s give our columns more descriptive column headings. We can rename columns using the rename function from the dplyr package. Let’s also convert the Date variable to a date format using the mdy function from the lubridate package. We start by taking the landings data frame we loaded into R, and working through a series of “pipes”, designated by the %>% operation, which progressively analyzes the data from one step to the next. Essentially, the output of one line is fed into the input of the next line.\n# Start with the landings_data data frame\nlandings_data <- landings_data %>%\n  # Rename the columns\n  rename(Year = yy,\n         Date = dat,\n         Trip_ID = trip,\n         Effort_Hours = effort,\n         Gear = gr,\n         Species = sp,\n         Length_cm = l_cm,\n         Weight_g = w_cm) %>%\n  # Turn the date column into a date format that R recognizes\n  mutate(Date = mdy(Date)) \n\nlandings_data\n\n## # A tibble: 7,214 x 8\n##     Year       Date Trip_ID Effort_Hours  Gear        Species Length_cm\n##    <int>     <date>   <int>        <int> <chr>          <chr>     <dbl>\n##  1  2003 2003-04-30       1           10  Trap Caesoi cunning        36\n##  2  2003 2003-04-30       1           10  trap  Caesio cuning        29\n##  3  2003 2003-04-30       1           10  Trap  Caesio cuning        34\n##  4  2003 2003-04-30       1           10  Trap  Caesio cuning        36\n##  5  2003 2003-04-30       1           10  Trap  Caesio cuning        34\n##  6  2003 2003-04-30       1           10  Trap Caesoi cunning        28\n##  7  2003 2003-04-30       1           10  Trap  Caesio cuning        30\n##  8  2003 2003-04-30       1           10  Trap  Caesio cuning        27\n##  9  2003 2003-04-30       1           10  Trap  Caesio cuning        33\n## 10  2003 2003-04-30       1           10  Trap  Caesio cuning        35\n## # ... with 7,204 more rows, and 1 more variables: Weight_g <dbl>"
  },
  {
    "objectID": "loadingData.html#missing-values",
    "href": "loadingData.html#missing-values",
    "title": "6  Loading Data and Data Cleaning",
    "section": "6.3 Missing values",
    "text": "6.3 Missing values\nNext, let’s check our data frame to determine if there are any missing values by subsetting observations (rows) in our dataframe that have missing values using the complete_cases function and the logical operator for negation, ! .\nlandings_data[!complete.cases(landings_data),]\n\n## # A tibble: 3 x 8\n##    Year       Date Trip_ID Effort_Hours     Gear       Species Length_cm\n##   <int>     <date>   <int>        <int>    <chr>         <chr>     <dbl>\n## 1  2003 2003-05-01      10           10     <NA> Caesio cuning    19.000\n## 2  2003 2003-05-01      10           10 Handline Caesio cuning    19.000\n## 3  2004 2004-12-18      NA            9     Trap Caesio cuning    20.104\n## # ... with 1 more variables: Weight_g <dbl>\nThere are 3 rows in our dataframe with missing values. If we want to remove observations with missing data from our dataset we can use the na.omit function which will remove any rows with missing values from our dataset:\nlandings_data <- na.omit(landings_data)\n\nlandings_data\n\n## # A tibble: 7,211 x 8\n##     Year       Date Trip_ID Effort_Hours  Gear        Species Length_cm\n##    <int>     <date>   <int>        <int> <chr>          <chr>     <dbl>\n##  1  2003 2003-04-30       1           10  Trap Caesoi cunning        36\n##  2  2003 2003-04-30       1           10  trap  Caesio cuning        29\n##  3  2003 2003-04-30       1           10  Trap  Caesio cuning        34\n##  4  2003 2003-04-30       1           10  Trap  Caesio cuning        36\n##  5  2003 2003-04-30       1           10  Trap  Caesio cuning        34\n##  6  2003 2003-04-30       1           10  Trap Caesoi cunning        28\n##  7  2003 2003-04-30       1           10  Trap  Caesio cuning        30\n##  8  2003 2003-04-30       1           10  Trap  Caesio cuning        27\n##  9  2003 2003-04-30       1           10  Trap  Caesio cuning        33\n## 10  2003 2003-04-30       1           10  Trap  Caesio cuning        35\n## # ... with 7,201 more rows, and 1 more variables: Weight_g <dbl>\nChecking the data structure again, we can see that the 3 rows containing NA values have been removed from our dataframe. You may not always wish to remove NA values from a dataset, if you still want to keep other values in that observation. Even if you want to keep observations with NA values in the dataset, it is still good to identify NAs and know where they occur to ensure they don’t create problems during analyses."
  },
  {
    "objectID": "loadingData.html#typos",
    "href": "loadingData.html#typos",
    "title": "6  Loading Data and Data Cleaning",
    "section": "6.4 Typos",
    "text": "6.4 Typos\nWe can check for typos by using the unique function, which will tell us all of the unique values found within a particular column. As an example, let’s look at the Gear variable.\nunique(landings_data$Gear)\n\n## [1] \"Trap\"     \"trap\"     \"Muroami\"  \"Handline\" \"Gillnet\"  \"Trolling\"\n## [7] \"Speargun\"\nThe gear variable has 7 unique values, however, we know there should only be 6 gears present in the dataset. We can see that “trap” appears twice because capitalization was inconsistent. The lower case ‘t’ causes R to read it as a unique value gear type. We can fix this by making sure all of our values in the Gear variable are consistent and have all lowercase letters using the tolower function. Alternatively, we could change them to all uppercase using the toupperfunction.\nlandings_data <- landings_data %>%\n  mutate(Gear = tolower(Gear))\n\nunique(landings_data$Gear)\n\n## [1] \"trap\"     \"muroami\"  \"handline\" \"gillnet\"  \"trolling\" \"speargun\"\nNow we have the correct number (6) of unique gears in our dataset.\nNow, let’s check another our Species variable for typos:\nunique(landings_data$Species)\n\n## [1] \"Caesoi cunning\" \"Caesio cuning\"\nThe species is showing 2 unique values, but we know there should only be one species in our dataset. It appears there is a spelling error on one of our species names. We can check how many times each of the 2 species spellings occurs in our dataset by using the nrow function on a filtered subset of data for each of the two Species values:\nlandings_data %>%\n  filter(Species == \"Caesoi cunning\") %>%\n  nrow()\n\n## [1] 2\n\nlandings_data %>%\n  filter(Species == \"Caesio cuning\") %>%\n  nrow()\n\n## [1] 7209\nIt looks like “Caesoi cunning” likely the typo because it only appears twice in our dataset, while “Caesio cunning” appears (7209) times. We can fix this by replacing the misspelled Species value and replacing it with the value that is spelled correctly. We do this using mutate and replace.\nlandings_data <- landings_data %>%\n  mutate(Species = replace(Species,Species == \"Caesoi cunning\", \"Caesio cuning\"))\n\nunique(landings_data$Species)\n\n## [1] \"Caesio cuning\"\nNow we have only one species value in our Species variable in our dataset, which is correct. The unique values of all categorical columns (i.e., gear type, species name, etc) should be examined during the data screening and cleaning process."
  },
  {
    "objectID": "loadingData.html#errors",
    "href": "loadingData.html#errors",
    "title": "6  Loading Data and Data Cleaning",
    "section": "6.5 Errors",
    "text": "6.5 Errors\nErrors in numeric/integer values may be caused from typos during data entry or from an error during the data collection process (for example, maybe the scale was broken or not zeroed out before weighing). To look at the range and distribution of a numeric variable, the summary function can be used.\nsummary(landings_data$Length_cm)\n\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    2.00   23.00   25.00   25.81   27.00 2400.00\nLooks like we have a max Length_cm value that is order of magnitude higher than the mean and median values. Visualizing numeric data is another great way to screen continuous data and identify data outlines that may be caused from errors in the dataset:\nplot(landings_data$Length_cm)\n We can clearly see there is an outlier in our data (upper left corner of the plot). We are not sure how this error occurred, but we know that this is not correct. In fact, we know that the maximum possible size of our species 100 cm. We know that a measurement or typo error must have occurred for any Length_cm values that are over 100 cm We can remove these erroneous data by only including observations in our dataset with values over 100 cm (species maximum size) using the filter function:\nlandings_data <- landings_data %>%\n  filter(Length_cm < 100)\n\nplot(landings_data$Length_cm)\n Now all of our data contains accurate length observations that are in the range of our species length. This process of plotting and examining should be conducted for each of our numeric variables before conducting any analyses to identify any outliers and to remove any erroneous data. In this example, we will skip this step for the Weight_g and Effort_Hours column, although you may wish to do this as a learning exercise on your own."
  },
  {
    "objectID": "loadingData.html#saving-clean-data",
    "href": "loadingData.html#saving-clean-data",
    "title": "6  Loading Data and Data Cleaning",
    "section": "6.6 Saving clean data",
    "text": "6.6 Saving clean data\nNow that we have completed our data cleaning and screening, let’s examine the structure of our data frame again:\nlandings_data\n\n## # A tibble: 7,208 x 8\n##     Year       Date Trip_ID Effort_Hours  Gear       Species Length_cm\n##    <int>     <date>   <int>        <int> <chr>         <chr>     <dbl>\n##  1  2003 2003-04-30       1           10  trap Caesio cuning        36\n##  2  2003 2003-04-30       1           10  trap Caesio cuning        29\n##  3  2003 2003-04-30       1           10  trap Caesio cuning        34\n##  4  2003 2003-04-30       1           10  trap Caesio cuning        36\n##  5  2003 2003-04-30       1           10  trap Caesio cuning        34\n##  6  2003 2003-04-30       1           10  trap Caesio cuning        28\n##  7  2003 2003-04-30       1           10  trap Caesio cuning        30\n##  8  2003 2003-04-30       1           10  trap Caesio cuning        27\n##  9  2003 2003-04-30       1           10  trap Caesio cuning        33\n## 10  2003 2003-04-30       1           10  trap Caesio cuning        35\n## # ... with 7,198 more rows, and 1 more variables: Weight_g <dbl>\nWe now have [7208] observations, with [8] variables, and with each variable being the correct data type. We can compare this to our raw dataset and see that we removed 6 observations (3 observations had missing values and 3 had error). This script may come in handy if, for example, we realize that the maximum size of our species is actually 200 cm (not 100 cm). We will know that our dataset does not include any length observations over 100 cm because we have documented our cleaning process and can easily go back to this script and change the 100 to a 200 and rerun this script. If we receive more data, we can also simply re-run this script, and all data cleaning steps will be performed again automatically.\nWe can save this dataset using a new name so that we have a copy of both the raw, and clean data. Now, we are ready to summarize and analyze our clean dataset.\nwrite_csv(landings_data,\"_data/sample_landings_data_clean.csv\")"
  },
  {
    "objectID": "loadingData.html#helpful-resources",
    "href": "loadingData.html#helpful-resources",
    "title": "6  Loading Data and Data Cleaning",
    "section": "6.7 Helpful Resources",
    "text": "6.7 Helpful Resources\n\nIntroduction to data cleaning with R\nA data cleaning example\nRemoving outliers"
  },
  {
    "objectID": "sample.html",
    "href": "sample.html",
    "title": "7  Sample Data in R",
    "section": "",
    "text": "require(tidyverse)\nrequire(magrittr)\nWe rarely have complete data about something we want to know. For example, we often want to know who will win a political election, but we do not have the time to ask every potential voter who they plan to vote for. And people’s attitudes change during a campaign.\nHowever, it is possible to take get the opinion of a small number of people and make an estimate of the possible outcomes of the election.\nThat small number of people is called a sample, and the estimate we make about the broader population is called an inference."
  },
  {
    "objectID": "sample.html#types-of-sampling",
    "href": "sample.html#types-of-sampling",
    "title": "7  Sample Data in R",
    "section": "7.1 Types of Sampling",
    "text": "7.1 Types of Sampling\nThe method used to determine what subset of people or things you base your inference on is called sampling.\nThere are two broad types of sampling techniques, and a number of subtypes within those broad types.\nProbability sampling is when any member of the target population has a known probability of being included in the sampled population. Such techniques include:\n\nSimple random sampling involves randomly selecting members of the population. There is an equal probability that any member of the For example, a random number generator (like the Excel RAND() function) can be used to select a subset of numbers from a phone number list that covers the population. In the natural sciences, an example might be placing remote cameras at random locations in an area when trying to get a count of the population of a specific type of animal in that area\nSystematic sampling involves selecting members from the target population at a regular interval. For example, every 10th number from a sorted list of addresses Stratified sampling involves taking random samples from different subgroups of the target population and then using known information about the size or characteristics of those subgroups to adjust the results. For example, if studying the opinions about college students toward a university policy, different classes (freshmen, sophomores, etc) or majors might have different opinions about that policy. Separating calculations into different sub-groups can be useful for discerning the differences between those groups Nonprobability sampling is used in cases where probability sampling is impractical. While you cannot use statistical techniques to make clear inferences about the broader population from these types of samples, descriptive results from these samples can offer suggestions of research paths that would justify the expense and difficulty of further probability sampling. Some common subtypes of nonprobability sampling include:\nConvenience sampling involves sampling a group of people you can conveniently access. For example, American college students are some of the most studied populations in history. College students tend to be young and from a specific set of social classes, so they cannot be said to be a random sample of the broader population. However, college students are readily accessible to the academic researchers that are their professors, and they often are happy to be research subjects in exchange for pizza or other relatively small amounts of compensation\nPurposive sampling is a variant on convenience sampling that is commonly used when information about a specific subgroup of the broader population is needed. For example, if information about the opinions of men are needed, volunteers might stand on a busy street corner and specifically target only men that pass by for further inquiry (and likely rejection)\nSnowball sampling involves asking sampled subjects for recommendations of other people that might fit a specific profile. For example, if studying a relatively small ethnic group, asking a subject for a recommendation on friends and family members that might participate would make it easier to find other members of that ethnic group"
  },
  {
    "objectID": "sample.html#descriptive-statistics",
    "href": "sample.html#descriptive-statistics",
    "title": "7  Sample Data in R",
    "section": "7.2 Descriptive Statistics",
    "text": "7.2 Descriptive Statistics\n\npeng = palmerpenguins::penguins\npeng |> glimpse()\n\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen, Torgerse~\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, ~\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, ~\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186~\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ~\n$ sex               <fct> male, female, female, NA, female, male, female, male~\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007~\n\n\nThe range() of values as well as the mean() and median() are standard descriptive statistics to run on any quantitative variable.\n\npeng %>% pull(bill_length_mm) %>% range(na.rm = TRUE)\n\n[1] 32.1 59.6\n\npeng %>% pull(bill_length_mm) %>% mean(na.rm = TRUE)\n\n[1] 43.92193\n\npeng %>% pull(bill_length_mm) %>% median(na.rm = TRUE)\n\n[1] 44.45\n\npeng %>% pull(bill_length_mm) %>% sd(na.rm = TRUE)\n\n[1] 5.459584\n\n\nThe distribution of values across the range can be assessed with a histogram and/or density plot.\n\npeng %>% \n  ggplot(aes(x = bill_length_mm))+\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite values (stat_bin).\n\n\n\n\n\nFigure 7.1: Distribution of bill length shown by histogram\n\n\n\n\n\npeng %>% \n  ggplot(aes(x = bill_length_mm))+\n  geom_density(aes(y = ..count..))+\n  geom_histogram(alpha = .2, fill = \"darkorchid\")\n\nWarning: Removed 2 rows containing non-finite values (stat_density).\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite values (stat_bin).\n\n\n\n\n\nFigure 7.2: Distribution of bill length shown by density"
  },
  {
    "objectID": "sample.html#categorical-variables",
    "href": "sample.html#categorical-variables",
    "title": "7  Sample Data in R",
    "section": "7.3 Categorical Variables",
    "text": "7.3 Categorical Variables\nThe table() function shows the counts of different values in a categorical variable.\n\npeng %$%\n  table(species)\n\nspecies\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n\nA table() can also be visualized as a barplot() with values converted to percents\n\npeng %>% \n  ggplot(aes(x = species)) +\n  geom_bar()\n\n\n\n\nFigure 7.3: barplot of a categorical species of palmer penguins"
  },
  {
    "objectID": "sample.html#confidence-interval",
    "href": "sample.html#confidence-interval",
    "title": "7  Sample Data in R",
    "section": "7.4 Confidence Interval",
    "text": "7.4 Confidence Interval\nIf we sample a characteristic of a population that can be represented as a continuous variable, we can take the mean of that sample and get the sample mean. But how confident can we be that sample mean is anywhere close to the population mean?\nA curious and wondrous fact about random sampling is that if you take a sufficient number of random samples (usually 30 or more) from any kind of distribution (normal or not), the distribution of deviations (errors) between the sampled values and the population mean will be a normal distribution.\nThis is the central limit theorem, which was originally proposed by French mathemetician Abraham de Moive in 1733 and later developed by French mathemetician Pierre-Simon Laplace in 1812 and Russian mathemetician Aleksandr Lyapunov in 1901.\nThe implication of the central limit theorem is that you can use a standard deviation and the rules of probability to calculate confidence intervals and assess how reliable your sampling results are. The central limit theorem is one of the most important theorems in statistics.\nFor example, within a normal distribution:\n\nWe know that around 68% of the values are within one standard deviation of the mean and we know that 99.7% of the values are within two standard deviations of the mean.\nTherefore, we can be 68% certain that our sample mean is within one standard deviation of the population, and 99.7% certain that our sample mean is within two standard deviations of the population.\n\n\n7.4.1 Standard Error\nSince our primary interest is is a potential amount of error between the sample mean and population mean, we can use the standard deviation of the sample and the size of the sample to calculate the standard error. The standard error is used to estimate how far your sampled data may differ from the population data.\nStandard error for a sample mean (x) is calculated as the standard deviation of the population (σ) divided by the square root of the sample size. The greater the sample size, the closer the sample mean can be assumed to get to the actual population mean.\n\\(\\sigma \\bar x = \\frac{\\sigma}{\\sqrt{n}}\\)\nSince we rarely know the standard deviation of the population (which is why we’re sampling in the first place) we estimate the standard error using the standard deviation of the sample (s):\n\\(\\sigma \\bar x = \\frac{S}{\\sqrt{n}}\\)"
  },
  {
    "objectID": "sample.html#confidence-intervals-for-means",
    "href": "sample.html#confidence-intervals-for-means",
    "title": "7  Sample Data in R",
    "section": "7.5 Confidence Intervals For Means",
    "text": "7.5 Confidence Intervals For Means\nThe confidence level you want determines how many standard errors above or below the mean you are willing to accept. The most common confidence levels are 95% and 99% (Investopedia 2022).\nIn a normal distribution, we know that 95% of the values (2.5 percentile to 97.5 percentile) are within 1.96 standard deviations (a z-score of ±1.96) above or below the mean.\nAccordingly, if we have a random sample, we can be 95% confident (confidence interval) that the actual population mean is within 1.96 standard errors above or below the sample mean (s).\n\\(d\\pm1.96\\times\\sigma\\bar x\\)\nThe confidence interval is sometimes described in terms of margin of error.\n\nThe confidence interval is the whole range of values above and below the mean.\nThe margin of error is the difference between the mean and the bottom or top of the confidence interval (z-score times standard error).\n\nUsing the GPA data in the sample data above, the confidence interval based on standard error can be calculated in R:\n\nbl  = peng %>% pull(bill_length_mm)\n\nn = length(bl)\n\nstderr = sd(bl, na.rm = TRUE) / sqrt(n)\n\nmoe = round(1.96 * stderr, 2)\n\nprint(paste(\"The margin of error is\", moe))\n\n[1] \"The margin of error is 0.58\"\n\nbl.mean = round(mean(bl, na.rm = TRUE), 2)\n\nprint(paste(\"The 95% confidence interval is\", bl.mean - moe, \" to \",bl.mean + moe))\n\n[1] \"The 95% confidence interval is 43.34  to  44.5\"\n\n\nThe MOE can be visualized with a semitransparent rect() around the mean line.\n\nplot(density(bl, na.rm = TRUE), lwd=3, col=\"navy\")\n\nrect(xleft = bl.mean - moe, ybottom = -1, xright = bl.mean + moe, \n    ytop = 1, border=NA, col=\"#00000020\")\n\nabline(v = bl.mean, lwd=3, col=\"darkred\")"
  },
  {
    "objectID": "sample.html#confidence-intervals-for-proportions",
    "href": "sample.html#confidence-intervals-for-proportions",
    "title": "7  Sample Data in R",
    "section": "7.6 Confidence Intervals for Proportions",
    "text": "7.6 Confidence Intervals for Proportions\nIf your sample data is dichotomous (e.g. Mac vs PC), or categorical that can be expressed as dichotomous (e.g. people whose favorite fruit is apple), what you are estimating is the population proportion in each group (x% use MAC, y% use PC). In that case, the confidence interval is:\n\\(p\\pm Z \\times \\sigma_p\\)\nbased on the standard error for proportions:\n\\(\\sigma_p = \\sqrt{p \\times (1-p)/n}\\)\nWhere:\nZ is the z-score for the desired confidence interval (1.96 for a 95% level of confidence) p is the proportion from the sample n is the size of the sample\nFor example, in a survey of 53 students in regard to their laptop operating system:\n\nprop = peng %>% pull(species) %>% table() %>% prop.table() \nstderr = sqrt(prop - (1 - prop)/nrow(peng))\nmoe = 1.96 * stderr\n\npaste0(\"Estimated \", names(prop), \" species = \", \n    round(100 * (prop - moe)), \"% to \", \n    round(100 * (prop + moe)), \"%\")\n\n[1] \"Estimated Adelie species = -86% to 174%\"   \n[2] \"Estimated Chinstrap species = -67% to 106%\"\n[3] \"Estimated Gentoo species = -81% to 153%\""
  },
  {
    "objectID": "sample.html#confidence-interval-for-means",
    "href": "sample.html#confidence-interval-for-means",
    "title": "7  Sample Data in R",
    "section": "7.7 Confidence Interval for Means",
    "text": "7.7 Confidence Interval for Means\nT is used instead of the z-score to calculate a margin of error for sample means or proportions:\n\\(s \\pm t\\times \\sigma \\bar x\\)\n\ndf = nrow(peng) -1\n\nt = qt(p = 0.95, df = df)\n\nstderr = sd(peng$bill_length_mm, na.rm = TRUE)/sqrt(nrow(peng))\nmoe = round(t * stderr, 2)\nmean.peng = mean(peng$bill_depth_mm, na.rm = TRUE)\npaste(\"Estimated average length is\", mean.peng - moe, \"to\", mean.peng + moe)\n\n[1] \"Estimated average length is 16.6611695906433 to 17.6411695906433\""
  },
  {
    "objectID": "sample.html#confidence-interval-for-proportions",
    "href": "sample.html#confidence-interval-for-proportions",
    "title": "7  Sample Data in R",
    "section": "7.8 Confidence Interval for Proportions",
    "text": "7.8 Confidence Interval for Proportions\n\\(p \\pm t \\times \\sigma_p\\) p ± t * σp\nUsing the example above of a survey where 40% of respondendents indicated using a Mac rather than a PC for their home computer or laptop, we can see the wider margin of error for proportions associated with a small sample:\n\nleo = table(peng$species)\n\nleo = leo / sum(leo)\n\nstderr = sqrt(leo * (1 - leo) / nrow(peng))\n\nt = qt(0.95, nrow(peng) - 1)\n\nmoe = t * stderr\n\npaste0(\"Estimated \", names(leo), \" users (small sample) = \", \n    round((leo - moe) * 100, 1), \"% - \", \n    round((leo + moe) * 100, 1), \"%\")\n\n[1] \"Estimated Adelie users (small sample) = 39.8% - 48.6%\"   \n[2] \"Estimated Chinstrap users (small sample) = 16.2% - 23.3%\"\n[3] \"Estimated Gentoo users (small sample) = 31.8% - 40.3%\""
  },
  {
    "objectID": "sample.html#how-big-a-sample-do-i-need-to-have-the-confidence-i-want",
    "href": "sample.html#how-big-a-sample-do-i-need-to-have-the-confidence-i-want",
    "title": "7  Sample Data in R",
    "section": "7.9 How Big a Sample Do I Need to Have the Confidence I Want?",
    "text": "7.9 How Big a Sample Do I Need to Have the Confidence I Want?\nUsing simple algebra, we can transform the formulas for margin of error to find the sample size (n) that we need get the margin of error (E) that we are able to tolerate:\nFor population mean estimates: $n = (Z S/ E $\nFor population proportion estimates:\n\\(n = Z2 \\times p \\times (1-p)/ E2\\)\nEstimating s and p For the mean formula, you need a sample mean (s), and for the proportion formula you need the sample proportion (p). But since you have not yet done the sampling yet, you cannot know these values.\nThere are three imperfect but practical ways to estimate these values:\n\nTwo-stage sampling design involves doing a preliminary survey to get an estimate of the sample mean (s) or the proportion (p)\nFor means or proportions, when available, you can use results from a prior survey or estimate\nFor proportions you can use p = 50% as a worst case scenario. This causes the p * (1 - p) part of the formula to be its maximimum possible value so the sample size is the largest possible value\n\nNote that the examples below presume a random sample from the entire population you are basing the estimate upon. If you are trying to make estimates about Americans, your random sample would need to be drawn from a list of all Americans with 100% participation. Drawing a perfectly random sample from anything other than a trivial or captive population is usually impossible, requiring more sophisticated modeling techniques to adjust the results and compensate for segments of the population that were undersampled.\nExample Sample Size Following the examples using male height above, suppose you wish to get an estimate for GPA ±0.1 points (E = 0.1) with 95% confidence (Z = 1.96). For s you use the mean from the sample of 3.009143.\n\nZ = 1.96\ns = 3.009143\nE = 0.1\nn = (Z * s / E)^2\n\npaste(\"Minimum sample size =\", ceiling(n))\n\n[1] \"Minimum sample size = 3479\"\n\n\nFollowing on the proportions example above, suppose you wish to get a better estimate of the percent of students that use Macs as their personal home computers or laptops ±1% (E = 0.01) at a 95% level of confidence (Z = 1.96). Using the estimated proportion of 47% (p = 0.47) from the survey given above:\n\nZ = 1.96\np = 0.47\nE = 0.01\nn = (Z^2) * p * (1 - p) / (E^2)\n\npaste(\"Minimum sample size =\", ceiling(n))\n\n[1] \"Minimum sample size = 9570\""
  },
  {
    "objectID": "sample.html#calculations-in-excel",
    "href": "sample.html#calculations-in-excel",
    "title": "7  Sample Data in R",
    "section": "7.10 Calculations in Excel",
    "text": "7.10 Calculations in Excel\n\n7.10.1 Confidence Interval for Means\nUsing a simulated sample of heights from 100 American men in this CSV file, this formula can be used to calculate standard error:\n\n=STDEV(A2:A101) / SQRT(COUNTA(A2:A101))\n\nTo calculate margin of error for means given:\n\nThat standard error in cell B2\nThe Z-score for a 95% level of confidence (z = 1.96) in cell B3\n\n\n=B3 * B2 / SQRT(B4)\n\n\n\n7.10.2 Confidence Interval for Proportions\nUsing survey responses from this CSV file, we can calculate the confidence interval for proportions in Excel.\nWhen dealing with categorical data in Excel, you can use the COUNTIF() function to find the number of cells in your sample that match a particular value. The first parameter is the data and the second parameter is the condition. Note that the second parameter must be enclosed in quotes.\nFor example, with the CSV file linke above, the data in cells A2:A33 is either “Mac” or “PC”. This will return the number of Mac users:\n\n=COUNTIF(A2:A33, \"Mac\")\n\nYou can then get the proportion (percentage) of yes cells with:\n\n=COUNTIF(A2:A22, \"Mac\") / COUNTA(A2:A22)\n\nIf you put your proportion in cell B2 and your sample size in cell B3, the standard error for proportions is:\n\n=SQRT(B2 * (1 - B2) / B3)\n\nIf you then put the Z-score for your level of confidence in cell B3 (for 95% confidence, this is 1.96), the Excel formula for margin of error for proportions is:\n\n=B4 * SQRT(B2 * (1 - B2) / B3)\n\nZ-Scores For Other Levels of Confidence In Excel, z-scores for other levels of confidence can be calculated with the NORMSINV() function. With the confidence interval in cell B2 (as a percent on a scale of 0 to 1):\n\n=NORMSINV(1 - ((1 - B2) / 2))\n\nConfidence Interval for Small Samples In Excel, the TINV() function can be used to calculate t for use with standard error to calculate a confidence interval.\nTo calculate the margin of error for means given:\nA level of confidence in cell B2 The standard deviation in cell B3 The sample size in cell B4\n\n=TINV(1 - B2, B4 - 1) * B3 / SQRT(B4)\n\nTo calculate the margin of error for proportions:\n\n=TINV(1 - B3, B4 - 1) * SQRT(B2 * (1 - B2) / B4)```\n\nExcel Confidence Interval Functions\nAs you might expect, Excel has functions to simplify confidence intervals for means. CONFIDENCE.NORM() can be used to calculate margin of error with sample sizes over 30 and CONFIDENCE.T can be used with sample sizes of 30 or less. Both take the same parameters:\n\n\n=CONFIDENCE.NORM(alpha, stdev, sample_size)\n\n\n=CONFIDENCE.T(alpha, stdev, sample_size)\n\nalpha = 1 minus level of confidence (0.05 for 95% confidence level) stdev = standard deviation of the sample sample_size = count of values in the sample For example, given a level of confidence in cell B2, the standard deviation in cell B3, and the sample size in cell B4, the margin of error (for a sample size less than 30) using the t-distribution function:\n\n=CONFIDENCE.T(1 - B2, B3, B4)\n\nExcel does not have a convenience function for confidence interval for proportions.\nExample Sample Size Estimation Following the examples using male height above, suppose you wish to get an estimate for female height ±1 inch (E = 1) with 95% confidence (Z = 1.96). For s you use a value you have seen on the internet of 63.7 inches. The power() function is used for the exponent:\n\n=POWER(1.96 * 63.7 / 1, 2)\n\nFollowing on the proportions example above, suppose you wish to get a better estimate of the percent of Americans that use Macs as their personal home computers or laptops &plusmin;5% at a 95% level of confidence (Z = 1.6). Using the small sample estimated mean of 40% given above:\n\n=POWER(1.96, 2) * 0.4 * (1 - 0.4) / POWER(0.05, 2)"
  },
  {
    "objectID": "basicFishStats.html",
    "href": "basicFishStats.html",
    "title": "8  Basic Fisheries Statistics",
    "section": "",
    "text": "One of the first analyses you may be interested in is calculating annual landings in the fishery. To calculate annual landings, take your landings_data data frame, add a column for weight of individual fish in kilograms by using the mutate function, group the data by year by using the group_by function, and then summarize the data for each year by summing the total weight of all fish caught in each year using the summarize and sum functions.\n# Start with the landings data frame\nannual_landings <- landings_data %>% \n  # Add colomn for kilograms by dividing gram column by 1000\n  mutate(Weight_kg = Weight_g / 1000) %>%\n  # Group the data by year\n  group_by(Year) %>% \n  # Next, summarize the total annual landings per year\n  summarize(Annual_Landings_kg = sum(Weight_kg,na.rm=TRUE))\n\n## Display a table of the annual landings data\nannual_landings\n\n## # A tibble: 9 x 2\n##    Year Annual_Landings_kg\n##   <int>              <dbl>\n## 1  2003          310.40914\n## 2  2004          565.30807\n## 3  2005          163.24191\n## 4  2006           37.11914\n## 5  2010          131.84178\n## 6  2011          156.77825\n## 7  2012          101.53198\n## 8  2013          579.52008\n## 9  2014         1193.75519\nNote the use of na.rm = TRUE in the code above. This is an important argument of many R functions (sum() in this case) and it tells R what to do with NA values in your data. Here, we are telling R to first remove NA values before calculating the sum of the Weight_kg variable. By default, many functions will return NA if any value is NA, which is often not desirable.\nYou may be interested in looking at landings across different gear types. Here, we now group the data frame by both the year and the gear type in order to summarize the total landings by year and by gear.\n# Start with the landings data frame\nannual_gear_landings <- landings_data %>% \n  # Add colomn for kilograms by dividing gram column by 1000\n  mutate(Weight_kg = Weight_g / 1000) %>%\n  # Group the data by year and gear type\n  group_by(Year,Gear) %>% \n  # Next, summarize the total annual landings per year and gear type\n  summarize(Annual_Landings_kg = sum(Weight_kg,na.rm=TRUE))\n\n## Display a table of the annual landings data by gear type\nannual_gear_landings\n\n## # A tibble: 39 x 3\n## # Groups:   Year [?]\n##     Year     Gear Annual_Landings_kg\n##    <int>    <chr>              <dbl>\n##  1  2003  gillnet          13.413401\n##  2  2003 handline           2.874861\n##  3  2003  muroami         247.879049\n##  4  2003     trap          46.241825\n##  5  2004  gillnet           4.189301\n##  6  2004 handline          57.705893\n##  7  2004  muroami         370.866460\n##  8  2004 speargun           9.476406\n##  9  2004     trap         118.683464\n## 10  2004 trolling           4.386547\n## # ... with 29 more rows"
  },
  {
    "objectID": "basicFishStats.html#calculating-catch-per-unit-effort-cpue",
    "href": "basicFishStats.html#calculating-catch-per-unit-effort-cpue",
    "title": "8  Basic Fisheries Statistics",
    "section": "8.2 Calculating Catch-per-Unit-Effort (CPUE)",
    "text": "8.2 Calculating Catch-per-Unit-Effort (CPUE)\nYou may also be interested in calculating catch-per-unit-effort (CPUE). CPUE is calculated by dividing the catch of each fishing trip by the number of hours fished during that trip. This gives CPUE in units of kilograms per hour. The median for every year is then calculated in order to remove outliers - some fishers are much more efficient than others.\n# Start with the landings data frame\ncpue_data <- landings_data %>% \n  # Add colomn for kilograms by dividing gram column by 1000\n  mutate(Weight_kg = Weight_g / 1000) %>%\n  # Group by year and Trip ID so that you can calculate CPUE for every trip in every year\n  group_by(Year,Trip_ID) %>% \n  # For each year and trip ID, calculate the CPUE for each trip by dividing the sum of the catch, converted from grams to kilograms, by the trip by the number of fishing hours\n  summarize(Trip_CPUE = sum(Weight_kg) / mean(Effort_Hours)) %>% \n  # Next, just group by year so we can calculate median CPUE for each year across all trips in the year\n  group_by(Year) %>% \n  # Calculate median CPUE for each year\n  summarize(Median_CPUE_kg_hour = median(Trip_CPUE))\n\n# Display a table of the CPUE data\ncpue_data\n\n## # A tibble: 9 x 2\n##    Year Median_CPUE_kg_hour\n##   <int>               <dbl>\n## 1  2003          0.31834277\n## 2  2004          0.26233292\n## 3  2005          0.40145105\n## 4  2006          0.44029501\n## 5  2010          0.01742840\n## 6  2011          0.03123217\n## 7  2012          0.03123217\n## 8  2013          0.19638408\n## 9  2014          0.88216281"
  },
  {
    "objectID": "basicFishStats.html#calculating-percent-mature",
    "href": "basicFishStats.html#calculating-percent-mature",
    "title": "8  Basic Fisheries Statistics",
    "section": "8.3 Calculating Percent Mature",
    "text": "8.3 Calculating Percent Mature\nYou may also wish to analyze your length data. One analysis would be to determine the percentage of mature fish in the catch in every year of the data frame. First let's define m95, the length at which 95% of fish are mature. For Caesio cuning, we know this is 15.9cm. Next, let's add a column to the data frame using the mutate function that represents whether each fish is mature or not (represented by a TRUE or FALSE), group the data frame by year, and then summarize for each year the percentage of mature fish out of the total number of sampled fish.\n# Define m95, the length at which 95% of fish are mature\nm95 = 15.9\n\n# Start with the landings data frame\nlandings_data %>% \n  # Add a column to the data that indicates whether each length measurement is from a mature or immature fish. If it's mature, this value should be TRUE; if immature, FALSE.\n  mutate(Mature = Length_cm > m95) %>% \n  # Group by year so we can see the percent mature for every year\n  group_by(Year) %>% \n  # The percentage mature is equal to the  number of mature fish divided by the total number of fish and multiplied by 100\n  summarize(Percent_Mature = sum(Mature) / n() * 100) \n\n## # A tibble: 9 x 2\n##    Year Percent_Mature\n##   <int>          <dbl>\n## 1  2003       98.56916\n## 2  2004       98.62188\n## 3  2005       97.73371\n## 4  2006      100.00000\n## 5  2010       91.80556\n## 6  2011       99.77629\n## 7  2012       99.65398\n## 8  2013       99.46164\n## 9  2014       99.55665\nOver 90% of the fish are mature throughout the time series, which is a great sign!"
  },
  {
    "objectID": "basicFishStats.html#helpful-resources",
    "href": "basicFishStats.html#helpful-resources",
    "title": "8  Basic Fisheries Statistics",
    "section": "8.4 Helpful Resources",
    "text": "8.4 Helpful Resources\n\nData Wrangling with dplyr and tidyr Cheat Sheet is a very helpful resource for learning how to wrangle (manipulate) data in R"
  },
  {
    "objectID": "basicFishPlotting.html",
    "href": "basicFishPlotting.html",
    "title": "9  Plotting Fisheries Data",
    "section": "",
    "text": "This document will now walk you through how to make make some basic fisheries plots using the data frames you created in the previous analysis section and the ggplot plotting function. When using ggplot, first start with your data frame and initialize the ggplot by specifying the plot’s aesthetics (variables) using aes(). Then use the + operation to add at least one geometry (type of plot, such as a scatter plot) and any additional features to the plot. To learn more about ggplot, the Data Visualization with ggplot2 Cheat Sheet is a very helpful resource, as is this ggplot cookbook."
  },
  {
    "objectID": "basicFishPlotting.html#plotting-landings",
    "href": "basicFishPlotting.html#plotting-landings",
    "title": "9  Plotting Fisheries Data",
    "section": "9.1 Plotting Landings",
    "text": "9.1 Plotting Landings\nLet’s plot a time series of annual landings data. We start with the annual landings data we made in the previous step, and then feed this into a ggplot.\n# Start with the annual_landings data frame you created in the last step\nannual_landings %>%\n  # Initialize a ggplot of annual landings versus year\n  ggplot(aes(x=Year,y=Annual_Landings_kg)) +\n  # Tell ggplot that the plot type should be a scatter plot\n  geom_point() +\n  # Also add a line connecting the points\n  geom_line() + \n  # Change the y-axis title\n  ylab(\"Annual Landings [kg/year]\") + \n  # Add figure title\n  ggtitle(\"Annual landings of Caesio cuning\") \n\nIn this example, we are using aes(x=Year, y=Annual_Landings_kg) to specify that the we want to plot years on the x-axis and annual landings on the y-axis. We then want to visualize these variables with both a scatter plot (geom_point()) and a line plot (geom_line()) geometry.\nIt appears landings were going down between 2004 and 2011, but have been increasing since then. Again, you may be interested in looking across different gear types. To plot, we use ggplot’s faceting functionality, which tells ggplot to divide up the data by a certain variable, Gear in this case, and make multiple similar plots. You can use the facet_wrap() function to accomplish this.\n# Start with the landings data frame\nannual_gear_landings %>% \n  # First, group the data by year\n  group_by(Year,Gear) %>% \n  # Initialize a ggplot of annual landings versus year\n  ggplot(aes(x=Year,y=Annual_Landings_kg)) +\n  # Tell ggplot that the plot type should be a scatter plot\n  geom_point() +\n  # Also add a line connecting the points\n  geom_line() + \n  # Change the y-axis title\n  ylab(\"Normalized annual Landings [kg/year]\") + \n  # Add figure title\n  ggtitle(\"Normalized annual landings of Caesio cuning\") +\n  # This tells the figure to plot by all different gear types\n  facet_wrap(~Gear) \n\nIt now becomes clear that the recent increase in catch seems to be concentrated in speargun and trap fishing. Meanwhile, catch from muroami, a very destructive type of gear where nets are driven into the reef, has dropped to 0 since a ban of that gear in 2012 - a good sign that management regulation is working."
  },
  {
    "objectID": "basicFishPlotting.html#plotting-cpue",
    "href": "basicFishPlotting.html#plotting-cpue",
    "title": "9  Plotting Fisheries Data",
    "section": "9.2 Plotting CPUE",
    "text": "9.2 Plotting CPUE\nYou may also be interested in plotting median catch-per-unit-effort (CPUE). You take your CPUE data frame made in the last step and feed it into ggplot.\n# Start with the CPUE data frame\ncpue_data %>% \n  # Initialize a ggplot of median CPUE versus year\n  ggplot(aes(x=Year,y=Median_CPUE_kg_hour)) +\n  # Tell ggplot that the plot type should be a scatter plot\n  geom_point() +\n  # Also add a line connecting the points\n  geom_line() + \n  # Change the y-axis title\n  ylab(\"Median CPUE [kg/hour]\") + \n  # Add figure title\n  ggtitle(\"Median CPUE for Caesio cuning\") \n\nCPUE appears to have increased significantly during the last years. This may be due to increasing abundance in the water, which would be a good thing, but may also be indicative of increased gear efficiency coinciding with the transition to traps and spearguns, which may be concerning."
  },
  {
    "objectID": "basicFishPlotting.html#plotting-length-frequency",
    "href": "basicFishPlotting.html#plotting-length-frequency",
    "title": "9  Plotting Fisheries Data",
    "section": "9.3 Plotting Length Frequency",
    "text": "9.3 Plotting Length Frequency\nFinally, let’s first look at the length data from the catch, which gives an indication of the size structure and health of the population. Let’s look at the length data for 2014, which is the most recent year of data available. We first filter the data to be only from 2014 using the filter() function. We then create a histogram of the length data, which shows how many individuals of each size class were measured in the catch. On the histogram, we’ll also add a vertical line to show the length at which fish mature to get a sense of how sustainable the catch is - the catch should be composed mostly of mature fish. This information comes from the life history parameter data input file.\n# Start with the landings data frame\nlandings_data %>% \n  # Filter data to only look at length measurements from 2014\n  filter(Year == 2014) %>% \n  # Initialize ggplot of data using the length column\n  ggplot(aes(Length_cm)) + \n  # Tell ggplot that the plot type should be a histogram\n  geom_histogram() + \n  # Change x-axis label\n  xlab(\"Length [cm]\") + \n  # Add figure title\n  ggtitle(\"Length histogram of Caesio cuning in the catch\\nLength at 95% maturity shown as a red line.\") + \n  # Add a red vertical line for m95, the length at which 95% of fish are mature. Any fish below this length may be immature. Use the m95 value defined in the previous section\n  geom_vline(aes(xintercept=m95),color=\"red\") \n\nYou might also be interested in seeing how the size composition varies by gear type. You can recreate the figure above, but separate the histograms out by gear type using ggplot’s “facet” function.\n# Start with the landings data frame\nlandings_data %>% \n  # Filter data to only look at length measurements from 2014\n  filter(Year == 2014) %>% \n  # Initialize ggplot of data using the length column\n  ggplot(aes(Length_cm)) + \n  # Tell ggplot that the plot type should be a histogram\n  geom_histogram() + \n  # Change x-axis label\n  xlab(\"Length [cm]\") + \n  # Add figure title\n  ggtitle(\"Length histogram of Caesio cuning in the catch by gear type\\nLength at 95% maturity shown as a red line.\") + \n  # Add a red line for m95, the length at which 95% of fish are mature. Any fish below this length may be immature. Use the life_history_parameter data frame to get this value.\n  geom_vline(aes(xintercept=m95),color=\"red\") + \n  # This tells the figure to plot by all different gear types, known as facetting\n  facet_wrap(~Gear) \n\nIt appears as if the size structure is about the same from each gear, although by far the most amount of fish are caught using speargun. Very few fish are caught using trolling.\nThese plots indicate a generally increasing catch, CPUE, and a healthy size structure. Our results demonstrate that the population is likely doing fairly well, and may be recovering since the 2012 ban of muroami fishing gear."
  },
  {
    "objectID": "basicFishPlotting.html#helpful-resources",
    "href": "basicFishPlotting.html#helpful-resources",
    "title": "9  Plotting Fisheries Data",
    "section": "9.4 Helpful Resources",
    "text": "9.4 Helpful Resources\n\nData Visualization with ggplot2 Cheat Sheet\nggplot cookbook"
  },
  {
    "objectID": "simulate.html",
    "href": "simulate.html",
    "title": "10  Simulate Data",
    "section": "",
    "text": "There are many reasons we might want to simulate data in R, and I find being able to simulate data to be incredibly useful in my day-to-day work. But how does someone get started simulating data?\n\nrequire(tidyverse)\nrequire(patchwork)\n\ntheme_set(theme_bw(base_size = 11))\n\nR has several tools for generating and simulating data that can be useful for testing and teaching"
  },
  {
    "objectID": "simulate.html#linear-sequences",
    "href": "simulate.html#linear-sequences",
    "title": "10  Simulate Data",
    "section": "10.2 Linear Sequences",
    "text": "10.2 Linear Sequences\nLinear sequences of numbers are commonly used in R, notably for iterating through elements of data structures. For sequences of successive integers, the colon operator is usually the simplest choice. The result is all integers between the number before the colon and the number after the colon, inclusive. The sequence can be ascending or descending:\n\nx = 1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nx = -1:10\nx\n\n [1] -1  0  1  2  3  4  5  6  7  8  9 10\n\n\nFor sequences of real numbers or sequences that proceed in increments not equal to one use the seq function and specify the value of the three arguments as seq(from, to, by):\n\nx = seq(10,100,10)\nx\n\n [1]  10  20  30  40  50  60  70  80  90 100\n\n\n\nx = seq(-90,90,30)\nx\n\n[1] -90 -60 -30   0  30  60  90\n\n\nFor regular sequences of specific numbers the rep() (repeat) function can be used. The parameters are:\nx: the value to repeat times: the number of times to repeat that value\n\nx = rep(1,10)\nx\n\n [1] 1 1 1 1 1 1 1 1 1 1\n\n\n\nrep(c(\"SE\", \"NE\"), each = 5)\n\n [1] \"SE\" \"SE\" \"SE\" \"SE\" \"SE\" \"NE\" \"NE\" \"NE\" \"NE\" \"NE\"\n\n\n\nrep(c(\"SE\", \"NE\"), times = 5)\n\n [1] \"SE\" \"NE\" \"SE\" \"NE\" \"SE\" \"NE\" \"SE\" \"NE\" \"SE\" \"NE\""
  },
  {
    "objectID": "simulate.html#nonlinear-sequences",
    "href": "simulate.html#nonlinear-sequences",
    "title": "10  Simulate Data",
    "section": "10.3 Nonlinear Sequences",
    "text": "10.3 Nonlinear Sequences\nLinear sequences can be transformed to nonlinear sequences of numbers using a variety of functions. Some examples are given below.\n\n10.3.1 Exponential and Logarithmic Sequences\nMany phenomena, such as growth, follow exponential or logarithmic curves. Natural exponential and logarithms (powers and roots of Euler’s Number) are supported with the exp() and log() functions.\n\na = 1:10\nb = exp(a)\nb\n\n [1]     2.718282     7.389056    20.085537    54.598150   148.413159\n [6]   403.428793  1096.633158  2980.957987  8103.083928 22026.465795\n\n\n\n\n\n\n\n\nTable 10.1:  Table of linear of sequence of number 1:10 and its natuarla exponential sequence \n  \n  \n    \n      id\n      Linear\n      Exponential\n    \n  \n  \n    1\n1\n2.718282\n    2\n2\n7.389056\n    3\n3\n20.085537\n    4\n4\n54.598150\n    5\n5\n148.413159\n    6\n6\n403.428793\n    7\n7\n1096.633158\n    8\n8\n2980.957987\n    9\n9\n8103.083928\n    10\n10\n22026.465795\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nFigure 10.1: Plot of linear of sequence of number 1:10 and its natuaral exponential sequence\n\n\n\n\n\n\n10.3.2 Logarithmic\n\na = 1:10\nb = log(a)\nb\n\n [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n [8] 2.0794415 2.1972246 2.3025851\n\n\n\n\n\n\n\n\nTable 10.2:  Table of linear of sequence of number 1:10 and its natuaral logarithmic sequence \n  \n  \n    \n      id\n      Linear\n      Logarithmic\n    \n  \n  \n    1\n1\n0.0000000\n    2\n2\n0.6931472\n    3\n3\n1.0986123\n    4\n4\n1.3862944\n    5\n5\n1.6094379\n    6\n6\n1.7917595\n    7\n7\n1.9459101\n    8\n8\n2.0794415\n    9\n9\n2.1972246\n    10\n10\n2.3025851\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nFigure 10.2: Table of linear of sequence of number 1:10 and its natuaral logarithmic sequence\n\n\n\n\n\n\n10.3.3 Exponential\nExponential sequences can be created based on any base using the carat operator ^:\n\na = 3\nb = a^(0:10)\nb\n\n [1]     1     3     9    27    81   243   729  2187  6561 19683 59049\n\n\n\nbt = tibble(b) %>% rename(data = 1) %>% \n  mutate(id = rep(0:10, times = 1)) \n\nbt %>% \n  ggplot(aes(x = id, y = data))+\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = 1:10)+\n  scale_y_continuous(name = \"values\",breaks = scales::pretty_breaks(n = 10))+\n  theme(axis.title.x = element_blank())\n\n\n\n\nFigure 10.3: Exponential sequence\n\n\n\n\n\n\n10.3.4 Cyclical Sequences\nMany phenomena such as temperature or market activity occur in regularly repeating cycles. The sin() and cos() functions can be used to create cyclical sequences. The parameter is in radians (multiples of 2π), and the native pi constant can be used to convert a linear sequence to radians:\n\na = seq(0, 6, length.out = 25)\nb = sin(a * pi)\nb\n\n [1]  0.000000e+00  7.071068e-01  1.000000e+00  7.071068e-01  1.224606e-16\n [6] -7.071068e-01 -1.000000e+00 -7.071068e-01 -2.449213e-16  7.071068e-01\n[11]  1.000000e+00  7.071068e-01  3.673819e-16 -7.071068e-01 -1.000000e+00\n[16] -7.071068e-01 -4.898425e-16  7.071068e-01  1.000000e+00  7.071068e-01\n[21]  6.123032e-16 -7.071068e-01 -1.000000e+00 -7.071068e-01 -7.347638e-16\n\n\n\n\n\n\n\n\nTable 10.3:  Table of linear of sequence of number 1:10 and its sin sequence \n  \n  \n    \n      id\n      Linear\n      Sinusoid\n    \n  \n  \n    1\n0.00\n0.000000e+00\n    2\n0.25\n7.071068e-01\n    3\n0.50\n1.000000e+00\n    4\n0.75\n7.071068e-01\n    5\n1.00\n1.224606e-16\n    6\n1.25\n-7.071068e-01\n    7\n1.50\n-1.000000e+00\n    8\n1.75\n-7.071068e-01\n    9\n2.00\n-2.449213e-16\n    10\n2.25\n7.071068e-01\n    11\n2.50\n1.000000e+00\n    12\n2.75\n7.071068e-01\n    13\n3.00\n3.673819e-16\n    14\n3.25\n-7.071068e-01\n    15\n3.50\n-1.000000e+00\n    16\n3.75\n-7.071068e-01\n    17\n4.00\n-4.898425e-16\n    18\n4.25\n7.071068e-01\n    19\n4.50\n1.000000e+00\n    20\n4.75\n7.071068e-01\n    21\n5.00\n6.123032e-16\n    22\n5.25\n-7.071068e-01\n    23\n5.50\n-1.000000e+00\n    24\n5.75\n-7.071068e-01\n    25\n6.00\n-7.347638e-16\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nFigure 10.4: Plot of linear of sequence of number 1:10 and its sin sequence"
  },
  {
    "objectID": "simulate.html#normally-distributed-sequences",
    "href": "simulate.html#normally-distributed-sequences",
    "title": "10  Simulate Data",
    "section": "10.4 Normally-Distributed Sequences",
    "text": "10.4 Normally-Distributed Sequences\nA normally distributed sequence of numbers can be created using the qnorm() quantile function. The parameters:\n\nx: A linear sequence of probabilities in the range 0 to 1\nmean: The mean (μ)\nsd: The standard deviation (σ)\n\n\n10.4.1 qnorm\nThis function returns the value of the inverse cumulative density function (cdf) of the normal distribution given a certain random variable p, a population mean μ, and the population standard deviation σ\n\na = seq(0, 1, 0.02)\nb = qnorm(p = a, mean = 2, sd = 1)\nb\n\n [1]        -Inf -0.05374891  0.24931393  0.44522641  0.59492844  0.71844843\n [7]  0.82501321  0.91968066  1.00554212  1.08463491  1.15837877  1.22780679\n[13]  1.29369744  1.35665459  1.41715849  1.47559949  1.53230120  1.58753687\n[19]  1.64154121  1.69451921  1.74665290  1.79810652  1.84903078  1.89956628\n[25]  1.94984642  2.00000000  2.05015358  2.10043372  2.15096922  2.20189348\n[31]  2.25334710  2.30548079  2.35845879  2.41246313  2.46769880  2.52440051\n[37]  2.58284151  2.64334541  2.70630256  2.77219321  2.84162123  2.91536509\n[43]  2.99445788  3.08031934  3.17498679  3.28155157  3.40507156  3.55477359\n[49]  3.75068607  4.05374891         Inf\n\n\n\nat = tibble(a%>% as.numeric())  %>% rename(data = 1)\nbt = tibble(b) %>% rename(data = 1)\n\nbinded = bind_rows(at,bt) %>% \n  mutate(id = rep(1:length(a), times = 2), mode = rep(c(\"seq\", \"qnorm\"), each = length(a)))\n\n\n\nWarning: Removed 2 rows containing non-finite values (stat_bin).\n\n\n\n\n\nFigure 10.5: Plot of linear of sequence and its normalized sequence with qnorm\n\n\n\n\n\n\n10.4.2 rnorm\nWhen we want to generate a vector of normally distributed random variables, we use rnorm function and supply it with length n, a population mean μ and population standard deviation σ.\n\nExample: In this example, with the use of the rnorm() function we are generating a vector of 10 normally distributed random variables with mean temperature=25 and sd=2.\n\n\na = rnorm(n = 10, mean = 25, sd = 2)\na\n\n [1] 26.83946 24.63983 24.46281 25.08249 25.42845 29.02642 29.06418 27.07543\n [9] 26.32608 24.23981\n\n\n\n\n10.4.3 runif\nThe runif() function generates random deviates of the uniform distribution when the minimum and maximum values are provided. The function is simply written as runif(n, min = 0, max = 1) . We may easily generate n number of random samples within any interval, defined by the min and the max argument.\n\nExample: In this example, with the use of the runif()` function to generate a vector of 10 normally distributed random variables with minimum temperature of 23 and maximum temperature=28.\n\n\na = runif(10, min = 23, max = 28)\na\n\n [1] 24.95333 27.58876 26.47779 24.82032 27.86841 23.22590 25.55820 25.57930\n [9] 25.62108 26.91224"
  },
  {
    "objectID": "simulate.html#sequences-based-on-percentiles",
    "href": "simulate.html#sequences-based-on-percentiles",
    "title": "10  Simulate Data",
    "section": "10.5 Sequences Based on Percentiles",
    "text": "10.5 Sequences Based on Percentiles\nYou will occasionally find statistical data given as percentiles. For example, in this document the CDC gives the following percentiles for the height of men in the USA:\n\nPercentile: 5     10    15    25    50    75    85    90    95\nHeight (in):    64.3  65.4  66.1  67.2  69.1  71.2  72.3  73.0  74.1\n\nThis means that 5% of men are below 64.3 inches, 25% are below 67.2 inches, etc. By using a spline, it is possible to simulate a curve that fits those percentile points, then use the predict() function to create a representative sample. The CDC indicates that they based their estimates on 5,232 examined persons (sample size), which is what is simulated below:\n\n# Table 12.  Height in inches for males aged 20 and over and number of examined\n# persons, mean, standard error of the mean, and selected percentiles, by race\n# and Hispanic origin and age: United States, 2011–2014\n\npheight = c(64.3, 65.4, 66.1, 67.2, 69.1, 71.2, 72.3, 73.0, 74.1)\npercentile = c(5, 10, 15, 25, 50, 75, 85, 90, 95)\n\n# Create a spline to fit those points\nspline = smooth.spline(percentile, pheight)\n\n# Simulate 5,232 points that cover the range of percentiles\nheight = predict(spline, 100 * (1:5232) / 5233)\n\n# Plot the simulated data\nplot(percentile, pheight, col=\"blue\", pch=16)\nlines(height$x, height$y)\n\n\n\n# Print the descriptive statistics\npaste(\"mean = \", mean(height$y))\n\n[1] \"mean =  69.1732940854836\"\n\npaste(\"stdev =\", sd(height$y))\n\n[1] \"stdev = 2.82859108609513\"\n\npaste(\"stderr =\", sd(height$y) / sqrt(length(height$y)))\n\n[1] \"stderr = 0.0391053611211175\""
  },
  {
    "objectID": "simulate.html#logistic-sequences",
    "href": "simulate.html#logistic-sequences",
    "title": "10  Simulate Data",
    "section": "10.6 Logistic Sequences",
    "text": "10.6 Logistic Sequences\nPhenomena like growth to the carrying capacity of an ecosystem follow a logistic curve, which can be generated using the self-starting logistic model function SSlogis(). The parameters are:\n\ninput: A linear sequence\nAsym: The maximum output value (carrying capacity)\nxmid: The x value that is at the middle of the curve\nscal: The scaling (width) of the curve\n\n\na = 1:20\nb = SSlogis(input = a, Asym = 100, xmid = 10, scal = 1)\nb\n\n [1]  0.01233946  0.03353501  0.09110512  0.24726232  0.66928509  1.79862100\n [7]  4.74258732 11.92029220 26.89414214 50.00000000 73.10585786 88.07970780\n[13] 95.25741268 98.20137900 99.33071491 99.75273768 99.90889488 99.96646499\n[19] 99.98766054 99.99546021\n\n\n\nplot(b, type=\"b\")\ntext(1, 10, \"Initial Colonization\", pos=4)\ntext(11, 50, \"High Growth\", pos=4)\ntext(13, 90, \"Limit of Carrying Capacity\", pos=4)\n\n\n\n\n\n\n\n\n\n\nTable 10.4:  Table of linear of sequence of number 1:10 and its logistic sequence \n  \n  \n    \n      id\n      Linear\n      Logit\n    \n  \n  \n    1\n1\n0.01233946\n    2\n2\n0.03353501\n    3\n3\n0.09110512\n    4\n4\n0.24726232\n    5\n5\n0.66928509\n    6\n6\n1.79862100\n    7\n7\n4.74258732\n    8\n8\n11.92029220\n    9\n9\n26.89414214\n    10\n10\n50.00000000\n    11\n11\n73.10585786\n    12\n12\n88.07970780\n    13\n13\n95.25741268\n    14\n14\n98.20137900\n    15\n15\n99.33071491\n    16\n16\n99.75273768\n    17\n17\n99.90889488\n    18\n18\n99.96646499\n    19\n19\n99.98766054\n    20\n20\n99.99546021\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nFigure 10.6: Plot of linear of sequence of number 1:10 and its logistic sequence\n\n\n\n\n\nset.seed(123)\n\na = rnorm(n = 60, mean = 44, sd = 20) %>% sort()\na = runif(n = 60, min = 20, max = 80) %>% sort()\n\nSSlogis(input = a, Asym = 120, xmid = 50, scal = 2) %>% plot()"
  },
  {
    "objectID": "simulate.html#random-sequences",
    "href": "simulate.html#random-sequences",
    "title": "10  Simulate Data",
    "section": "10.7 Random Sequences",
    "text": "10.7 Random Sequences\nAn easy way to generate numeric data is to pull random numbers from some distribution. This can be done via the functions for generating random deviates. These functions always start with r (for “random”).\nThe basic distributions that I use the most for generating random numbers are the normal (rnorm()) and uniform (runif()) distributions.\nThere are many other distributions available as part of the stats package (e.g., binomial, F, log normal, beta, exponential, Gamma) and, as you can imagine, even more available in add-on packages. I recently needed to generate data from the Tweedie distribution to test a modeling tool, which I could do via package tweedie.\nThe r functions for a chosen distribution all work basically the same way. We define how many random numbers we want to generate in the first argument (n) and then define the parameters for the distribution we want to draw from. This is easier to see with practice, so let’s get started.\nRandom sequences of numbers are useful for testing analysis code, and are essential for formal statistical techniques like Monte Carlo methods.\nRandom sequences of numbers do not follow a regular pattern. While computers are deterministic machines that cannot generate truly random sequences of numbers, there are a variety of algorithms that generate pseudo-random sequences that are close enough to random to meet the needs of researchers.\n\n10.7.1 Uniformly-Distributed Random Sequences\nThe most basic random number generating function is the runif() function, which generates a random sequence in a uniform distribution (the unif part of the name). The parameters:\n\nn: The count of numbers to generate\nmin: The minimum possible value in the sequence (defaults to 0)\nmax: The maximum possible value in the sequence (defaults to 1)\nExample: Generate sequence of nile perch that has minimum value of 20 and maximum value of 120\n\n\na = runif(n = 10, min = 20, max = 80) %>% sort()\na\n\n [1] 35.90107 35.90196 38.74689 48.87739 53.87543 55.66059 62.49742 70.38607\n [9] 74.11246 74.79129\n\n\nNote that the runif function help us when want to simulate sample when a minimum and maximum value of that population is given.\n\n\n10.7.2 Normally-Distributed Random Sequences\nI use rnorm() a lot, sometimes with good reason and other times when I need some numbers and I really don’t care too much about what they are.\nThere are three arguments to rnorm(). From the Usage section of the documentation:\nrnorm(n, mean = 0, sd = 1)\nThe n argument is the number of observations we want to generate. The mean and sd arguments show what the default values of the parameters are (note that sd is the standard deviation, not the variance). Not all r functions have defaults to the parameter arguments like this.\nTo get 10 random numbers from a normal distribution we can write code like:\n\na = rnorm(n = 10, mean = 55,sd = 8)\na\n\n [1] 50.19792 72.49866 67.26089 53.11440 46.78863 49.31675 57.05507 53.02646\n [9] 52.21966 47.38705\n\n\nYou notes that the `rnorm`` function help us to simulate sample when a statistic parameter mean and standard deviation of that population is given.\n\n\n10.7.3 Setting the random seed for reproducible random numbers\nSecond, if we run this code again we’ll get different numbers.\n\na = rnorm(n = 10, mean = 55,sd = 8)\na\n\n [1] 54.63978 48.72076 41.65646 51.95819 62.35197 50.39722 59.86371 42.05694\n [9] 54.55550 59.15526\n\n\nTo get reproducible random numbers we need to set the seed via set.seed().\nMaking sure someone else will be able to exactly reproduce your results when running the same code can be desirable in teaching. It is also is useful when making an example dataset to demonstrate a coding issue, like if you were asking a code question on Stack Overflow.\nYou’ll also see me set the seed when I’m making a function for a simulation and I want to make sure it works correctly. Otherwise in most simulations we don’t actually want or need to set the seed.\n\nset.seed(123)\na = rnorm(n = 10, mean = 55,sd = 8)\na\n\n [1] 50.51619 53.15858 67.46967 55.56407 56.03430 68.72052 58.68733 44.87951\n [9] 49.50518 51.43470"
  },
  {
    "objectID": "simulate.html#discrete-counts-with-rpois",
    "href": "simulate.html#discrete-counts-with-rpois",
    "title": "10  Simulate Data",
    "section": "10.8 Discrete counts with rpois()",
    "text": "10.8 Discrete counts with rpois()\nLet’s look at one last function for generating random numbers, this time for generating discrete integers (including 0) from a Poisson distribution with rpois().\nI use rpois() for generating counts for creating data to be fit with generalized linear models. This function has also helped me gain a better understanding of the shape of Poisson distributions with different means.\nThe Poisson distribution is a single parameter distribution. The function looks like:\nrpois(n, lambda)\nThe single parameter argument, lambda, is the mean. It has no default setting so must always be defined by the user.\nLet’s generate five values from a Poisson distribution with a mean of 2.5. Note that mean of the Poisson distribution can be any non-negative value (i.e., it doesn’t have to be an integer) even though the observed values will be discrete integers.\n\nrpois(n = 5, lambda = 2.5)\n\n[1] 4 3 3 7 3\n\n\nExample of using the simulated numbers from rpois() Let’s explore the Poisson distribution a little more, seeing how the distribution changes when the mean of the distribution changes. Being able to look at how the Poisson distribution changes with the mean via simulation helped me understand the distribution better, including why it so often does a poor job modeling ecological count data.\nWe’ll draw 100 values from a Poisson distribution with a mean of 5. We’ll name this vector y and take a look at a summary of those values.\n\ny = rpois(100, lambda = 5)\n\nThe vector of values we simulated here fall between 1 and 11.\n\nsummary(y)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    3.00    5.00    4.87    6.00   10.00 \n\n\n\nhist(y)\n\n\n\n\nLet’s do the same thing for a Poisson distribution with a mean of 100. The range of values is pretty narrow; there are no values even remotely close to 0.\n\ny = rpois(100, lambda = 100)\nsummary(y)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  76.00   94.75  100.00  100.62  106.00  123.00 \n\n\n\nhist(y)\n\n\n\n\nAn alternative to the Poisson distribution for discrete integers is the negative binomial distribution. Packages MASS has a function called rnegbin() for random number generation from the negative binomial distribution."
  },
  {
    "objectID": "simulate.html#generate-character-vectors-with-rep",
    "href": "simulate.html#generate-character-vectors-with-rep",
    "title": "10  Simulate Data",
    "section": "10.9 Generate character vectors with rep()",
    "text": "10.9 Generate character vectors with rep()\nQuantitative variables are great, but in simulations we’re often going to need categorical variables, as well.\nIn my own work these are usually sort of “grouping” or “treatment” variable, with multiple individuals/observations per group/treatment. This means I need to have repetitions of each character value. The rep() function is one way to avoid having to write out an entire vector manually. It’s for replicating elements of vectors and lists.\n\n10.9.1 Using letters and LETTERS\nThe first argument of rep() is the vector to be repeated. One option is to write out the character vector you want to repeat. You can also get a simple character vector through the use of letters or LETTERS. These are built in constants in R. letters is the 26 lowercase letters of the Roman alphabet and LETTERS is the 26 uppercase letters.\nLetters can be pulled out via the extract brackets ([). I use these built-in constants for pure convenience when I need to make a basic categorical vector and it doesn’t matter what form those categories take. I find it more straightforward to type out the word and brackets than a vector of characters (complete with all those pesky quotes and such 😆).\nHere’s the first two letters.\n\nletters[1:2]\n\n[1] \"a\" \"b\"\n\n\nAnd the last 17 LETTERS.\n\nLETTERS[10:26]\n\n [1] \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\" \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\n\n\n\n10.9.2 Repeat each element of a vector with each\nThere are three arguments that help us repeat the values in the vector in rep() with different patterns: each, times, and length.out. These can be used individually or in combination.\nWith each we repeat each unique character in the vector the defined number of times. The replication is done “elementwise”, so the repeats of each unique character are all in a row.\nLet’s repeat two characters three times each. The resulting vector is 6 observations long.\nThis is an example of how I might make a grouping variable for simulating data to be used in a two-sample analysis.\n\nrep(letters[1:2], each = 3)\n\n[1] \"a\" \"a\" \"a\" \"b\" \"b\" \"b\"\n\n\n\n\n10.9.3 Repeat a whole vector with the times argument\nThe times argument can be used when we want to repeat the whole vector rather than repeating it elementwise.\nWe’ll make a two-group variable again, but this time we’ll change the repeating pattern of the values in the variable.\n\nrep(letters[1:2], times = 3)\n\n[1] \"a\" \"b\" \"a\" \"b\" \"a\" \"b\"\n\n\n\n\n10.9.4 Set the output vector length with the length.out argument\nThe length.out argument has rep() repeat the whole vector. However, it repeats the vector only until the defined length is reached. Using length.out is another way to get unbalanced groups.\nRather than defining the number of repeats like we did with each and times we define the length of the output vector.\nHere we’ll make a two-group variable of length 5. This means the second group will have one less value than the first.\n\nrep(letters[1:2], length.out = 5)\n\n[1] \"a\" \"b\" \"a\" \"b\" \"a\"\n\n\n\n\n10.9.5 Repeat each element a different number of times\nUnlike each and length.out, we can use times with a vector of values. This allows us to repeat each element of the character vector a different number of times. This is one way to simulate unbalanced groups.\nUsing times with a vector repeats each element like each does. I found this a little confusing as it makes it harder to remember which argument repeats “elementwise” and which “vectorwise”. But length.out always repeats “vectorwise”, so that’s something.\nLet’s repeat the first element twice and the second four times.\n\nrep(letters[1:2], times = c(2, 4) )\n\n[1] \"a\" \"a\" \"b\" \"b\" \"b\" \"b\"\n\n\n\n\n10.9.6 Combining each with times\nAs your simulation situation get more complicated, like if you are simulating data from a blocked design or with multiple sizes of experimental units, you may need more complicated patterns for your categorical variable. The each argument can be combined with times to first repeat each value elementwise (via each) and then repeat that whole pattern (via times).\nWhen using times this way it will only take a single value and not a vector.\nLet’s repeat each value twice, 3 times.\n\nrep(letters[1:2], each = 2, times = 3)\n\n [1] \"a\" \"a\" \"b\" \"b\" \"a\" \"a\" \"b\" \"b\" \"a\" \"a\" \"b\" \"b\"\n\n\n\n\n10.9.7 Combining each with length.out\nSimilarly we can use each with length.out. This can lead to some imbalance.\nHere we’ll repeat the two values twice each and then repeat that pattern until we hit a total final vector length of 7.\n\nrep(letters[1:2], each = 2, length.out = 7)\n\n[1] \"a\" \"a\" \"b\" \"b\" \"a\" \"a\" \"b\"\n\n\nNote you can’t use length.out and times together (if you try, length.out will be given priority and times ignored).\n\n\n10.9.8 Creating datasets with quantiative and categorical variables\nWe now have some tools for creating quantitative data as well as categorical. Which means it’s time to make some datasets! We’ll create several simple ones to get the general idea."
  },
  {
    "objectID": "simulate.html#simulate-data-with-no-differences-among-two-groups",
    "href": "simulate.html#simulate-data-with-no-differences-among-two-groups",
    "title": "10  Simulate Data",
    "section": "10.10 Simulate data with no differences among two groups",
    "text": "10.10 Simulate data with no differences among two groups\nLet’s start by simulating data that we would use in a simple two-sample analysis with no difference between groups. We’ll make a total of 6 observations, three in each group.\nWe’ll be using the tools we reviewed above but will now name the output and combine them into a data.frame. This last step isn’t always necessary, but can help you keep things organized.\nFirst we’ll make separate vectors for the continuous and categorical data and then bind them together via data.frame().\nNotice there is no need to use cbind() here, which is commonly done by R beginners (I know I did!). Instead we can use data.frame() directly.\n\ngroup = rep(letters[1:2], each = 3)\nresponse = rnorm(n = 6, mean = 0, sd = 1)\ndata.frame(group,  response)\n\n  group    response\n1     a -0.49929202\n2     a  0.21444531\n3     a -0.32468591\n4     b  0.09458353\n5     b -0.89536336\n6     b -1.31080153\n\n\nWhen I make a data.frame like this I prefer to make my vectors and the data.frame simultaneously to avoid having a lot of variables cluttering up my R Environment.\nI often teach/blog with all the steps clearly delineated as I think it’s easier when you are starting out, so (as always) use the method that works for you.\n\ndata.frame(group = rep(letters[1:2], each = 3),\n           response = rnorm(n = 6, mean = 0, sd = 1) )\n\n  group   response\n1     a  1.9972134\n2     a  0.6007088\n3     a -1.2512714\n4     b -0.6111659\n5     b -1.1854801\n6     b  2.1988103\n\n\nNow let’s add another categorical variable to this dataset.\nSay we are in a situation involving two factors, not one. We have a single observations for every combination of the two factors (i.e., the two factors are crossed).\nThe second factor, which we’ll call factor, will take on the values “C”, “D”, and “E”.\n\nLETTERS[3:5]\n\n[1] \"C\" \"D\" \"E\"\n\n\nWe need to repeat the values in a way that every combination of group and factor is present in the dataset exactly one time.\nRemember the group factor is repeated elementwise.\n\nrep(letters[1:2], each = 3)\n\n[1] \"a\" \"a\" \"a\" \"b\" \"b\" \"b\"\n\n\nWe need to repeat the three values twice. But what argument do we use in rep() to do so?\n\nrep(LETTERS[3:5], ?)\n\nDoes each work?\n\nrep(LETTERS[3:5], each = 2)\n\n[1] \"C\" \"C\" \"D\" \"D\" \"E\" \"E\"\n\n\nNo, if we use each then each element is repeated twice and some of the combinations of group and the new variable will be missing.\nThis is a job for the times or length.out arguments, so the whole vector is repeated. We can repeat the whole vector twice using times, or via length.out = 6. I decided to do the former.\nIn the result below we can see every combination of the two factors is present once.\n\ndata.frame(group = rep(letters[1:2], each = 3),\n           factor = rep(LETTERS[3:5], times = 2),\n           response = rnorm(n = 6, mean = 0, sd = 1) )\n\n  group factor   response\n1     a      C  1.3124130\n2     a      D -0.2651451\n3     a      E  0.5431941\n4     b      C -0.4143399\n5     b      D -0.4762469\n6     b      E -0.7886028"
  },
  {
    "objectID": "simulate.html#simulate-data-with-a-difference-among-groups",
    "href": "simulate.html#simulate-data-with-a-difference-among-groups",
    "title": "10  Simulate Data",
    "section": "10.11 Simulate data with a difference among groups",
    "text": "10.11 Simulate data with a difference among groups\nThe dataset above is one with “no difference” among groups. What if we want data where the means are different between groups? Let’s make two groups of three observations where the mean of one group is 5 and the other is 10. The two groups have a shared variance (and so standard deviation) of 1.\nRemembering how rnorm() works with a vector of means is key here. The function draws iteratively from each distribution.\n\nresponse = rnorm(n = 6, mean = c(5,10), sd = 1)\nresponse\n\n[1]  4.405383 11.650907  4.945972 10.119245  5.243687 11.232476\n\n\nHow do we get the group pattern correct?\n\ngroup = rep(letters[1:2], ?)\n\nWe need to repeat the whole vector three times instead of elementwise.\nTo get the groups in the correct order we need to use times or length.out in rep(). With length.out we define the output length of the vector, which is 6. Alternatively we could use times = 3 to repeat the whole vector 3 times.\n\ngroup = rep(letters[1:2], length.out = 6)\n\nThese can then be combined into a data.frame. Working out this process is another reason why sometimes we build each vector separately prior to combining them into a data.frame.\n\ndata.frame(group,  response)\n\n  group  response\n1     a  4.405383\n2     b 11.650907\n3     a  4.945972\n4     b 10.119245\n5     a  5.243687\n6     b 11.232476"
  },
  {
    "objectID": "simulate.html#multiple-quantitative-variables-with-groups",
    "href": "simulate.html#multiple-quantitative-variables-with-groups",
    "title": "10  Simulate Data",
    "section": "10.12 Multiple quantitative variables with groups",
    "text": "10.12 Multiple quantitative variables with groups\nFor our last dataset we’ll have two groups, with 10 observations per group.\n\nrep(LETTERS[3:4], each = 10)\n\n [1] \"C\" \"C\" \"C\" \"C\" \"C\" \"C\" \"C\" \"C\" \"C\" \"C\" \"D\" \"D\" \"D\" \"D\" \"D\" \"D\" \"D\" \"D\" \"D\"\n[20] \"D\"\n\n\nLet’s make a dataset that has two quantitative variables, unrelated to both each other and the groups. One variable ranges from 10 and 15 and one from 100 and 150.\nHow many observations should we draw from each uniform distribution?\n\nrunif(n = ?, min = 10, max = 15)\n\nWe had 2 groups with 10 observations each and 2*10 = 20. So we need to use n = 20 in runif().\nHere is the dataset made in a single step.\n\ndata.frame(group = rep(LETTERS[3:4], each = 10),\n           x = runif(n = 20, min = 10, max = 15),\n           y = runif(n = 20, min = 100, max = 150))\n\n   group        x        y\n1      C 11.51452 138.0200\n2      C 11.94102 107.3542\n3      C 10.80238 117.9028\n4      C 14.31276 133.6666\n5      C 14.76551 126.1911\n6      C 12.81822 117.4901\n7      C 11.64774 112.0265\n8      C 14.98309 102.9096\n9      C 11.17410 111.8310\n10     C 13.06336 144.5039\n11     D 10.54089 140.5914\n12     D 12.43516 137.3758\n13     D 10.49723 107.7456\n14     D 10.80583 106.2371\n15     D 11.41496 148.7363\n16     D 12.91936 121.8065\n17     D 13.65854 123.2008\n18     D 10.82760 108.2649\n19     D 14.33234 129.2468\n20     D 13.54287 113.5389\n\n\nWhat happens if we get the number wrong? If we’re lucky we get an error that will help us troubleshoot the problem.\n\ndata.frame(group = rep(LETTERS[3:4], each = 10),\n           x = runif(n = 15, min = 10, max = 15),\n           y = runif(n = 15, min = 100, max = 150))\n\nBut if we get things wrong and the number we use happens to go into the number we need evenly, R will recycle the vector to the end of the data.frame().\nThis is a hard mistake to catch. If you look carefully through the output below you can see that the continuous variables start to repeat on line 10 because I used n = 10 instead of n = 20.\n\ndata.frame(group = rep(LETTERS[3:4], each = 10),\n           x = runif(n = 10, min = 10, max = 15),\n           y = runif(n = 10, min = 100, max = 150))\n\n   group        x        y\n1      C 11.15048 131.3573\n2      C 13.45604 100.0233\n3      C 11.41426 110.8622\n4      C 14.05199 135.2436\n5      C 10.46958 110.7583\n6      C 14.11015 140.6967\n7      C 12.13714 115.3882\n8      C 13.77944 134.3871\n9      C 13.31193 146.6341\n10     C 12.22264 105.7890\n11     D 11.15048 131.3573\n12     D 13.45604 100.0233\n13     D 11.41426 110.8622\n14     D 14.05199 135.2436\n15     D 10.46958 110.7583\n16     D 14.11015 140.6967\n17     D 12.13714 115.3882\n18     D 13.77944 134.3871\n19     D 13.31193 146.6341\n20     D 12.22264 105.7890"
  },
  {
    "objectID": "simulate.html#repeatedly-simulate-data-with-replicate",
    "href": "simulate.html#repeatedly-simulate-data-with-replicate",
    "title": "10  Simulate Data",
    "section": "10.13 Repeatedly simulate data with replicate()",
    "text": "10.13 Repeatedly simulate data with replicate()\nThe replicate() function is a real workhorse when making repeated simulations. It is a member of the apply family in R, and is specifically made (per the documentation) for the repeated evaluation of an expression (which will usually involve random number generation).\nWe want to repeatedly simulate data that involves random number generation, so that sounds like a useful tool.\nThe replicate() function takes three arguments:\nn, which is the number of replications to perform. This is to set the number of repeated runs we want. expr, the expression that should be run repeatedly. This is often a function. simplify, which controls the type of output the results of expr are saved into. Use simplify = FALSE to get output saved into a list instead of in an array.\n\n10.13.1 Simple example of replicate()\nLet’s say we want to simulate some values from a normal distribution, which we can do using the rnorm() function as above. But now instead of drawing some number of values from a distribution one time we want to do it many times. This could be something we’d do when demonstrating the central limit theorem, for example.\nDoing the random number generation many times is where replicate() comes in. It allows us to run the function in expr exactly n times.\nHere I’ll generate 5 values from a standard normal distribution three times. Notice the addition of simplify = FALSE to get a list as output.\nThe output below is a list of three vectors. Each vector is from a unique run of the function, so contains five random numbers drawn from the normal distribution with a mean of 0 and standard deviation of 1.\n\nset.seed(16)\nreplicate(n = 3, \n          expr = rnorm(n = 5, mean = 0, sd = 1), \n          simplify = FALSE )\n\n[[1]]\n[1]  0.4764134 -0.1253800  1.0962162 -1.4442290  1.1478293\n\n[[2]]\n[1] -0.46841204 -1.00595059  0.06356268  1.02497260  0.57314202\n\n[[3]]\n[1]  1.8471821  0.1119334 -0.7460373  1.6582137  0.7217206\n\n\nNote if I don’t use simplify = FALSE I will get a matrix of values instead of a list. Each column in the matrix is the output from one run of the function.\nIn this case there will be three columns in the output, one for each run, and 5 rows. This can be a useful output type for some simulations. I focus on list output throughout the rest of this post only because that’s what I have been using recently for simulations.\n\nset.seed(16)\n\nreplicate(n = 3, \n          expr = rnorm(n = 5, mean = 0, sd = 1) )\n\n           [,1]        [,2]       [,3]\n[1,]  0.4764134 -0.46841204  1.8471821\n[2,] -0.1253800 -1.00595059  0.1119334\n[3,]  1.0962162  0.06356268 -0.7460373\n[4,] -1.4442290  1.02497260  1.6582137\n[5,]  1.1478293  0.57314202  0.7217206\n\n\n\n\n10.13.2 An equivalent for() loop example\nA for() loop can be used in place of replicate() for simulations. With time and practice I’ve found replicate() to be much more convenient in terms of writing the code. However, in my experience some folks find for() loops intuitive when they are starting out in R. I think it’s because for() loops are more explicit on the looping process: the user can see the values that i takes and the output for each i iteration is saved into the output object because the code is written out explicitly.\nIn my example I’ll save the output of each iteration of the loop into a list called list1. I initialize this as an empty list prior to starting the loop. To match what I did with replicate() I do three iterations of the loop (i in 1:3), drawing 5 values via rnorm() each time.\nThe result is identical to my replicate() code above. It took a little more code to do it but the process is very clear since it is explicitly written out.\n\nset.seed(16)\n\nlist1 = list() # Make an empty list to save output in\nfor (i in 1:3) { # Indicate number of iterations with \"i\"\n    list1[[i]] = rnorm(n = 5, mean = 0, sd = 1) # Save output in list for each iteration\n}\n\nlist1\n\n[[1]]\n[1]  0.4764134 -0.1253800  1.0962162 -1.4442290  1.1478293\n\n[[2]]\n[1] -0.46841204 -1.00595059  0.06356268  1.02497260  0.57314202\n\n[[3]]\n[1]  1.8471821  0.1119334 -0.7460373  1.6582137  0.7217206\n\n\n\n\n10.13.3 Using replicate() to repeatedly make a dataset\nEarlier we were making datasets with random numbers and some grouping variables. Our code looked like:\n\ndata.frame(group = rep(letters[1:2], each = 3),\n           response = rnorm(n = 6, mean = 0, sd = 1) )\n\n  group   response\n1     a -1.6630805\n2     a  0.5759095\n3     a  0.4727601\n4     b -0.5427317\n5     b  1.1276871\n6     b -1.6477976\n\n\nWe could put this process as the expr argument in replicate() to get many simulated datasets. I would do something like this if I wanted to compare the long-run performance of two different statistical tools using the exact same random datasets.\nI’ll replicate things 3 times again to easily see the output. I still use simplify = FALSE to get things into a list.\n\nsimlist = replicate(n = 3, \n          expr = data.frame(group = rep(letters[1:2], each = 3),\n                            response = rnorm(n = 6, mean = 0, sd = 1) ),\n          simplify = FALSE)\n\nWe can see this result is a list of three data.frames.\n\nstr(simlist)\n\nList of 3\n $ :'data.frame':   6 obs. of  2 variables:\n  ..$ group   : chr [1:6] \"a\" \"a\" \"a\" \"b\" ...\n  ..$ response: num [1:6] -0.314 -0.183 1.47 -0.866 1.527 ...\n $ :'data.frame':   6 obs. of  2 variables:\n  ..$ group   : chr [1:6] \"a\" \"a\" \"a\" \"b\" ...\n  ..$ response: num [1:6] 1.03 0.84 0.217 -0.673 0.133 ...\n $ :'data.frame':   6 obs. of  2 variables:\n  ..$ group   : chr [1:6] \"a\" \"a\" \"a\" \"b\" ...\n  ..$ response: num [1:6] -0.943 -1.022 0.281 0.545 0.131 ...\n\n\n\nsimlist[[1]]\n\n  group   response\n1     a -0.3141739\n2     a -0.1826816\n3     a  1.4704785\n4     b -0.8658988\n5     b  1.5274670\n6     b  1.0541781"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "11  Summary",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "installR.html",
    "href": "installR.html",
    "title": "Appendix A — R language installation",
    "section": "",
    "text": "Go to the RStudio Comprehensive R Archive Network (CRAN) mirror at https://cran.rstudio.com. [You can select a different mirror by going to http://www.r-project.org, selecting the \"download R\" link in the \"Getting Started\" box, and selecting a mirror location from the ensuing page.]\nSelect the \"Download R for Windows\" link.\n\nSelect the \"base\" option.\n\nSelect the \"Download R 3.2.2 for Windows\" option (or similar if the version number has changed … the version is now at least 3.4.3). Either run the program or note where you saved this executable program on your computer.\n\nIf you did not run the program, then locate the downloaded file on your computer (called \"R-3.2.2-win.exe\" or similar if the version number has changed) and run it.\nSelect \"English\" language in the first dialog box (depending on your version of Windows you may have received security warnings before this dialog box appears).\nPress Next on the next two dialog boxes (first, a simple description; second, a user agreement).\nSelect a location to install R (simply use the default location if the location is not important to you – in the dialog box below I installed in a custom directory). Press Next.\n\nAt this point you can choose to install 32- or 64-bit or both versions of R. If you do not have a 64-bit computer, then you must install the 32-bit version. If you do have a 64-bit computer, then I suggest, initially and for simplicity, installing only one version or the other. I usually install the 32-bit version as it has some slight advantages when not working with extremely large data sets and with other software that I have installed on my machine (see this R FAQ){:target=\"_blank\"}. In this demonstration, I will install only the 32-bit version of R by deselecting the \"64-bit Files\" option. Press Next.\n\nSelect the \"No (accept defaults)\" (this is the default) option. Press Next.\nDecide whether to create a shortcut in your Start Menu folder (I suggest NOT). Press Next.\nDecide whether or not to create desktop or Quick Launch icons (top two choices) and whether to register the version number and associate .RData files with R (bottom two choices). Generally, you will want to register the version number and associate the .RData files with R. If you plan to use RStudio, then you will not run R directly and I suggest then that you do not create Desktop or Quick Launch icons. Press Next.\n\nR should then begin installing files into the directory you chose previously. If everything goes well, then you should get one last dialog box noting such. Press Finish.\n\nIf you plan to use RStudio, then you can now install it with these directions."
  },
  {
    "objectID": "installRstudio.html",
    "href": "installRstudio.html",
    "title": "Appendix B — What is RStudio?",
    "section": "",
    "text": "R is an open-source software environment for statistical computing and graphics that runs on Windows, Mac OS, and many UNIX platforms. Unlike many other programs, users interact with R through a command line rather than through a graphical user interface. While such an interface may be unfamiliar to many users, its primary strength is the ability for users to develop scripts of commands to perform various analyses that can then be easily repeated.\nRStudio is an open-source integrated development environment (IDE) that serves as a front-end “on top” of R. RStudio eases the user’s interaction with R by providing some of the conveniences of a GUI and, more importantly, a means for efficiently constructing and running R scripts. Among other conveniences, RStudio provides a four-panel layout that includes a feature-rich source-code editor (includes syntax highlighting, parentheses completion, spell-checking, etc.), a tight link to the R console, a system for examining objects saved in R, an interface to R help, and extended features to examine and save plots.\nRStudio is easy to learn. This page has directions for installing and configuring Rstudio for use in a Windows operating system and a brief introduction to using RStudio."
  },
  {
    "objectID": "installRstudio.html#installing-rstudio",
    "href": "installRstudio.html#installing-rstudio",
    "title": "Appendix B — What is RStudio?",
    "section": "B.1 Installing RStudio",
    "text": "B.1 Installing RStudio\n\nGo to the RStudio desktop download page.\nSelect the link from the “Installers for Supported Platforms” list that corresponds to the operating system appropriate for your computer. Either run the program or note where this executable program is saved on your computer.\n\nIf you did not run the program, then locate and run the downloaded file (called “RStudio-0.99.484.exe” or similar if the version number has changed … the version is now at least 1.1.383).\nPress Next on the first “Welcome” dialog box (depending on your version of Windows you may receive security warnings before this dialog box appears).\nSelect a location to install RStudio (simply use the default location if the location is not important to you – in the dialog box below I installed in a custom directory). Press Next.\n\nDecide whether or not to create a shortcut in the Start Menu folder (I suggest you DO). Press Install.\nRStudio should then begin installing files into the directory you chose previously. If everything goes well then you should get one last dialog box noting such. Press Finish.\nIf you did not create a shortcut above then you will need to locate the “rstudio.exe” file inside the “RStudio/bin” folders inside the folder you chose to install RStudio in. On my computer, for example, this file is inside of “C:/apps/RStudio/bin”."
  },
  {
    "objectID": "installRstudio.html#configuring-rstudio",
    "href": "installRstudio.html#configuring-rstudio",
    "title": "Appendix B — What is RStudio?",
    "section": "B.2 Configuring RStudio",
    "text": "B.2 Configuring RStudio\n\nOpen RStudio.\nSelect the Tools menu and Global Options submenu. In the ensuing dialog box select the General icon on the left (this should already be selected).\n\nDepending on your installation, the R version should read “[Default][32-bit]” followed by the path to the R program (as shown in the dialog box below). If you installed the 64-bit version of R, then select the Change... button and then “use your machine’s default version of R64 (64-bit)”.\nYou can either leave the other selections at their defaults or change them as you see fit (my preferences are shown in the dialog box below). However, I strongly urge you to deselect “Restore .RData into workspace at startup” and make sure “Save workspace to .RData on exit:” is set to “Never.”\n\n\nSelect the Packages icon in the Global Options dialog box opened above. It is useful to set a CRAN mirror in this dialog box. I prefer the “Global (CDN) - Rstudio” option but you may want to choose a location nearer to you (through the Change... button). All other options can remain at their defaults.\n\nSelect the Code icon in the Global Options dialog box opened above and the Display tab. I suggest, in addition to the default selections, selecting the “Highlight selected line,” “Show margin,” and “Show syntax highlighting in console input.”\n\nAt times I find the code completion options in RStudio irritating. If you do as well, you can either turn this option off or tweak its settings within the Completion tab under the Code icon in the Global Options` dialog box opened above.\nNo other options need to be set for most introductory purposes. Press OK."
  },
  {
    "objectID": "installRstudio.html#introducing-rstudio",
    "href": "installRstudio.html#introducing-rstudio",
    "title": "Appendix B — What is RStudio?",
    "section": "B.3 Introducing RStudio",
    "text": "B.3 Introducing RStudio\n\nB.3.1 RStudio Design\nRStudio is organized around a four-panel layout.\n\nThe upper-left panel is the R Script Editor. R commands are typed into this panel and submitted to the R Console in the lower-left panel. For most applications, you will type R commands into the Script Editor and submit them to the Console; you will not type commands directly into the Console. The Script Editor is a high-level text editor, whereas the Console is the R program.\nThe upper-right panel contains at least two tabs — Environment and History. Many items listed under the Environment tab can be double-clicked to open them for viewing as a tab in the Script Editor. The History tab simply shows all of the commands that you have submitted to the Console during the current session.\nThe lower-right panel contains at least five tabs – Files, Plots, Packages, Help, and Viewer. The Plots tab will show the plots produced by commands submitted to the Console. One can cycle through the history of constructed plots with the arrows on the left side of the plot toolbar and plots can be saved to external files using the “Export” tab on the plot toolbar (see figure above). A list of all installed packages is seen by selecting the Packages tab (packages can also be installed through this tab as described in a separate document). Help for each package can be obtained by clicking on the name of the package. The help will then appear in the Help tab.\n\n\nB.3.2 Basic Usage\nYour primary interaction with RStudio will be through developing R scripts in the Script Editor, submitting those scripts to the Console, and viewing textual or tabular results in the Console and graphical results in the Plot panel. In this section, I briefly introduce how to construct and run R scripts in RStudio.\nOne opens a blank file for an R script by selecting the New icon () and then R Script; selecting the File menu, New submenu, and R Script item; or with <CTRL>+<Shift>+N. In the newly created Script Editor panel, type the three lines exactly as shown below (for the moment, don’t worry about what these lines do.).\n\n\n\ndat <- rnorm(100)    # create random normal data (n=100) hist(dat,main=\"\")    # histogram of data without a title summary(dat)         # summary statistics\n\n\nThese commands must be submitted to the Console to perform the requested calculations. Commands may be submitted to the Console in a variety of ways:\n\nPut the cursor on a line in the Script Editor and press the Run icon (); altenatively press <CTRL>+<Enter>). This will submit that line to the Console and move the cursor to the next line in the Script Editor. Pressing  (or <CTRL>+<Enter>) will submit this next line. And so on.\nSelect all lines in the Script Editor that you wish to submit and press  (or <CTRL>+<Enter>).\n\nThe RStudio layout after using the first method is shown in the figure above.\nThe R Script in the Script Editor should now be saved by selecting the File menu and the Save item (alternatively, pressing <CTRL>+S). RStudio can now be closed (do NOT save the workspace). When RStudio is restarted later, the script can be reopened (choose the File menu and the Open file ... submenu if the file is not already in the Script Editor) and resubmitted to the Console to exactly repeat the analyses. (Note that the results of commands are not saved in R or RStudio; rather the commands are saved and resubmitted to reperform the analysis.)"
  },
  {
    "objectID": "installPackages.html",
    "href": "installPackages.html",
    "title": "Appendix C — Install packages in R",
    "section": "",
    "text": "Packages distributed via the Comprehensive R Archive Network (CRAN) extend the functionality of R. These directions explain how to install packages from within R (not using RStudio). If you have chosen to use RStudio, then goto the directions for installing packages within RStudio.\n\nOpen R. (If R is not installed, then follow these directions to install R for Windows.)\nSelect the Packages menu and Install package(s) submenu items.\nIn the dialog box, select the packages to install (use the <CTRL> key to select multiple packages). R should now install these packages plus all packages that these depend on. This may take several minutes and you should see several \"package 'xxx' successfully unpacked and MD5 sums checked\" messages.\n\nDepending on your priveleges on your machine, you may get a warning at this point about a library that \"is not writable\" and then be prompted with a dialog box asking \"Would you like to use a personal library instead?\" You can select Yes on this dialog box. A second dialog box will appear with a question that starts with \"Would you like to create a personal library.\" You can also select Yes on this dialog box."
  },
  {
    "objectID": "installPackages.html#installing-fsa-and-fsadata",
    "href": "installPackages.html#installing-fsa-and-fsadata",
    "title": "Appendix C — Install packages in R",
    "section": "C.2 Installing FSA and FSAdata",
    "text": "C.2 Installing FSA and FSAdata\nThe FSA and FSAdata packages are distributed on CRAN and can be installed using the directions above."
  },
  {
    "objectID": "lw.html",
    "href": "lw.html",
    "title": "Appendix D — Weight-Length Relationships",
    "section": "",
    "text": "DVR with More than Two Groups\nDummy variable regression (DVR) was introduced in Chapter 7 of Introductory Fisheries Analyses with R in the context of determining if the slope and y-intercept parameters of weight-length relationship regressions differed between fish captured in two different years. That analysis may be extended to more than two groups, though more dummy variables are required and special methods are needed to determine which pairs of groups (if any) differ. This supplement demonstrates how to extend the DVR to more than two groups."
  },
  {
    "objectID": "lw.html#required-packages-for-this-supplement",
    "href": "lw.html#required-packages-for-this-supplement",
    "title": "Appendix D — Weight-Length Relationships",
    "section": "D.1 Required Packages for this Supplement",
    "text": "D.1 Required Packages for this Supplement\nFunctions used in this supplement require the packages shown below.\n> library(FSA)\n> library(dplyr)\n> library(car)"
  },
  {
    "objectID": "lw.html#data-used-in-this-supplement",
    "href": "lw.html#data-used-in-this-supplement",
    "title": "Appendix D — Weight-Length Relationships",
    "section": "D.2 Data Used in this Supplement",
    "text": "D.2 Data Used in this Supplement\nWeights (g) and total lengths (mm) from Ruffe (Gymnocephalus cernuus) captured in the St.Louis River Harbor (Minnesota and Wisconsin) were used in Chapter 7 of IFAR and will also be used in this supplement. These data are from Ogle and Winfield (2009) and are in RuffeSLRH.csv ( download). To eliminate within-season variability, only Ruffe captured in July are used here. Additionally, a factored version of year was created, the common logarithms of weight and length were created, and the fishID, month, and day variables were removed to save space in the output.\n> ruf <- read.csv(\"RuffeSLRH.csv\") %>%\n    filterD(month==7) %>%\n    mutate(fYear=factor(year),logW=log10(wt),logL=log10(tl)) %>%\n    select(-fishID,-month,-day)\n> headtail(ruf)\n\n     year  tl   wt fYear      logW     logL\n1    1988  78  6.0  1988 0.7781513 1.892095\n2    1988  81  7.0  1988 0.8450980 1.908485\n3    1988  82  7.0  1988 0.8450980 1.913814\n1936 2004 137 28.0  2004 1.4471580 2.136721\n1937 2004 143 31.4  2004 1.4969296 2.155336\n1938 2004 174 82.4  2004 1.9159272 2.240549\nFor the first example below, only fish from 1990, 1995, and 2000 were examined. In the second example below, fish from 1992 to 1995 were examined.\n> ruf1 <- filterD(ruf,year %in% c(1990,1995,2000))\n> ruf2 <- filterD(ruf,year %in% 1992:1995)"
  },
  {
    "objectID": "lw.html#full-model",
    "href": "lw.html#full-model",
    "title": "Appendix D — Weight-Length Relationships",
    "section": "D.3 Full Model",
    "text": "D.3 Full Model\nThe number of dummy variables required to represent kk groups is k−1k−1. Thus, in Chapter 7 of IFAR, only one dummy variable was required to represent the two groups. In this example, three groups (the years) are being examined and, thus, two dummy variables are needed. For example, lm() will ultimately treat the “1990” group as the reference group and create two dummy variables as follows\nfYear1995={1, if captured in 19950, if NOT captured in 1995fYear1995={1, if captured in 19950, if NOT captured in 1995\nfYear2000={1, if captured in 20000, if NOT captured in 2000fYear2000={1, if captured in 20000, if NOT captured in 2000\nThese dummy variables are each combined with the log10(L)log10(L) covariate to construct the following ultimate full model\nlog10(Wi)=log10(α)+βlog10(Li)+δ1fYear1995+δ2fYear2000+γ1fYear1995∗log10(Li)+γ2fYear2000∗log10(Li)+ϵi(1)log10(Wi)=log10(α)+βlog10(Li)+δ1fYear1995+δ2fYear2000+γ1fYear1995∗log10(Li)+γ2fYear2000∗log10(Li)+ϵi(1)\nSubstitution of appropriate values for the dummy variables into Equation 1 shows how Equation 1 simultaneously represents the weight-length relationship regressions for all three years (Table 1). From these submodels, it is apparent that αα is the y-intercept for the reference (e.g., 1990) group, ββ is the slope for the reference group, δiδi is the difference in y-intercepts between the iith and reference groups, and γiγi is the difference in slopes between the iith and reference groups. By extension, the interaction variables measure differences in slopes and the dummy variables measure differences in y-intercepts.\nTable 1: The submodels by capture year represented by the full model.\n\n\n\n\n\n\n\n\n\nYear\nfYear1995\nfYear2000\nSubmodel (log10(Wi)=log10(Wi)=)\n\n\n\n\n1990\n0\n0\nlog10(α)+βlog10(Li)log10(α)+βlog10(Li)\n\n\n1995\n1\n0\n(log10(α)+δ1)+(β+γ1)∗log10(Li)(log10(α)+δ1)+(β+γ1)∗log10(Li)\n\n\n2000\n0\n1\n(log10(α)+δ2)+(β+γ2)∗log10(Li)(log10(α)+δ2)+(β+γ2)∗log10(Li)\n\n\n\nThe model in Equation 1 is fit with lm() using a formula of the form y~x*factor exactly as described in Chapter 7 of IFAR (again noting that lm() will create the appropriate dummy and interaction variables).\n> fit1 <- lm(logW~logL*fYear,data=ruf1)\nThe linearity and homoscedasticity assumptions (Figure 1-Left) and normality assumption (Figure 1-Right) are largely met with this model.\n> residPlot(fit1,legend=FALSE)\n\nFigure 1: Modified residual plot (Left) and histogram of residuals (Right) from fitting a dummy variable regression to the log-transformed weights and lengths of Ruffe captured in 1990, 1995, and 2000.\nAn ANOVA table is constructed (using Anova() from car) and interpreted (sequentially starting at the bottom of the table) as described in Chapter 7 of IFAR.\n> Anova(fit1)\n\nAnova Table (Type II tests)\n\nResponse: logW\n           Sum Sq  Df   F value    Pr(>F)\nlogL       43.529   1 22278.435 < 2.2e-16\nfYear       0.451   2   115.429 < 2.2e-16\nlogL:fYear  0.052   2    13.395 2.159e-06\nResiduals   0.971 497                    \nFrom these results it is apparent that the interaction term is a significant predictor in the model. In relation to Equation 1 this suggests that at least one of γ1γ1 or γ2γ2 is significantly different than zero, which implies that the slope of the relationship for fish captured in 1995, 2000, or both differs significantly from the slope for fish captured in 1990. Additionally, it is possible that the slopes for fish captured in 1995 and 2000 also differ, but this cannot be assessed with this model.\nThe ANOVA table for the fit of the full model is useful for determining if there is some difference in the regression model parameters among groups, but it cannot specifically identify where those differences occur. Specific differences are identified in the next section."
  },
  {
    "objectID": "lw.html#pinpointing-specific-differences-among-slopes",
    "href": "lw.html#pinpointing-specific-differences-among-slopes",
    "title": "Appendix D — Weight-Length Relationships",
    "section": "D.4 Pinpointing Specific Differences Among Slopes",
    "text": "D.4 Pinpointing Specific Differences Among Slopes\nNo simple method exists (such as Tukey’s Honestly Significant Difference (HSD) method) for comparing slopes among all pairs of groups while controlling for an increase in experimentwise error due to multiple comparisons. One strategy to perform these tests, though, is to fit the DVR as in the previous section and extract all of the p-values for comparisons of slopes to the reference level. Another model is then fit where the reference level has been changed and all p-values for comparisons of slopes to this level are extracted. This process is repeated until p-values for all pairwise comparisons have been extracted. The set of p-values from these comparisons is then submitted to procedures, such as one of the sequential Bonferroni or false discovery rate procedures, that are designed to adjust a set of p-values for multiple comparisons.\nControlling the experimentwise error rate for multiple comparisons is easily accomplished in R with p.adjust(). However, the sequential resetting of the reference level, fitting a new model, and extracting the p-values from all slope comparisons is tedious, especially for many groups. Fortunately, compSlopes() from FSA automates the tedious computation of the set of p-values and use of p.adjust() to control the experimentwise error rate.\nThe compSlopes() function requires a DVR model fit from lm() as its first argument. The method to control for multiple comparisons is set with method= and may be any of the methods returned by p.adjust.methods() (see ?p.adjust).\n> compSlopes(fit1)\n\nMultiple Slope Comparisons (using the 'holm' adjustment)\n\n  comparison     diff  95% LCI  95% UCI p.unadj   p.adj\n1  1995-1990 -0.17186 -0.27049 -0.07324 0.00067 0.00134\n2  2000-1990 -0.22852 -0.31738 -0.13965 0.00000 0.00000\n3  2000-1995 -0.05665 -0.15236  0.03905 0.24538 0.24538\n\n\nSlope Information (using the 'holm' adjustment)\n\n  level  slopes 95% LCI 95% UCI p.unadj p.adj\n3  2000 2.79665 2.73611 2.85719       0     0\n2  1995 2.85330 2.77918 2.92743       0     0\n1  1990 3.02516 2.96011 3.09022       0     0\nTwo sets of results are returned by compSlopes(). The top set of results shows the difference in slopes (diff), unadjusted confidence interval for the difference in slopes (95% LCI and 95% UCI), and unadjusted (p.unadj) and adjusted (p.adj) p-values for testing that the difference in slopes is not zero. These results show that the slope for 1990 is significantly greater than the slopes for 1995 and 2000, and that the slopes for 1995 and 2000 do not differ significantly.\nThe bottom set of results shows the slope (slope), unadjusted confidence interval for the slope (95% LCI and 95% UCI), and unadjusted (p.unadj) and adjusted (p.adj) p-values for testing that the slope is not equal to zero. In this case, this output is primarily for completeness, as these hypothesis are not generally of interest with weight-length relationship regressions."
  },
  {
    "objectID": "lw.html#a-summary-plot",
    "href": "lw.html#a-summary-plot",
    "title": "Appendix D — Weight-Length Relationships",
    "section": "D.5 A Summary Plot",
    "text": "D.5 A Summary Plot\nA plot that shows the transformed weight-length data with best-fit lines for each year superimposed (Figure 2) is constructed with the code below. This code follows that found in the IFAR book with the exception that col2rgbt() from FSA is used to add transparency to each color in a vector of colors.\n> ## Base plot\n> clrs1 <- c(\"black\",\"red\",\"blue\")\n> clrs2 <- col2rgbt(clrs1,1/4)\n> plot(logW~logL,data=ruf1,pch=19,col=clrs2[fYear],\n       xlab=\"log(Total Length)\",ylab=\"log(Weight)\")\n> ## Fitted lines\n> ( cfs <- coef(fit1) )\n\n   (Intercept)           logL      fYear1995      fYear2000 logL:fYear1995 \n    -4.9144676      3.0251636      0.2817809      0.3942964     -0.1718633 \nlogL:fYear2000 \n    -0.2285159 \n\n> minx <- min(ruf1$logL)\n> maxx <- max(ruf1$logL)\n> curve(cfs[1]+cfs[2]*x,from=minx,to=maxx,col=clrs1[1],lwd=2,add=TRUE)\n> curve((cfs[1]+cfs[3])+(cfs[2]+cfs[5])*x,from=minx,to=maxx,col=clrs1[2],lwd=2,add=TRUE)\n> curve((cfs[1]+cfs[4])+(cfs[2]+cfs[6])*x,from=minx,to=maxx,col=clrs1[3],lwd=2,add=TRUE)\n> ## Add legend\n> legend(\"topleft\",levels(ruf1$fYear),pch=19,col=clrs1)\n\nFigure 2: Log-transformed weight versus log-transformed length of Ruffe separated by capture year."
  },
  {
    "objectID": "lw.html#pinpointing-specific-differences-among-intercepts",
    "href": "lw.html#pinpointing-specific-differences-among-intercepts",
    "title": "Appendix D — Weight-Length Relationships",
    "section": "D.6 Pinpointing Specific Differences Among Intercepts",
    "text": "D.6 Pinpointing Specific Differences Among Intercepts\nWhen a difference in slopes exists, as in the previous example, it generally does not make sense to compare intercepts. However, if the slopes do not differ, then testing for differences in intercepts becomes important because, with parallel lines (i.e., same slopes), a difference in intercepts implies that the mean value of the response variable differs at every value of the explanatory variable.\nThe example below fits a DVR of the weight-length relationship regressions for the years from 1992 to 1995 (data.frame constructed above). In this example, the interaction term is not a significant predictor which indicates that none of the slopes differ. However, the term related to the factor variable is significant, which implies that at least one of the δiδi is different from zero. Thus, the y-intercept for at least of 1993, 1994, or 1995 differs from the y-intercept for 1992 (the reference level).\n> fit2 <- lm(logW~logL*fYear,data=ruf2)\n> Anova(fit2)\n\nAnova Table (Type II tests)\n\nResponse: logW\n           Sum Sq  Df    F value Pr(>F)\nlogL       75.798   1 45594.0523 <2e-16\nfYear       0.326   3    65.4373 <2e-16\nlogL:fYear  0.008   3     1.6373 0.1793\nResiduals   1.235 743                  \nThe same problem of incomplete comparisons and increasing experimentwise error occurs here with the y-intercepts as occurred with the slopes above. The solution, however, is different here because the y-intercept is a point estimate, whereas the slope estimates a rate of change. In this case, Tukey’s HSD method can be used to determine which pairs of y-intercepts differ. However, the observed values need to be adjusted to a single values of the covariate.\nThe adjusted value for an individual is computed by adding the individual’s residual from the full model to the predicted value of the response variable (from the full model gieven the individuals group) at a common covariate value. Visually, this adjustment may be thought of as sliding each point along the regression line (for the individual’s group) to a common value (for all groups) along the x-axis.\nThe compIntercepts() function from FSA performs these adjustments and applies Tukey’s HSD procedure to the adjusted values. The compIntercepts() function requires the fitted DVR as the first argument. By default, the response variables are are adjusted to the mean value of the covariate unless a value is given to common.cov=. For example, the actual y-intercepts will be used if common.cov=0. Also note that a warning is given if the DVR model contains an interaction term, though compIntercepts() will adjust for this by fitting a new model without the nonsignificant interaction term.\n> compIntercepts(fit2)\n\nWarning: Removed an interaction term from 'mdl' (i.e., assumed\n parallel lines) to test intercepts.\n\nTukey HSD on means adjusted assuming parallel lines\n\n  comparison          diff     95% LCI      95% UCI     p.adj\n1  1993-1992 -0.0560847608 -0.06723042 -0.044939103 0.0000000\n2  1994-1992 -0.0609843518 -0.07392816 -0.048040541 0.0000000\n3  1995-1992 -0.0616068456 -0.07461176 -0.048601927 0.0000000\n4  1994-1993 -0.0048995910 -0.01528465  0.005485465 0.6176269\n5  1995-1993 -0.0055220848 -0.01598321  0.004939036 0.5254957\n6  1995-1994 -0.0006224938 -0.01298177  0.011736778 0.9992208\n\n\nMean logW when logL=2.012621\n\n    1992     1993     1994     1995 \n1.171924 1.115839 1.110939 1.110317 \nTwo sets of results are returned by compIntercepts(). The top set of results shows the difference in y-intercepts (diff), unadjusted confidence interval for the difference in y-intercepts (95% LCI and 95% UCI), and adjusted (p.adj) p-values for testing that the difference in y-intercepts is not zero. These results show that the y-intercept for 1992 is significantly greater than the y-intercepts for all other years, which did not differ significantly.\nThe bottom set of results shows the mean value of the response variable at a common value of the covariate for each group. For example, the mean log10(W)log10(W) for when log10(L)log10(L)=2.012621 for fish captured in 1992 is 1.171924.\n> ## Base plot\n> clrs1 <- c(\"black\",\"red\",\"blue\",\"green\")\n> clrs2 <- col2rgbt(clrs1,1/4)\n> plot(logW~logL,data=ruf2,pch=19,col=clrs2[fYear],\n       xlab=\"log(Total Length)\",ylab=\"log(Weight)\")\n> ## Fitted lines\n> ( cfs <- coef(fit2) )\n\n   (Intercept)           logL      fYear1993      fYear1994      fYear1995 \n  -4.553035394    2.846025564   -0.197272960   -0.189460113   -0.079651305 \nlogL:fYear1993 logL:fYear1994 logL:fYear1995 \n   0.068720265    0.062584797    0.007274738 \n\n> minx <- min(ruf2$logL)\n> maxx <- max(ruf2$logL)\n> curve(cfs[1]+cfs[2]*x,from=minx,to=maxx,col=clrs1[1],lwd=2,add=TRUE)\n> curve((cfs[1]+cfs[3])+(cfs[2]+cfs[6])*x,from=minx,to=maxx,col=clrs1[2],lwd=2,add=TRUE)\n> curve((cfs[1]+cfs[4])+(cfs[2]+cfs[7])*x,from=minx,to=maxx,col=clrs1[3],lwd=2,add=TRUE)\n> curve((cfs[1]+cfs[5])+(cfs[2]+cfs[8])*x,from=minx,to=maxx,col=clrs1[4],lwd=2,add=TRUE)\n> ## Add legend\n> legend(\"topleft\",levels(ruf2$fYear),pch=19,col=clrs1)\n\nFigure 3: Log-transformed weight versus log-transformed length of Ruffe separated by capture year."
  },
  {
    "objectID": "lw.html#reproducibility-information",
    "href": "lw.html#reproducibility-information",
    "title": "Appendix D — Weight-Length Relationships",
    "section": "D.7 Reproducibility Information",
    "text": "D.7 Reproducibility Information\n\nCompiled Date: Thu Nov 05 2015\nCompiled Time: 8:54:13 PM\nR Version: R version 3.2.2 (2015-08-14)\nSystem: Windows, i386-w64-mingw32/i386 (32-bit)\nBase Packages: base, datasets, graphics, grDevices, methods, stats, utils\nRequired Packages: FSA, dplyr, car, captioner, knitr and their dependencies (assertthat, DBI, digest, evaluate, formatR, gdata, gplots, graphics, grDevices, highr, Hmisc, lazyeval, magrittr, markdown, MASS, methods, mgcv, nnet, pbkrtest, plotrix, plyr, quantreg, R6, Rcpp, sciplot, stats, stringr, tools, utils, yaml)\nOther Packages: captioner_2.2.3, car_2.1-0, dplyr_0.4.3, extrafont_0.17, FSA_0.8.4, knitr_1.11, rmarkdown_0.8.1\nLoaded-Only Packages: assertthat_0.1, DBI_0.3.1, digest_0.6.8, evaluate_0.8, extrafontdb_1.0, formatR_1.2.1, gdata_2.17.0, grid_3.2.2, gtools_3.5.0, htmltools_0.2.6, lattice_0.20-33, lazyeval_0.1.10, lme4_1.1-10, magrittr_1.5, MASS_7.3-44, Matrix_1.2-2, MatrixModels_0.4-1, mgcv_1.8-8, minqa_1.2.4, nlme_3.1-122, nloptr_1.0.4, nnet_7.3-11, parallel_3.2.2, pbkrtest_0.4-2, plyr_1.8.3, quantreg_5.19, R6_2.1.1, Rcpp_0.12.1, Rttf2pt1_1.3.3, SparseM_1.7, splines_3.2.2, stringi_1.0-1, stringr_1.0.0, tools_3.2.2, yaml_2.1.13\nLinks: Script / RMarkdown"
  },
  {
    "objectID": "lw.html#references",
    "href": "lw.html#references",
    "title": "Appendix D — Weight-Length Relationships",
    "section": "D.8 References",
    "text": "D.8 References\nOgle, D. H., and I. J. Winfield. 2009. Ruffe length-weight relationships with a proposed standard weight equation. North American Journal of Fisheries Management 29:850–858."
  },
  {
    "objectID": "basicstats.html",
    "href": "basicstats.html",
    "title": "Appendix E — Basic Data Manipulations",
    "section": "",
    "text": "Terminology\nCommon Back-Calculation Models\nData Organization & Manipulation\nComputing Back-Calculated Lengths\n\nScale-Length Relationships\nApplying the Back-Calculation Models"
  },
  {
    "objectID": "basicstats.html#terminology",
    "href": "basicstats.html#terminology",
    "title": "Appendix E — Basic Data Manipulations",
    "section": "E.1 Terminology",
    "text": "E.1 Terminology\nTwo types of measurements can be made on calcified structures. A radial measurement is the total distance from the center of the structure (e.g., focus of scale or nucleus of otolith) to the edge of an annulus. An incremental measurement is the distance between two successive annuli. Radial measurements are required for back-calculation of fish length.\nBack-calculation models estimate length at previous age ii (i.e., LiLi) from known values of length at time of capture (LCLC), scale radius to the iith annulus (SiSi), and scale radius at time of capture (SCSC). Several back-calculation models rely on the relationship between SCSC and LCLC. Depending on the model, a function of mean SCSC for a given LCLC (i.e., E(SC|LC)E(SC|LC) ) or mean LCLC for a given SCSC (i.e., E(LC|SC)E(LC|SC)) is used. These functions are not required to be linear, but often are, and in their linear form are represented as\nE(SC|LC)=a+bLC(1)E(SC|LC)=a+bLC(1)\nE(LC|SC)=c+dSC(2)E(LC|SC)=c+dSC(2)\nFitting these regressions is demonstrated below."
  },
  {
    "objectID": "basicstats.html#common-back-calculation-models",
    "href": "basicstats.html#common-back-calculation-models",
    "title": "Appendix E — Basic Data Manipulations",
    "section": "E.2 Common Back-Calculation Models",
    "text": "E.2 Common Back-Calculation Models\nThe first back-calculation model was jointly developed by Knut Dahl and Einar Lea and appeared in Lea (1910). The underlying concept of the Dahl-Lea model is that growth of the calcified structure is in exact proportion to growth in length of the fish. With this, the ratio of SiSi to SCSC is the same as the ratio of LiLi to LCLC. Rearrangement of this equality yields the Dahl-Lea back-calculation model\nLi=SiSCLC(3)Li=SiSCLC(3)\nThe Dahl-Lea model describes a family of straight lines that pass through the origin and each observed (SC,LCSC,LC) point. Visually (Figure 1), the estimated LiLi for a particular fish is found by locating SiSi along the x-axis, moving vertically until the straight line for that fish is met, and then moving horizontally to the point on the y-axis.\n\nFigure 1: Plot of length-at-capture versus scale radius for West Bearskin Lake Smallmouth Bass in 1990. All four methods of backcalculation are shown for fish 704 (S2=3.49804S2=3.49804, LC=218LC=218, and SC=7.44389SC=7.44389; black point and line) with calculational steps shown with the arrows. Fish 701 is shown as the gray point and line for comparative purposes.\nFraser (1916) was the first to describe, but Lee (1920) was the first to formally derive, the back-calculation model from the concept that “the growth increment of the scale is, on the average …, a constant proportion of the growth increment of the fish” (Francis 1990). In practice, the Fraser-Lee model modified the Dahl-Lea model by adjusting for the length of the fish when the calcified structure forms (i.e., L=cL=c when S=0S=0), that is,\nLi=SiSC(LC−c)+c(4)Li=SiSC(LC−c)+c(4)\nwhere cc comes from the length of the fish at the time of scale formation, the intercept of the length-scale relationship regression (e.g., from Equation 2), or from published “standards” for a species (Carlander 1982). The Fraser-Lee model describes a family of lines with an intercept of cc that pass through the (SC,LCSC,LC) point (Francis (1990); Figure 1).\nThe scale proportional hypothesis (SPH) model was named by Francis (1990), but was first recognized by Whitney and Carlander (1956) when they said “{i}f the scale was 10 per cent larger when the fish was caught than the average scale for that size of fish, [then] the scale would be 10 per cent larger than normal throughout the life.” If “average” and “normal” are considered to be expected values, then this hypothesis can be written as\nSiE[S|Li]=SCE[S|LC]SiE[S|Li]=SCE[S|LC]\nIf it is assumed that the scale-length relationship is linear, then the two expected values in these ratios are computed by plugging LiLi and LCLC, respectively, into the scale-length relationship (i.e., Equation 1) to produce\nSia+bLi=SCa+bLCSia+bLi=SCa+bLC\nwhich can be solved for LiLi to yield the general SPH back-calculation model\nLi=SiSC(LC+ab)−ab(5)Li=SiSC(LC+ab)−ab(5)\nThe linear SPH model produces a family of lines that all have an intercept of −ab−ab and pass through each observed (SC,LCSC,LC) point (Figure 1). The SPH model is the same as the Fraser-Lee model except that the intercept from Equation 2 is replaced with −ab−ab. Further note that the SPH model is the same as the Dahl-Lea model if a=0a=0.\nThe body proportional hypothesis (BPH) model was also named by Francis (1990) and was also first recognized by Whitney and Carlander (1956) when they said “{i}f a fish at time of capture were 10 per cent smaller than the average fish with that size of scale, [then] the fish would be 10 per cent smaller than the expected length for the size of that scale throughout life.” This hypothesis can be written as\nLiE[L|Si]=LCE[L|SC]LiE[L|Si]=LCE[L|SC]\nIf the length-scale relationship is linear then the expected values can be found by plugging SiSi and ScSc into Equation 2 to get\nLic+dSi=LCc+dSCLic+dSi=LCc+dSC\nwhich can be solved for LiLi to yield the general BPH back-calculation model\nLi=LCc+dSic+dSc(6)Li=LCc+dSic+dSc(6)\nThe linear BPH model produces a family of lines that have an intercept of cLCc+dSCcLCc+dSC and pass through each observed (SC,LCSC,LC) point (Figure 1). In contrast to the other back-calculation models, the BPH model uses lines with a different intercept for each fish. The linear BPH model is the same as the Dahl-Lea model if c=0c=0.\nVigliola and Meekan (2009) described 18 other models for the back-calculation of fish length. Functions for each of these models can be created with bcFuns() from FSA."
  },
  {
    "objectID": "basicstats.html#data-organization-manipulation",
    "href": "basicstats.html#data-organization-manipulation",
    "title": "Appendix E — Basic Data Manipulations",
    "section": "E.3 Data Organization & Manipulation",
    "text": "E.3 Data Organization & Manipulation\nIn wide or one-fish-per-line format, all information about a single fish, including all of the measurements from the calcified structure, is in one row of the data.frame. The wb90 data.frame contains radial measurments in the wide format (note that the portion shown below has four rows with columns that wrapped).\n> headtail(wb90,n=2)\n\n    yearcap fish agecap lencap    anu1    anu2    anu3    anu4    anu5    anu6\n1      1990  482      1     75 1.51076      NA      NA      NA      NA      NA\n2      1990  768      1     75 1.57989      NA      NA      NA      NA      NA\n180    1990  388      9    300 1.08462 2.03527 3.22724 4.63407 5.53355 6.53174\n181    1990  389      9    329 1.05913 2.18769 3.55137 4.40766 5.78634 7.58178\n       anu7    anu8     anu9   radcap\n1        NA      NA       NA  1.51076\n2        NA      NA       NA  1.57989\n180 7.27807 8.08080  9.38096  9.38096\n181 8.32094 9.46362 10.43491 10.43491\nFor the back-calculation of fish length, these data must be radial, and not incremental, measurements. If the wb90 data.frame had contained incremental measurements, then it could be converted to radial measurements with gConvert() from FSA. The gConvert() function requires the data frame with the incremental measurements as the first argument, the prefix (in quotes) for the columns that contain the incremental measurements in in.pre=, and the type of measurement to convert to in out.type= (the options are \"rad\" (the default) or \"inc\"). For example, the code below would create a new data.frame from wb90 with radial measurements (IF wb90 had incremental measurements).\n> wb90A <- gConvert(wb90,in.pre=\"anu\",out.type=\"rad\")\nFor efficient back-calculation, the data must also be converted to long or one-measurement-per-line format. As demonstrated in Introductory Fisheries Analyses with R, wide data may be converted to long data with gather() from tidyr. As a reminder, the arguments to gather() are the wide data.frame, a name for the new variable in the long format that will identify the individual (i.e., which radial measurement), a name for the new variable in the long format that will be the value for the individual (i.e., the radial measurement), and the variables in the wide format that contain the measurements.\n> wb90r <- gather(wb90,agei,radi,anu1:anu9) %>%\n    arrange(fish,agei)\n> headtail(wb90r)\n\n     yearcap fish agecap lencap  radcap agei    radi\n1       1990    0      7    278 9.06803 anu1 1.50631\n2       1990    0      7    278 9.06803 anu2 3.11450\n3       1990    0      7    278 9.06803 anu3 4.51154\n1627    1990  998      7    298 8.54805 anu7 8.54805\n1628    1990  998      7    298 8.54805 anu8      NA\n1629    1990  998      7    298 8.54805 anu9      NA\nAs noted in the IFAR book, there are three potential problems with this result. First, the new agei variable contains the names of the variables from the original wide format data.frame (e.g., anu1, anu2) rather than numbers that correspond to the age that the annulus was formed. Converting these labels to numbers begins by replacing the “anu” prefix with blanks (or an empty string) using str_sub() with the vector of names as the first argument, start=1 (because “anu” is a prefix) and end=3 (because “anu” is three characters long). The result from str_sub(), however, is a character that must then be converted to a numeric with as.numeric().\nThe second problem is that several of the radial measurements contain NA values. The non-NA values are found and retained by using !is.na() within filterD().\nThe third problem, while not an issue with these particular data, is that “plus growth” may have been recorded. “Plus growth” is growth on the margin of the calcified structure that does not represent a complete year of growth. If “plus growth” is present, then the new agei variable will have a value greater than the age-at-capture value. These instances should be removed from the new long format data.frame.\nThe following code adjusts for these three issues.\n> str_sub(wb90r$agei,start=1,end=3) <- \"\"\n> wb90r %<>% mutate(agei=as.numeric(agei)) %>%\n    filterD(!is.na(radi)) %>%\n    filterD(agei<=agecap)\n> headtail(wb90r)\n\n    yearcap fish agecap lencap  radcap agei    radi\n1      1990    0      7    278 9.06803    1 1.50631\n2      1990    0      7    278 9.06803    2 3.11450\n3      1990    0      7    278 9.06803    3 4.51154\n765    1990  998      7    298 8.54805    5 5.17646\n766    1990  998      7    298 8.54805    6 6.62240\n767    1990  998      7    298 8.54805    7 8.54805"
  },
  {
    "objectID": "basicstats.html#computing-back-calculated-lengths",
    "href": "basicstats.html#computing-back-calculated-lengths",
    "title": "Appendix E — Basic Data Manipulations",
    "section": "E.4 Computing Back-Calculated Lengths",
    "text": "E.4 Computing Back-Calculated Lengths\n\nE.4.1 Scale-Length Relationships\nThe scale-length (Equation 1) and length-scale (Equation 2) relationships required for all but the Dahl-Lea method are computed with the wide format data. Thus, the wide format data.frame must contain the length of the fish (e.g., lencap) and the total radius of the calcified structure (e.g., radcap) at the time of capture. Both linear relationships are fit with lm() and the coefficients should be extracted with coef() and saved into objects.\n> lm.sl <- lm(radcap~lencap,data=wb90)\n> ( a <- coef(lm.sl)[[1]] )\n\n[1] -1.304391\n\n> ( b <- coef(lm.sl)[[2]] )\n\n[1] 0.03537477\n\n> lm.ls <- lm(lencap~radcap,data=wb90)\n> ( c <- coef(lm.ls)[[1]] )\n\n[1] 41.65166\n\n> ( d <- coef(lm.ls)[[2]] )\n\n[1] 27.35597\n\n\nE.4.2 Applying the Back-Calculation Models\nThe LiLi estimated with a back-calculation model are most easily added to the long format data.frame. This is largely an exercise in adding a variable to the data.frame with mutate() from dplyr. For example, the LiLi computed with all four back-calculation models are added to wb90r below.\n> wb90r %<>% mutate(DL.len=(radi/radcap)*lencap,\n                    FL.len=(radi/radcap)*(lencap-c)+c,\n                    SPH.len=(-a/b)+(lencap+a/b)*(radi/radcap),\n                    BPH.len=lencap*(c+d*radi)/(c+d*radcap))\n> headtail(wb90r,n=2)\n\n    yearcap fish agecap lencap  radcap agei    radi    DL.len    FL.len\n1      1990    0      7    278 9.06803    1 1.50631  46.17918  80.91199\n2      1990    0      7    278 9.06803    2 3.11450  95.48171 122.82772\n766    1990  998      7    298 8.54805    6 6.62240 230.86847 240.25149\n767    1990  998      7    298 8.54805    7 8.54805 298.00000 298.00000\n      SPH.len   BPH.len\n1    76.92752  79.50736\n2   119.69064 121.72181\n766 239.17509 241.01809\n767 298.00000 298.00000\nFor example, the mean length-at-age may be computed from the back-calculated lengths (shown below for the Fraser-Lee results).\n> tmp <- wb90r %>%\n    group_by(agei) %>%\n    summarize(n=validn(FL.len),mn=mean(FL.len),sd=sd(FL.len)) %>%\n    as.data.frame()\n> tmp\n\n  agei   n       mn        sd\n1    1 181  78.5663  6.472692\n2    2 178 114.1527 10.453632\n3    3 155 146.7669 13.898434\n4    4  71 172.6512 15.339848\n5    5  64 201.0405 17.479717\n6    6  64 235.3834 23.350527\n7    7  50 268.5969 25.286958\n8    8   2 283.2237 26.912010\n9    9   2 314.5000 20.506097\nAdditionally, the mean length at each back-calculated age computed separately for each age-at-capture may be computed with sumTable() from FSA, where the left side of the formula is the quantitative variable to be summarized and the right side has grouping variables presented in row*column format.\n> sumTable(FL.len~agecap*agei,data=wb90r,digits=1)\n\n     1     2     3     4     5     6     7     8     9\n1 73.7    NA    NA    NA    NA    NA    NA    NA    NA\n2 79.7 113.3    NA    NA    NA    NA    NA    NA    NA\n3 77.3 112.8 148.9    NA    NA    NA    NA    NA    NA\n4 71.3 121.8 160.9 194.0    NA    NA    NA    NA    NA\n6 79.8 107.9 136.0 169.6 198.8 229.7    NA    NA    NA\n7 81.6 118.2 144.7 170.7 201.8 237.0 269.1    NA    NA\n9 71.2  99.8 135.0 166.1 197.5 236.0 256.4 283.2 314.5"
  },
  {
    "objectID": "basicstats.html#reproducibility-information",
    "href": "basicstats.html#reproducibility-information",
    "title": "Appendix E — Basic Data Manipulations",
    "section": "E.5 Reproducibility Information",
    "text": "E.5 Reproducibility Information\n\nCompiled Date: Thu Nov 05 2015\nCompiled Time: 6:48:50 PM\nR Version: R version 3.2.2 (2015-08-14)\nSystem: Windows, i386-w64-mingw32/i386 (32-bit)\nBase Packages: base, datasets, graphics, grDevices, methods, stats, utils\nRequired Packages: FSA, magrittr, dplyr, tidyr, stringr, captioner, knitr and their dependencies (assertthat, car, DBI, digest, evaluate, formatR, gdata, gplots, graphics, grDevices, highr, Hmisc, lazyeval, markdown, methods, plotrix, plyr, R6, Rcpp, sciplot, stats, stringi, tools, utils, yaml)\nOther Packages: captioner_2.2.3, dplyr_0.4.3, extrafont_0.17, FSA_0.8.4, FSAdata_0.3.2, knitr_1.11, magrittr_1.5, rmarkdown_0.8.1, stringr_1.0.0, tidyr_0.3.1\nLoaded-Only Packages: assertthat_0.1, DBI_0.3.1, digest_0.6.8, evaluate_0.8, extrafontdb_1.0, formatR_1.2.1, gdata_2.17.0, gtools_3.5.0, highr_0.5.1, htmltools_0.2.6, lazyeval_0.1.10, parallel_3.2.2, plyr_1.8.3, R6_2.1.1, Rcpp_0.12.1, Rttf2pt1_1.3.3, stringi_1.0-1, tools_3.2.2, yaml_2.1.13\nLinks: Script / RMarkdown"
  },
  {
    "objectID": "basicstats.html#references",
    "href": "basicstats.html#references",
    "title": "Appendix E — Basic Data Manipulations",
    "section": "E.6 References",
    "text": "E.6 References\nCarlander, K. D. 1982. Standard intercepts for calculating lengths from scale measurements for some centrarchid and percid fishes. Transactions of the American Fisheries Society 111:332–336.\nFrancis, R. I. C. C. 1990. Back-calculation of fish length: A critical review. Journal of Fish Biology 36:883–902.\nFraser, C. M. 1916. Growth of the spring salmon. Transactions of the Pacific Fisheries Society 1915:29–39.\nLea, E. 1910. On the methods used in the Herring-investigations. Publ. Circonst. Cons. perm. int. Explor. Mer 108(1):14–22.\nLee, R. M. 1920. A review of the methods of age and growth determination in fishes by means of scales. Fisheries Investigations, London Series 2 4((2)):1–32.\nVigliola, L., and M. G. Meekan. 2009. The back-calculation of fish growth from otoliths. Pages 174–211 in B. S. Green, B. D. Mapstone, G. Carlos, and G. A. Begg, editors. Tropical Fish Otoliths: Information for Assessment, Management, and Ecology. Springer.\nWeisberg, S. 1993. Using hard-part increment data to estimate age and environmental effects. Canadian Journal of Fisheries and Aquatic Sciences 50(6):1229–1237.\nWhitney, R. R., and K. D. Carlander. 1956. Interpretation of body-scale regression for computing body length of fish. Journal of Wildlife Management 20:21–27."
  },
  {
    "objectID": "recruitment.html",
    "href": "recruitment.html",
    "title": "Appendix F — Recruitment and logistic model",
    "section": "",
    "text": "The length-, weight-, or age-at-maturity is important to monitor for fish populations because these metrics are closely tied to reproductive potential and respond to changes in inter- and intra-specific densities and resource availability (Pope et al. 2010). Methods for modeling the relationship between maturity stage and length is demonstrated in this supplement. Results from these modeled relationships are then used to calculate metrics such as length at 50% maturity. These methods extend directly to use with age or weight data."
  },
  {
    "objectID": "recruitment.html#maturity-data",
    "href": "recruitment.html#maturity-data",
    "title": "Appendix F — Recruitment and logistic model",
    "section": "F.1 Maturity Data",
    "text": "F.1 Maturity Data\nRaw maturity data generally consists of a maturity statement (either “mature” or “immature”), size (length or weight), age, sex, and other variables as needed (e.g., capture date, capture location) recorded for individual fish. The maturity variable may need to be derived from more specific data about the “stage of maturation” recorded for each fish. Often, maturity will be recorded as a dummy or indicator variable – “0” for immature and “1” for mature – but this is not required for most modern software. Sex is an important variable to record as maturity should be analyzed separately for each sex (Pope et al. 2010).\nSummarized maturity data consists of the proportion of individuals that are mature within each age or length category. Age categories are generally the recorded ages, whereas recorded lengths are often categorized into bins. Age or length categories should be as narrow as possible but include enough individuals such that the proportion mature in each bin is reliably estimated.\nIn this supplement, the total length of the Rockfish was measured to the nearest cm. Length categories that were 2 cm were chosen to summarize the data to provide reasonable sample sizes (>10>10 fish) in the length ranges where the proportion of mature fish is most rapidly changing. These length categories are added to the data.frame with lencat() below (as described in Chapter 2 of the IFAR book).\n> df %<>% mutate(lcat2=lencat(length,w=2))\n> headtail(df)\n\n          date length age maturity stage year            era lcat2\n1   2003-09-02     31  10 Immature     1 2003 2002 and after    30\n2   2002-10-07     32   6 Immature     1 2002 2002 and after    32\n3   2000-07-18     32  11 Immature     1 2000       pre-2002    32\n146 2002-08-18     67  50   Mature     6 2002 2002 and after    66\n147 2002-10-07     68  88   Mature     6 2002 2002 and after    68\n148 2001-04-23     70  66   Mature     4 2001       pre-2002    70\nThe frequency of mature and immature fish in each length category is computed with xtabs() below. The raw frequencies are converted to “row proportions” with prop.table() to see the proportion of fish within each length category that are mature. Finally, a plot of the proportion of mature fish is constructed (Figure 1).\n> freq <- xtabs(~lcat2+maturity,data=df)\n> props <- prop.table(freq,margin=1)\n> round(props,3)   # for display only\n\n     maturity\nlcat2 Immature Mature\n   30    1.000  0.000\n   32    1.000  0.000\n   34    1.000  0.000\n   36    0.556  0.444\n   38    0.625  0.375\n   40    0.333  0.667\n   42    0.077  0.923\n   44    0.077  0.923\n   46    0.036  0.964\n   48    0.062  0.938\n   50    0.000  1.000\n   52    0.000  1.000\n   54    0.000  1.000\n   56    0.000  1.000\n   58    0.000  1.000\n   60    0.000  1.000\n   62    0.000  1.000\n   64    0.000  1.000\n   66    0.000  1.000\n   68    0.000  1.000\n   70    0.000  1.000\n\n> plot(props[,\"Mature\"]~as.numeric(rownames(props)),pch=19,\n       xlab=\"Total Length (cm)\",ylab=\"Proportion Mature\")\n\nFigure 1: Proportion of female Yelloweye Rockfish that were mature in each 2-cm length category.\nThese results show that the percentage of mature female Yellow Rockfish increases quickly between 34 and 42 cm."
  },
  {
    "objectID": "recruitment.html#modeling-with-raw-data",
    "href": "recruitment.html#modeling-with-raw-data",
    "title": "Appendix F — Recruitment and logistic model",
    "section": "F.2 Modeling with Raw Data",
    "text": "F.2 Modeling with Raw Data\n\nF.2.1 Fitting the Logistic Regression Model\nRaw maturity data is generally summarized with a logistic regression. A logistic regression is conducted with a binomial response variable and, generally, a quantitative explanatory variable. The relationship between the probability of “success” (pp) and the explanatory variable (length or age) is generally not linear (primarily due to the constraint that the probability is between 0 and 1). This relationship can be linearized by first transforming pp to the odds (i.e., p1−pp1−p). The odds related the probability of “successes” to “failures.” For example, an odds of 1 indicates that there is an equal probability of success and failure, whereas an odds of 3 indicates that the probability of success is three times the probability of failure.\nThe transformation to a linear relationship is completed by computing the logarithm of the odds (i.e., log(p1−p)log(p1−p)). The complete transformation from pp to log(p1−p)log(p1−p) is called the logit transformation.\nThus, the linear model in a logistic regression models the log odds as a function of an explanatory variable, XX, with\nlog(p1−p)=α+β1X(1)log(p1−p)=α+β1X(1)\nIn maturity analyses, the logistic regression is used to model the log odds of being mature as a function of either length or age. It will be shown later in this supplement how the log odds of being mature are back-transformed to the probability of being mature.\nLogistic regressions are fit with glm(), rather than the lm() used in the IFAR book. For a logistic regression the first argument to glm() is a formula of the form factor~quant where factor and quant generically represent factor and quantitative variables, respectively. The data.frame that contains factor and quant is given to data=. Finally, glm() is forced to use the logit transformation and fit a logistic regression by including family=binomial. For example, the glm() code below will model the log odds of being mature as a function of the length of the Rockfish.\n> glm1 <- glm(maturity~length,data=df,family=binomial)\nParameter estimates are extracted from the glm() object with coef(). Confidence intervals for the parameters of a logistic regression are best estiamted with bootstrapping (rather than normal theory). Bootstrapping is performed with bootCase() from car as described in Chapter 12 of the IFAR book.\n> bcL <- bootCase(glm1,B=1000)\n> cbind(Ests=coef(glm1),confint(bcL))\n\n                   Ests     95% LCI     95% UCI\n(Intercept) -16.9482593 -29.0623105 -11.8152911\nlength        0.4371786   0.3108028   0.7400613\nThe estimated slope indicates that a 1 cm increase in the length of the Yelloweye Rockfish will result in a 0.437 increase in the log odds that the Rockfish is mature. Changes in log odds are difficult to interpret. However, the back-transformed slope is interpreted as a multiplicative change in the odds of being mature. For example, a 1 cm increase in the length of the Yelloweye Rockfish will result in a 1.548 (i.e., e0.437e0.437) times increase in the odds that the Rockfish is mature.\n \n\n\nF.2.2 Predicted Probability of Being Mature\nThe probability of a fish being mature given the observed value of the explanatory variable (xx) can be computed by solving Equation 1 for pp,\np=eα+β1x1+eα+β1(2)p=eα+β1x1+eα+β1(2)\nThis prediction is computed with predict(), which requires the glm() object as the first argument, a data.frame with the values of the explanatory variable for which to make the prediction as the second argument, and type=\"response\" (which forces the prediction of the probability, rather than the log odds, of being mature). For example, the predicted probabilities of being mature for female Yelloweye Rockfish that are 32- and 42-cm total length are computed below.\n> predict(glm1,data.frame(length=c(32,42)),type=\"response\")\n\n        1         2 \n0.0493342 0.8042766 \nConfidence intervals for the predicted probability are formed by computing the prediction for each bootstrap sample and then extracting the values for the upper and lower 2.5% of these predictions. This process is most easily accomplished by forming a function that represents Equation 2 and then using apply() to apply that function to each row of the matrix containing the bootstrap samples. This is the same process as described in Chapter 12 of the IFAR book. The code below computes the 95% confidence intervals for the predicted probability of being mature for 32 cm long Yelloweye Rockfish.\n> predP <- function(cf,x) exp(cf[1]+cf[2]*x)/(1+exp(cf[1]+cf[2]*x))\n> p32 <- apply(bcL,1,predP,x=32)\n> quantile(p32,c(0.025,0.975))\n\n      2.5%      97.5% \n0.00313281 0.14033465 \nThus, the probability that a 32 cm Yelloweye Rockfish is mature is between 0.003 and 0.140.\n \n\n\nF.2.3 A Summary Plot\nA plot that illustrates the fit of the logistic regression (Figure 2) is constructed in several steps. First, a base plot that depicts the raw data is constructed. Take special note here that maturity is forced to be numeric between 0 and 1 for the plot and transparent points (as described in Chapter 3 of the IFAR book) are used because there is considerable overplotting with the “discrete” maturity and length data.\n> plot((as.numeric(maturity)-1)~length,data=df,\n       pch=19,col=rgb(0,0,0,1/8),\n       xlab=\"Total Length (cm)\",ylab=\"Proportion Mature\")\nSecond, points that represent the proportion mature for each 2-cm length bin are added to the plot with points() (recall that the summaries in props were constructed above). Note that pch=3 plots the points as “plus signs.”\n> points(props[,\"Mature\"]~as.numeric(rownames(props)),pch=3)\nFinally, the fitted line from the logistic regression is added by first using the glm() object to predict the probability of being mature for lengths that span the range of observed lengths and then plotting these values as a line with lines().\n> lens <- seq(30,70,length.out=99)\n> preds <- predict(glm1,data.frame(length=lens),type=\"response\")\n> lines(preds~lens,lwd=2)\n\nFigure 2: Fitted logistic regression for the proportion of female Yelloweye Rockfish that are mature by total length.\n   \n\n\nF.2.4 Length- or Age-at-Maturity\nA common metric in fisheries science is to find the length or age at which a certain percentage of fish are mature. For example, it is common to ask “what is the length or age at which 50% of the fish have reached maturity?” A general formula for computing this metric is found by solving Equation 1 for XX,\nx=log(p1−p)−αβ1(3)x=log(p1−p)−αβ1(3)\nIn the common case of finding XX for 50% maturity (i.e., p=0.5p=0.5), Equation 3 reduces to\nx=−αβ1(4)x=−αβ1(4)\nThe age at which 50% of the fish are mature is commonly symbolized as A50A50. Similarly, the length at which 90% of the fish are mature would be L90L90.\nThese calculations are simplified by creating a function to perform Equation 3.\n> lrPerc <- function(cf,p) (log(p/(1-p))-cf[[1]])/cf[[2]]\nThis functions takes the coefficents from the glm() object as the first argument and the probability of interest (pp) as the second argument. As examples, the lengths at which 50% and 90% of the female Yelloweye Rockfish are mature are computed below.\n> ( L50 <- lrPerc(coef(glm1),0.5) )\n\n[1] 38.76736\n\n> ( L90 <- lrPerc(coef(glm1),0.9) )\n\n[1] 43.79328\nConfidence intervals for these values are constructed from the bootstrap samples, similar to what was illustrated above for predicted values.\n> bL50 <- apply(bcL,1,lrPerc,p=0.5)\n> ( L50ci <- quantile(bL50,c(0.025,0.975)) )\n\n    2.5%    97.5% \n37.28749 40.10023 \n\n> bL90 <- apply(bcL,1,lrPerc,p=0.9)\n> ( L90ci <- quantile(bL90,c(0.025,0.975)) )\n\n    2.5%    97.5% \n41.68692 45.92492 \nThus, for example, the predicted length at which 50% of the Yelloweye Rockfish are mature is between 37.3 and 40.1, with 95% confidence.\nThe calculation of the L50L50 may be illustrated on a fitted-line plot (Figure 3) by adding the code below to the code used above to construct Figure 2.\n> lines(c(0,L50),c(0.5,0.5),lty=2,lwd=2,col=\"red\")\n> lines(c(L50,L50),c(-0.2,0.5),lty=2,lwd=2,col=\"red\")\n\nFigure 3: Fitted logistic regression for the proportion of female Yelloweye Rockfish that are mature by total length with L50L50 shown."
  },
  {
    "objectID": "recruitment.html#modeling-with-summarized-data",
    "href": "recruitment.html#modeling-with-summarized-data",
    "title": "Appendix F — Recruitment and logistic model",
    "section": "F.3 Modeling with Summarized Data",
    "text": "F.3 Modeling with Summarized Data\nSometimes maturity data is presented in summarized format – that is, the proportion of fish for each length that were mature. Such data can be computed from the original data.frame with the code below. Note that there are two “tricks” in this code. First, the maturity factor variable is coerced to be a numeric variable, but 1 is subtracted from this result because values of 1 for immature and 2 for mature are returned by as.numeric() when applied to a factor variable. Second, the mean of this result is the proportion of ones in the data, which is the proportion of mature fish.\n> df2 <- df %>%\n    group_by(length) %>%\n    summarize(pmat=mean(as.numeric(maturity)-1),\n              n=n()) %>%\n    as.data.frame()\n> headtail(df2)\n\n   length pmat n\n1      31    0 1\n2      32    0 4\n3      33    0 2\n33     67    1 1\n34     68    1 1\n35     70    1 1\nAlso note that this code is only used here to produce data to illustrate how to analyze summarized data. If one has raw data, as in this supplement, then the methods of the previous section should be used, though the technique used here provides identical answers.\nThe appropriate logistic regression model is again fit with glm(). However, the left side of the formula is the proportion of “successes” variable and weights= is set equal to the sample size used to compute each proportion. Once the model is fit, the same extractor functions can be used to summarize the results. [Note that glm() and bootCase() will return a warning about non-integer number of successes when used in this way.]\n> glm2 <- glm(pmat~length,data=df2,family=binomial,weights=n)\n> bcL2 <- bootCase(glm2,B=1000)\n> cbind(Ests=coef(glm2),confint(bcL2))\n\n                   Ests     95% LCI     95% UCI\n(Intercept) -16.9482593 -28.2429637 -12.1861359\nlength        0.4371786   0.3175022   0.7179604\n\n> predict(glm2,data.frame(length=c(32,42)),type=\"response\")\n\n        1         2 \n0.0493342 0.8042766 \n\n> p32a <- apply(bcL2,1,predP,x=32)\n> quantile(p32a,c(0.025,0.975))\n\n       2.5%       97.5% \n0.005604308 0.126490127 \nNote that the coefficients and predictions computed here are the same as in the results from using the raw data. The bootstrapped confidence intervals differ slightly due to the inherent randomization used in bootstrap resampling (and because a small number of bootstrap samples were used to produce this supplement)."
  },
  {
    "objectID": "recruitment.html#comparing-logistic-regressions-between-groups",
    "href": "recruitment.html#comparing-logistic-regressions-between-groups",
    "title": "Appendix F — Recruitment and logistic model",
    "section": "F.4 Comparing Logistic Regressions Between Groups",
    "text": "F.4 Comparing Logistic Regressions Between Groups\n\nF.4.1 Model Fitting\nIt may be important to determine if the fit of the logistic regression differs between two groups. For example, one may need to determine if the logistic regression parameters differ significantly between fish captured “pre-2002” and those captured in “2002 and after” (recall that these two “eras” are recorded in era in df).\nThe model required to answer this type of question is a logistic regression version of the dummy variable regression introduced in Chapter 7 of the IFAR book. Specifically, the right side of the formula in glm() is modified to be quant*factor where quant is the covariate (usually length or age) and factor is the factor variable that identifies the groups being compared. As noted in Chapter 7 of the IFAR book, this formula is actually shorthand for a model with three terms – quant and factor main effects and the interaction between the quant and factor variables. In this case, the model is fit as shown below.\n> glm3 <- glm(maturity~length*era,data=df,family=binomial)\nThe significance of terms in a general linear model are computed with a chi-square distribution and summarized in an “Analysis of Deviance Table”, rather than with an F distribution and ANOVA table as with a linear model. Fortunately, the Analysis of Deviance table using Type II tests is also retrieved with Anova() from car.\n> Anova(glm3)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: maturity\n           LR Chisq Df Pr(>Chisq)\nlength       68.599  1    < 2e-16\nera           0.005  1    0.94541\nlength:era    3.100  1    0.07831\nAs with the ANOVA table in a dummy variable regression, the Analysis of Deviance table should be read from the bottom. In this case, the interaction term is not signficant which suggests that the slopes for the logit-transformed models do not differ between the eras. The era main effect is also not signficant, which suggests that the y-intercepts for the logit-transformed models do not differ between the eras. Thus, there is no signficant difference in the logistic regressions between fish captured in the two eras.\n\n\nF.4.2 Comparing Lengths- or Ages-at-Maturity\nA p-value for testing whether the L50L50 differed between groups may be computed from the bootstrapped samples. However, this calculation requires several steps and a good understanding of the parameter estimates from the logistic regression model fit to both groups. Thus, the steps and the parameter estimates are described further below.\nBefore building the hypothesis test, lets examine the parameter estimates for the logistic regression model.\n> coef(glm3)\n\n             (Intercept)                   length        era2002 and after \n             -27.1314345                0.6956840               14.0137840 \nlength:era2002 and after \n              -0.3510082 \nNote that the parameter estimates under the (Intercept) and length headings are the intercept and slope, respectively, for the “reference” group in the model. The reference group is the alphabetically first group, unless the levels of the factor variable were changed by the user as was done in this supplement. The order of the levels can be observed with str() or levels().\n> levels(df$era)\n\n[1] \"pre-2002\"       \"2002 and after\"\nThus, the estimated intercept and slope of the logistic regression for the “pre-2002” era fish are -27.131 and 0.696, respectively.\nThe parameter estimates under the era2002 and after and length:era2002 and after are the differences in intercept and slope between the two eras. Thus, these values need to be added to the intercept and slope for the “pre-2000” era to estimate the intercept and slope for the “2002 and after” era. Thus, the estimated intercept and slope of the logistic regression for the “2002 and after” era fish are -13.118 and 0.345, respectively.\nThe first step in building the hypothesis test for whether L50L50 differs between eras is to construct the bootstrap samples from the glm() object.\n> bcL3 <- bootCase(glm3,B=1000)  \n> headtail(bcL3)\n\n        (Intercept)     length era2002 and after length:era2002 and after\n[1,]      -28.66650  0.7480834         17.630834              -0.44741290\n[2,]      -22.01427  0.5610905         -1.315964               0.05293831\n[3,]      -29.11699  0.7535871         17.701264              -0.46385684\n[998,]    -52.92599  1.2978597         43.104623              -1.04921293\n[999,]    -22.29335  0.5727242         11.420617              -0.27867057\n[1000,]  -610.63332 14.9158441        596.471707             -14.53849292\nThe L50L50 for fish from the “pre-2002” era is computed for each sample using only the first two columns of the bootstrap sample results (i.e., the intercept and slope for the “pre-2002” era) and the lrperc() function created and used in a previous section. The L50L50 for fish from the “2002 and after” era is computed similarly but the last two columns in the bootstrap sample results must be added to the first two columns (i.e., produce the intercept and slope for the “2002 and after” era).\n> L50.pre= apply(bcL3[,1:2],1,lrPerc,p=0.5)\n> L50.post=apply(bcL3[,1:2]+bcL3[,3:4],1,lrPerc,p=0.5)\nIf there was no difference in L50L50 between the two eras, then one would expect the means of these two groups to be the same or, equivalently, the mean of the differences in these two value to equal zero. The difference in L50L50 for each bootstrap sample is computed below.\n> L50.diff <- L50.pre-L50.post\nA two-sided p-value may be computed as two times the smaller of the proportions of samples that are less than or greater than 0. [Note that the code below exploits the fact that R will treat a TRUE as a 1 and a FALSE as a 0 such that the mean of a vector of TRUEs and FALSEs will return the proportion of TRUEs.]\n> ( p.L50.diff <- 2*min(c(mean(L50.diff>0),mean(L50.diff<0))) )\n\n[1] 0.556\nThis result suggests that there is no significant difference in the L50L50 for fish captured in the two eras (not surprising given that the logistic regression parameters did not differ between eras).\nConfidence intervals for the difference in L50L50 between the eras and the L50L50 for each era may be computed as before but making sure to use the correct vector of results.\n> ( ci.L50.diff <- quantile(L50.diff,c(0.025,0.975)) )\n\n     2.5%     97.5% \n-1.953197  4.214636 \n\n> ( ci.L50.pre <-  quantile(L50.pre,c(0.025,0.975)) )\n\n    2.5%    97.5% \n37.24793 40.91101 \n\n> ( ci.L50.post <- quantile(L50.post,c(0.025,0.975)) )\n\n    2.5%    97.5% \n34.97016 40.16213 \n   \n\n\nF.4.3 A Summary Plot\nThe construction of a plot that illustrates the fitted logistic regression lines for both groups is left largely as an exercise for the reader. Note that the code below uses several of the same ideas shown previously and relies on directions given in Chapter 3 of the IFAR book.\n> ## Set-up colors\n> clrs1 <- c(\"black\",\"red\")\n> clrs2 <- col2rgbt(clrs1,1/5)\n> ## Get predicted values for each era\n> lvls <- levels(df$era)\n> lens <- seq(30,70,length.out=99)\n> pa02 <- predict(glm3,type=\"response\",\n                  data.frame(length=lens,era=factor(\"2002 and after\",levels=lvls)))\n> pp02 <- predict(glm3,type=\"response\",\n                  data.frame(length=lens,era=factor(\"pre-2002\",levels=lvls)))\n> ## Make the base plot\n> plot((as.numeric(maturity)-1)~length,data=filterD(df,era==lvls[1]),\n       pch=19,col=clrs2[1],xlab=\"Total Length (cm)\",ylab=\"Proportion Mature\")\n> points((as.numeric(maturity)-1)~length,data=filterD(df,era==lvls[2]),\n       pch=19,col=clrs2[2])\n> ## Add the two fitted lines\n> lines(pa02~lens,lwd=2,col=clrs1[1])\n> lines(pp02~lens,lwd=2,col=clrs1[2])\n\nFigure 4: Fitted logistic regression for the proportion of female Yelloweye Rockfish that are mature by total length separated by the “pre-2002” and “2002 and after” eras."
  },
  {
    "objectID": "recruitment.html#reproducibility-information",
    "href": "recruitment.html#reproducibility-information",
    "title": "Appendix F — Recruitment and logistic model",
    "section": "F.5 Reproducibility Information",
    "text": "F.5 Reproducibility Information\n\nCompiled Date: Fri Nov 06 2015\nCompiled Time: 5:16:09 PM\nR Version: R version 3.2.2 (2015-08-14)\nSystem: Windows, i386-w64-mingw32/i386 (32-bit)\nBase Packages: base, datasets, graphics, grDevices, methods, stats, utils\nRequired Packages: FSA, magrittr, dplyr, lubridate, car, captioner, knitr and their dependencies (assertthat, DBI, digest, evaluate, formatR, gdata, gplots, graphics, grDevices, highr, Hmisc, lazyeval, markdown, MASS, memoise, methods, mgcv, nnet, pbkrtest, plotrix, plyr, quantreg, R6, Rcpp, sciplot, stats, stringr, tools, utils, yaml)\nOther Packages: captioner_2.2.3, car_2.1-0, dplyr_0.4.3, extrafont_0.17, FSA_0.8.4, knitr_1.11, lubridate_1.3.3, magrittr_1.5\nLoaded-Only Packages: assertthat_0.1, DBI_0.3.1, digest_0.6.8, evaluate_0.8, extrafontdb_1.0, formatR_1.2.1, gdata_2.17.0, grid_3.2.2, gtools_3.5.0, highr_0.5.1, htmltools_0.2.6, lattice_0.20-33, lazyeval_0.1.10, lme4_1.1-10, MASS_7.3-44, Matrix_1.2-2, MatrixModels_0.4-1, memoise_0.2.1, mgcv_1.8-8, minqa_1.2.4, nlme_3.1-122, nloptr_1.0.4, nnet_7.3-11, parallel_3.2.2, pbkrtest_0.4-2, plyr_1.8.3, quantreg_5.19, R6_2.1.1, Rcpp_0.12.1, rmarkdown_0.8.1, Rttf2pt1_1.3.3, SparseM_1.7, splines_3.2.2, stringi_1.0-1, stringr_1.0.0, tools_3.2.2, yaml_2.1.13\nLinks: Script / RMarkdown"
  },
  {
    "objectID": "recruitment.html#references",
    "href": "recruitment.html#references",
    "title": "Appendix F — Recruitment and logistic model",
    "section": "F.6 References",
    "text": "F.6 References\nGrolemund, G., and H. Wickham. 2011. Dates and times made easy with lubridate. Journal of Statistical Software 40(3):1–25.\nPope, K. L., S. E. Lochmann, and M. K. Young. 2010. Methods for assessing fish populations. Pages 325–351 in W. A. Hubert and M. C. Quist, editors. Inland fisheries management in north americaThird. American Fisheries Society, Bethesda, MD."
  }
]